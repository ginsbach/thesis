
    This thesis introduced a constraint programming methodology that operates on
    SSA compiler intermediate representation.
    The Compiler Analysis Description Language (CAnDL) and the extended Idiom
    Detection Language (IDL) were developed, and implemented in the LLVM
    framework.
    This made the constraint programming method available for program analysis.

    Several computational idioms were specified using CAnDL and IDL, enabling
    automatic recognition of adhering user code sections during compilation.
    The well-studied kernels among these idioms were stencils and varied
    forms of sparse and dense linear algebra.
    Complementing these established kernels, Complex Reduction and Histogram
    Computations (CReHCs) were introduced as a new grouping of computations.
    The evaluation on the established benchmark suites NPB and Parboil demonstrated
    that all of these computational idioms covered significant performance
    bottlenecks.

    Recognising these computational idioms enabled generic compilers to apply
    idiom-specific optimising transformations, which are traditionally available
    only within domain-specific tools.
    Such transformations included the automatic parallelisation of programs that
    were inaccessible to previous analysis approaches.

    Given this background, the idiom detection was performed on sequential C/C++
    programs, achieving automatic heterogeneous parallelisation.
    Idiomatic loops in the code were redirected to domain-specific code
    generators.
    These specialised tools leveraged the domain knowledge that is available for
    code in restrictive idioms.
    This approach was applicable to 10 of the 21 benchmark programs from NPB and
    Parboil, resulting in speedups between 1.26$\times$ and 275$\times$ over the
    sequential baseline versions.

    In summary, the thesis demonstrated that computational idioms are a
    suitable interface to heterogeneous acceleration and developed the
    methodology for automatically recognising such idioms.
    The main contributions are the method of constraint programming on SSA
    intermediate representation, the design and implementation of CAnDL and IDL,
    and the identification of generalised reductions as a significant class of
    benchmark bottlenecks.

\section{Contributions}

    \paragraph*{Constraint Programming on SSA Code}
    \Cref{chapter:theory} introduced constraint programming on
    SSA compiler intermediate representation.
    Using a characterisation of the static structure of SSA programs,
    {\it SSA constraint problems} were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    This thesis derived efficient algorithms for solving SSA constraint problems
    and discussed several significant types of constraint formulas, reflecting
    compiler analysis methods such as data flow and dominance relationships.

    The constraint programming methodology was derived starting from algebraic
    formulation through to developing the implementation in C++.
    It applies to any SSA compiler intermediate representation and is suitable
    for a wide range of analysis problems.

    \paragraph*{Specification Languages}
    \Cref{chapter:candl,chapter:reductions} designed and implemented two novel
    declarative specification languages: CAnDL (Compiler Analysis
    Description Language) and its extension IDL (Idiom Detection Langauge).
    These languages make the constraint programming method available in the
    LLVM framework.
    This enables constraint programming on the intermediate code of user
    programs during compilation with the Clang C/C++ compiler.

    \paragraph*{Complex Reduction and Histogram Computations}
    Complex Reduction and Histogram Computations (CReHCs) were introduced in
    \Cref{chapter:reductions} as a computational idiom.
    CReHCs are a generalisation of the well-understood scalar reduction that
    covers the indirect array accesses typically found in histogram
    calculations.
    This type of loop was not previously studied, but this thesis showed that
    shared parallelisation opportunities exist.
    Furthermore, an evaluation of established benchmark suites demonstrated
    that several of the performance bottlenecks in each of NPB, Parboil, and
    Rodinia are, in fact,  CReHCs.

    \paragraph*{Specification of Computational Idioms}
    \Cref{chapter:candl,chapter:reductions,chapter:idioms} specified a range of
    computational idioms in CAnDL and IDL.
    These included stencils, different forms of sparse and dense linear algebra,
    polyhedral Static Control Parts (SCoPs), and CReHCs.
    The constraint formulations enabled the automatic recognition of high-level
    algorithmic structure during compilation.
    This enabled generic compilers to apply domain-specific reasoning.

\paragraph*{Heterogeneous Acceleration Pipeline}

    \Cref{chapter:idioms} implemented a heterogeneous acceleration pipeline.
    The user code sections recognised by IDL specifications were redirected to
    domain-specific code generators for heterogeneous accelerator hardware.
    The evaluation demonstrated significant heterogeneous parallelisation
    speedups on established benchmark programs.

\section{Critical Analysis}

    The approaches of this thesis were built on the derivation of
    \Cref{chapter:theory} and then evaluated in diverse scenarios on C/C++
    program code.
    Despite the effort of bridging between the algebraic formulation and the
    application in real-world scenarios, the work remains a prototype, and
    several issues need to be addressed before it becomes viable for productive
    use.
    Importantly, this includes questions about the prevalence of idiomatic code,
    the affordability of significant compile time overhead, and the ability to
    compensate fundamental limitations of the underlying solver technology.

\paragraph*{Generality of Computational Idioms}  

    Computational idioms were defined in
    \Cref{chapter:reductions,chapter:idioms} and
    evaluated successfully on a range of established benchmark collections.
    However, the NPB and Parboil collections both are from areas of scientific
    computing.
    It remains unclear how generally applicable these idioms are on codebases
    from other domains.
    Similar concerns are faced by competing approaches, including the
    polyhedral model.

    More generally, it is unclear how much code in large-scale applications
    could be captured as ``idiomatic'' with even a very range and diverse set of
    idioms.
    Even some benchmark programs within NPB and Parboil contain idiosyncratic
    computations that are unlikely to reoccur in other contexts.
    An example is the ``sad'' program that computes the
    ``Sum of Absolute Differences'' algorithm used by the reference H.264 video
    encoder.

\paragraph*{Compile Time Cost}

    The compile-time cost of idiom detection was evaluated in
    \Cref{chapter:idioms}, showing that overheads between 35$\%$ and 115$\%$
    occurred across the benchmark programs.
    Given that the approach is built on constraint satisfiability methods, this
    is a reassuring result, as the compile times remain within one order of
    magnitude.
    Nonetheless, from the perspective of compiler optimisations, this might be
    a prohibitive cost.
    Moreover, the compile-time overhead is unevenly distributed, with
    disproportionately longer solver times on large functions and especially on
    code sections that are near misses to satisfying the specifications.

    \paragraph*{Bounded Number of Solver Variables}
    The specification languages were restricted by the solver to a finite number
    of variables in the underlying constraint problem.
    Moreover, each additional variable incurred a slight overhead, introducing
    a tradeoff between solver speed and the generality of the specification.
    This caused upper bounds for the number of features within many idiom
    specifications:
    The definition of CReHCs only allowed a maximum of two histograms, stencils
    only allowed a neighbourhood of up to 32 elements, and so on.

    Similar shortcomings are well-known in other disciplines that use constraint
    solvers, and some techniques for resolving them have been suggested by
    \citet{Krings2016Constraint} in the context of constraint logic programming.

\section{Future Work}

    Future work could focus on extending the approach to new idioms,
    improving the usability of specification languages, and applying dynamic
    analysis and machine learning techniques to complement the static analysis.
    Moreover, constraint specifications could eventually be generated from
    source code examples, eliminating the difficulty of writing them manually.

\paragraph*{Complementing Dynamic and Machine Learning Approaches}

    The static methods in this thesis could be enhanced with dynamic approaches
    and machine learning.
    Such methods would naturally complement the entirely static reasoning of
    constraint programming, but were outside the scope of this work.

    Dynamic profiling could be used to preselect candidate hot loops,
    drastically reducing the compile-time overhead.
    More advanced dynamic analysis could collect program features that are
    then fed into machine learning algorithms.
    The use of neural networks to guide compiler optimisations has been
    successfully studied in the relevant literature
    \citep{DBLP:journals/pieee/WangO18}.

    Dynamic methods are also suitable for automating the efficient memory
    transfers required between cooperating hardware in heterogeneous
    computing, and to rule out pointer aliasing.

\paragraph*{Unbounded Number of Variables}

    Related research disciplines found ways around some limitation of their
    underlying solvers.
    For example, Bounded Model Checking \cite{Clarke:2001:BMC:510986.510987}
    uses SAT solvers and involves the unrolling of loops to generate formulas.
    In order to fit the underlying solver, this requires the introduction of a
    certain finite iteration limit $k$.
    However, the checking process can be repeated with increasing values for
    $k$, successively ruling out possible violations.

    The same approach could be applied to constraint solving on SSA:
    The solver is invoked repeatedly with a gradually increasing number of
    variables.
    When no collect constraint exceeds variable capacity anymore, the
    process is terminated.
    This ensures that small solutions are found without unnecessary
    overhead, yet solutions requiring many variables are not discarded entirely.
    No upper limits for collect statements would be required with this scheme.

\paragraph*{Pointer-chasing Idioms}

    The ability to recognise sparse linear algebra sets the methods of this
    work apart from previous approaches.
    Going beyond a single data access indirection, IDL could be used to capture
    data access patterns that involve pointer chases.
    This would allow it to analyse graph operations, such as depth-first graph
    traversal and the PageRank algorithm.
    Parboil implements the breadth-first search program ``bfs'' that could be
    accelerated this way.

    Moreover, IDL could detect iterations over lists, and the insertion and
    removal of list items, extending work by
    \citet{Manilov:2018:GPI:3178372.3179511}.
    Such patterns are not usually performance-critical, but this would be
    another step toward an understanding of dynamic data structures in compilers.

    \paragraph*{Higher-level Specification Languages}
    Compared to previous approaches, the specification languages CAnDL and IDL
    simplify the implementation of compiler analysis functionality and enable
    the detection of more complex idiomatic structures.
    Nonetheless, knowledge of the compiler internals is still required to write
    correct specifications.
    Furthermore, the precise extent to which the dependence of CAnDL and IDL on
    LLVM IR is necessary has not been explored thoroughly.

    Future work could investigate languages that abstract away the
    compiler-specific nature of CAnDL and IDL, providing an improved programming
    experience.
    The pseudocode and matching specifications in
    \Cref{csr_lilacwhat_fig,jds_lilacwhat_fig} in \Cref{chapter:idioms} suggest
    how to achieve this:
    For restricted domains (e.g.\ SPMV), generating IDL from high-level
    expressions is straightforward.

    \paragraph*{Generating Specifications from Examples}
    The automatic generation of specifications from examples is another
    promising idea for follow-up research.
    Initial attempts have shown promise \citep{DBLP:conf/IEEEpact/CollieGO19}.
    Using a graph-matching algorithm that operates on SSA code, codes that
    implement variations of the same idiom will be matched together optimising
    a quality metric.
    This matching will automatically identify code structures that are shared
    between the examples and discard those features that were unique to
    specific samples.
    The common structures will then be separated and turned into constraint
    conditions.

    Suitable quality metrics for graph matching will be critical to the success
    of this approach and could be eventually be tuned with machine learning
    approaches, such as neural networks.
    The manually implemented idioms from this thesis will provide suitable
    training data.

    Eventually, the manual curation of example codes that group together into
    computational idioms could become redundant.
    Automatic profiling of large quantities of code will allow the automatic
    identification of all the relevant hot loops.
    The hot loops could then be grouped into computational idioms using
    automatic clustering algorithms that use a distance metric between SSA code
    based on a success score for the previously discussed graph matching
    approach.

    \paragraph*{Phase-Ordering for Normalisation}
    Constraint programming on LLVM IR relied on several preceding optimisation
    passes for normalising the intermediate generated from user programs.
    The relationship between normalisation and optimisation in compiler
    transformation passes poses interesting research questions.

    To allow the specification of idioms on predictable intermediate code
    structures, the solver is invoked after the Clang optimisation pipeline.
    Optimisations such as loop-independent code motion help eliminate the
    artifacts of superficial implementation decisions by mapping many different
    input programs onto the same optimised code.

    While this approach is effective in practice, LLVM optimisation passes were
    not designed for this purpose.
    Instead, they aim to maximise runtime performance and minimise code size.
    Incidentally, these two goals often coincide with normalising behaviour.

    However, there are clear exceptions to this rule.
    The outcome of loop unrolling introduces unpredicatbility, as the decision
    to unroll is based on opaque heuristics.
    Similarly, threshold parameters for loop unswitching interact with loop
    inversion in obfuscating ways.
    Conditionals from inverted loops in loop nests are sometimes -- but
    now always -- propagated out.
    Strength reduction changes the opcodes of instructions in special cases.

    These challenges were resolved pragmatically in this research.
    High threshold values were set, some optimisations were disabled
    (loop unrolling, vectorisation), and others were partially reversed
    (strength reduction).

    Instead of pruning the ``-Os'' or ``-O2'' presets, future work could
    establish special-purpose normalisation compilers by re-evaluating the
    phase-ordering from scratch.
    According to a new metric, individual compiler optimisations could be
    evaluated on a spectrum from ``perturbing'' to ``normalising''.
    The most highly-scoring passes would then selected for normalisation before
    IDL and CAnDL.
    
\section{Summary}

    This chapter first gave a short summary, outlining the development of the
    approaches in the thesis and putting the achievements into context.
    The core contributions of the work were then listed individually in a
    separate section.
    The critical analysis discussed implementation issues and brought attention
    to details that could profit from further evaluation.

    Interesting challenges remain for future work.
    The suggestions included improvements of the basic approach for constraint
    programming on SSA compiler intermediate representation code, enhanced
    usability of the specification languages, and the application to novel domains.
