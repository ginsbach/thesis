
    This thesis introduced a constraint programming methodology that operates on
    SSA compiler intermediate representation.
    The Compiler Analysis Description Language (CAnDL) and the extended Idiom
    Detection Language (IDL) were developed, and implemented in the LLVM
    framework.
    This made the constraint programming method available for program analysis.

    Several computational idioms were specified using CAnDL and IDL, enabling
    automatic recognition of adhering user code sections during compilation.
    The well-studied kernels among these idioms were stencils and varied
    forms of sparse and dense linear algebra.
    Complementing these established kernels, Complex Reduction and Histogram
    Computations (CReHCs) were introduced as a new grouping of computations.
    The evaluation on the established benchmark suites NPB and Parboil demonstrated
    that all of these computational idioms covered significant performance
    bottlenecks.

    Recognising these computational idioms enabled generic compilers to apply
    idiom-specific optimising transformations, which are traditionally available
    only within domain-specific tools.
    Such transformations included the automatic parallelisation of programs that
    were inaccessible to previous analysis approaches.

    Given this background, the idiom detection was performed on sequential C/C++
    programs, achieving automatic heterogeneous parallelisation.
    Idiomatic loops in the code were redirected to domain-specific code
    generators.
    These specialised tools leveraged the domain knowledge that is available for
    code in restrictive idioms.
    This approach was applicable to 10 of the 21 benchmark programs from NPB and
    Parboil, resulting in speedups between 1.26$\times$ and 275$\times$ over the
    sequential baseline versions.

    In summary, the thesis demonstrated that computational idioms are a
    suitable interface to heterogeneous acceleration and developed the
    methodology for automatically recognising such idioms.
    The main contributions are the method of constraint programming on SSA
    intermediate representation, the design and implementation of CAnDL and IDL,
    and the identification of generalised reductions as a significant class of
    benchmark bottlenecks.

\section{Contributions}

\subsection*{Constraint Programming on SSA Code}

    \Cref{chapter:theory} introduced constraint programming on
    SSA compiler intermediate representation.
    Using a characterisation of the static structure of SSA programs,
    {\it SSA constraint problems} were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    This thesis derived efficient algorithms for solving SSA constraint problems
    and discussed several significant types of constraint formulas, reflecting
    compiler analysis methods such as data flow and dominance relationships.

    The constraint programming methodology was derived starting from algebraic
    formulation through to developing the implementation in C++.
    It applies to any SSA compiler intermediate representation and is suitable
    for a wide range of analysis problems.

\subsection*{CAnDL and IDL}

    \Cref{chapter:candl,chapter:reductions} designed and implemented two
    related declarative programming languages: the Compiler Analysis
    Description Language (CAnDL) and the extended Idiom Detection Langauge
    (IDL).
    These languages make the constraint programming methodology available in the
    LLVM framework.
    This enables constraint programming on the SSA intermediate code of user
    programs during compilation with the Clang C/C++ compiler.

\subsection*{Complex Reduction and Histogram Computations}

    Complex Reduction and Histogram Computations (CReHCs) were introduced in
    \Cref{chapter:reductions} as a computational idiom.
    CReHCs are a generalisation of the well-understood scalar reduction that
    covers the indirect array accesses typically found in histogram
    calculations.
    This type of loop was not previously studied, but this thesis showed that
    shared parallelisation opportunities exist.
    Furthermore, an evaluation of established benchmark suites demonstrated
    that several of the performance bottlenecks in each of NPB, Parboil, and
    Rodinia are, in fact,  CReHCs.

\subsection*{Formulation of Computational Idioms}

    \Cref{chapter:candl,chapter:reductions,chapter:idioms} formulated a range of
    different computational idioms in CAnDL and IDL.
    These idioms included stencils, different forms of sparse and dense linear
    algebra, polyhedral Static Control Parts, and CReHCs.
    The formulations enabled the automatic recognition of high-level algorithmic
    structure during compilation, making it possible for generic compilers to
    apply domain-specific reasoning.

\subsection*{Idiomatic Heterogeneous Acceleration Pipeline}

    \Cref{chapter:idioms} implemented an IDL-based integrated heterogeneous
    acceleration pipeline.
    The code sections recognised by IDL specifications were redirected to
    domain-specific code generators for heterogeneous accelerator hardware.
    This resulted in significant parallelisation speedups on benchmark
    programs.

\section{Critical Analysis}

    The approaches of this thesis were built on the derivation of
    \Cref{chapter:theory} and then evaluated in diverse scenarios on C/C++
    program code.
    Despite the effort of bridging between the algebraic formulation and the
    application in real-world scenarios, the work remains a prototype, and
    several issues need to be addressed before it becomes viable for productive
    use.
    Importantly, this includes questions about the prevalence of idiomatic code,
    the affordability of significant compile time overhead, and the ability to
    compensate fundamental limitations of the underlying solver technology.

\subsection*{Generality of Computational Idioms}  

    Computational idioms were defined in
    \Cref{chapter:reductions,chapter:idioms} and
    evaluated successfully on a range of established benchmark collections.
    However, the NPB and Parboil collections both are from areas of scientific
    computing.
    It remains unclear how generally applicable these idioms are on codebases
    from other domains.
    Similar concerns are faced by competing approaches, including the
    polyhedral model.

    More generally, it is unclear how much code in large-scale applications
    could be captured as ``idiomatic'' with even a very range and diverse set of
    idioms.
    Even some benchmark programs within NPB and Parboil contain idiosyncratic
    computations that are unlikely to reoccur in other contexts.
    An example is the ``sad'' program that computes the
    ``Sum of Absolute Differences'' algorithm used by the reference H.264 video
    encoder.

\subsection*{Compile Time Cost}

    The compile-time cost of idiom detection was evaluated in
    \Cref{chapter:idioms}, showing that overheads between 35$\%$ and 115$\%$
    occurred across the benchmark programs.
    Given that the approach is built on constraint satisfiability methods, this
    is a reassuring result, as the compile times remain within one order of
    magnitude.
    Nonetheless, from the perspective of compiler optimisations, this might be
    a prohibitive cost.
    Moreover, the compile-time overhead is unevenly distributed, with
    disproportionately longer solver times on large functions and especially on
    code sections that are near misses to satisfying the specifications.

\subsection*{Finite Number of Variables}

    CAnDL and IDL are limited to finite numbers of variables in underlying
    constraint problems.
    This results in upper bounds for the number of features many idioms:
    CReHCs can only contain a maximum of two histograms, stencil codes can only
    use a neighbourhood of up to 32 items, and so on.
    In practice, this involves a tradeoff between performance and the generality
    of the idiom specification, as each additional variable introduces a slight
    overhead for the solver.

\section{Future Work}

    Future work will focus on extending the approach to new idioms,
    improving the usability of specification languages, and
    applying dynamic analysis and machine learning techniques to complement the
    static analysis.
    Furthermore, constraint specifications will eventually be generated from
    code examples automatically, removing the difficulty of writing them
    manually.

\subsection*{Complementing with Dynamic and Machine Learning Approaches}

    The presented static methods will be complemented with dynamic approaches
    and machine learning.
    Such methods complement the entirely static reasoning of constraint
    programming on compiler immediate representation naturally but were
    outside the scope of this thesis.

    Dynamic analysis could be used to preselect candidate hot loops, drastically
    reducing the compile-time overhead.
    In particular, dynamic analysis could collect program features that are
    then fed into machine learning algorithms.
    The use of neural networks for guiding compiler optimisations has been
    successfully studied in the relevant literature
    \citep{DBLP:journals/pieee/WangO18}.
    Dynamic methods are also required to automate the efficient memory transfers
    that are required between heterogeneous participants, and to rule out
    pointer aliasing.

\subsection*{Infinite Number of Variables}

    Other research disciplines worked around some limitation of underlying
    solvers.
    For example, Bounded Model Checking \cite{Clarke:2001:BMC:510986.510987}
    uses SAT solvers and involves the complete unrolling of loops.
    In order to fit the underlying solver, this requires the introduction of a
    certain finite iteration limit $k$.
    The entire checking process can then be repeated with larger and larger
    values of $k$, successively ruling out more possible violations.

    The same approach could be applied to constraint solving on SSA:
    The solver is invoked repeatedly with successively increasing numbers of
    variables.
    When no \texttt{collect} statement exceeds variable capacity anymore, the
    process is terminated.
    This ensures that small solutions are found without unnecessary
    overhead, yet solutions that use large numbers of variables are not
    discarded entirely.
    No upper limits for collect statements are required with this scheme.

\subsection*{Pointer-chasing Idioms}

    The ability to express sparse linear algebra sets the detection ability of
    constraint programming on SSA immediate representation apart from previous
    approaches using the polyhedral model or data-flow analysis.
    Going beyond simple data access indirection, IDL will be used on data access
    patterns that involve pointer chases.
    This will enable it to detect graph operations, such as depth-first search
    and the PageRank algorithm.
    Furthermore, this approach could detect list traversal, insertion, and
    removal of elements.
    Such operations are not typically performance-critical but could be the
    first step to a deeper semantic understanding of programs in compilers.

\subsection*{Simpler Specification Languages}

    Compared to previous approaches, the specification languages CAnDL and IDL
    simplify the implementation of compiler analysis functionality and enable
    the detection of more complex idiomatic structures.
    However, profound knowledge of compiler internals is still required to
    write correct specifications.
    Furthermore, while idiom specifications are mostly independent of
    LLVM IR as the specific underlying intermediate representation, the precise
    extent of this independence has not been explored thoroughly.

    Future work will investigate languages that abstract the IR-specific nature
    of CAnDL and IDL, and provide a better programming experience.
    The pseudocode and corresponding IDL specifications in
    \Cref{csr_lilacwhat_fig,jds_lilacwhat_fig} in \Cref{chapter:idioms} already
    suggest how this can be achieved:
    For restricted domains (e.g.\ SPMV), generating IDL from high-level
    expressions is straightforward.

\subsection*{Generating Specifications by Example}

    Future work will also expand on the automatic generation of idiom
    specifications from example code fragments.
    Initial work has shown promise \citep{DBLP:conf/IEEEpact/CollieGO19}.
    Using a graph-matching algorithm that operates on SSA code, codes that
    implement variations of the same idiom will be matched together optimising
    a quality metric.
    This matching will automatically identify code structures that are shared
    between the examples and discard those features that were unique to
    specific samples.
    The common structures will then be separated and turned into constraint
    conditions.

    Suitable quality metrics for graph matching will be critical to the success
    of this approach and will be eventually be tuned with machine learning
    approaches, such as neural networks.
    The manually implemented idioms from this thesis will provide suitable
    training data.

    Eventually, the manual curation of example codes that group together into
    computational idioms could become redundant.
    Automatic profiling of large quantities of code will allow the automatic
    identification of all the relevant hot loops.
    The hot loops could then be grouped into computational idioms using
    automatic clustering algorithms that use a distance metric between SSA code
    based on a success score for the previously discussed graph matching
    approach.
