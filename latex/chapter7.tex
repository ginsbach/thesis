
    This thesis introduced a constraint programming methodology that operates on
    SSA compiler intermediate representation.
    The Compiler Analysis Description Language (CAnDL) and the extended Idiom
    Detection Language (IDL) were developed, and implemented in the LLVM
    framework.
    This made the constraint programming method available for program analysis.

    Several computational idioms were specified using CAnDL and IDL, enabling
    automatic recognition of adhering user code sections during compilation.
    The well-studied kernels among these idioms were stencils and varied
    forms of sparse and dense linear algebra.
    Complementing these established kernels, Complex Reduction and Histogram
    Computations (CReHCs) were introduced as a new grouping of calculations.
    The evaluation on the established benchmark suites NPB and Parboil demonstrated
    that all of these computational idioms covered significant performance
    bottlenecks.

    Recognising these computational idioms enabled generic compilers to apply
    idiom-specific optimising transformations, which are traditionally available
    only within domain-specific tools.
    Such transformations included the automatic parallelisation of programs that
    were inaccessible to previous analysis approaches.

    Given this background, the idiom detection was performed on sequential C/C++
    programs, achieving automatic heterogeneous parallelisation.
    Idiomatic loops in the code were redirected to domain-specific code
    generators.
    These specialised tools leveraged the domain knowledge that is available for
    code in restrictive idioms.
    This approach was applicable to 10 of the 21 benchmark programs from NPB and
    Parboil, resulting in speedups between 1.26$\times$ and 275$\times$ over the
    sequential baseline versions.

    In summary, this thesis demonstrated that computational idioms are a
    suitable interface to heterogeneous acceleration, and developed approaches
    for automatically recognising them.
    The main contributions are the methodology of constraint programming on SSA
    intermediate representation, the design and implementation of CAnDL and IDL,
    and the identification of generalised reductions as a significant class of
    benchmark bottlenecks.

\section{Contributions}

    \paragraph*{Constraint Programming on Compiler Intermediate Representation}
    \Cref{chapter:theory} introduced constraint programming on
    SSA form intermediate code.
    Using a characterisation of the static structure of SSA programs,
    {\it SSA constraint problems} were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    This thesis derived efficient algorithms for solving SSA constraint problems
    and discussed significant classes of constraint formulas, reflecting
    compiler analysis methods such as data flow and dominance relationships.

    The constraint programming methodology was derived starting from algebraic
    formulation through to developing the implementation in C++.
    It applies to any SSA compiler intermediate representation and is suitable
    for a wide range of analysis problems.

    \paragraph*{Specification Languages}
    \Cref{chapter:candl,chapter:reductions} designed and implemented two novel
    declarative specification languages: CAnDL (Compiler Analysis
    Description Language) and its extension IDL (Idiom Detection Langauge).
    These languages make the constraint programming method available in the
    LLVM framework.
    This enables constraint programming on the intermediate code of user
    programs during compilation with the Clang C/C++ compiler.

    \paragraph*{Complex Reduction and Histogram Computations}
    Complex Reduction and Histogram Computations (CReHCs) were introduced in
    \Cref{chapter:reductions} as a computational idiom.
    CReHCs are a generalisation of the well-understood scalar reduction, that
    additionally covers the indirect array accesses typically found in histogram
    calculations.
    This type of loop was not previously studied, but this thesis showed that
    shared parallelisation opportunities exist.
    Moreover, an evaluation on established benchmark suites demonstrated
    that several performance bottlenecks in each of NPB, Parboil, and
    Rodinia are, in fact,  CReHCs.

    \paragraph*{Specification of Computational Idioms}
    \Cref{chapter:candl,chapter:reductions,chapter:idioms} specified a range of
    computational idioms in CAnDL and IDL.
    These included stencils, different forms of sparse and dense linear algebra,
    polyhedral Static Control Parts (SCoPs), and CReHCs.
    The constraint formulations enabled the automatic recognition of high-level
    algorithmic structure during compilation.
    This enabled generic compilers to apply domain-specific reasoning.

\paragraph*{Heterogeneous Acceleration Pipeline}

    \Cref{chapter:idioms} implemented a heterogeneous acceleration pipeline.
    The user code sections recognised by IDL specifications were redirected to
    domain-specific code generators for heterogeneous accelerator hardware.
    The evaluation demonstrated significant heterogeneous parallelisation
    speedups on established benchmark programs.

\section{Critical Analysis}

    The approaches of this thesis were built on the derivation of
    \Cref{chapter:theory} and evaluated in several scenarios on C/C++ program
    code.
    Despite the effort of bridging between the algebraic formulation and the
    application in real-world scenarios, the work remains a prototype, and
    several issues need to be addressed before it becomes viable for productive
    use.
    Importantly, this includes questions about the prevalence of idiomatic code,
    the affordability of significant compile time overhead, and the ability to
    compensate for limitations of the underlying solver technology.

\paragraph*{Universality of Computational Idioms}  

    Computational idioms were specified in
    \Cref{chapter:reductions,chapter:idioms} and
    evaluated successfully on a range of established benchmark collections.
    However, the NPB and Parboil collections are both from areas of scientific
    computing.
    It remains unclear how generally applicable these idioms are on codebases
    from other domains.
    Similar concerns are faced by competing approaches, including the
    polyhedral model.

    More generally, it is unclear how much code in large-scale applications
    could be captured as ``idiomatic'' even with a more extensive set of idioms.
    Even some benchmark programs within NPB and Parboil contain idiosyncratic
    computations that are unlikely to reoccur in other contexts.
    An example is the ``sad'' program that computes the
    ``Sum of Absolute Differences'' algorithm used by the reference H.264 video
    encoder.

\paragraph*{Compile Time Cost}

    The compile time cost of idiom detection was evaluated in
    \Cref{chapter:idioms}, showing that overheads between 35$\%$ and 115$\%$
    occurred across the benchmark programs.
    Given that the approach is built on constraint satisfiability methods, this
    is a reassuring result, as the compile times remain within one order of
    magnitude.
    Nonetheless, from the perspective of compiler optimisations, this might be
    a prohibitive cost.
    Moreover, the compile time overhead is unevenly distributed, with
    disproportionately longer solver times on large functions and especially on
    code sections that are near misses to satisfying the specifications.

    \paragraph*{Bounded Number of Solver Variables}
    The specification languages were restricted by the solver to a finite number
    of variables in the underlying constraint problems.
    Moreover, each additional variable incurred a slight overhead, introducing
    a tradeoff between solver speed and the generality of the specifications.
    This caused upper bounds for the number of features within many idiom
    specifications:
    The definition of CReHCs only allowed a maximum of two histograms, stencils
    only allowed a neighbourhood of up to 32 elements, and so on.

    Similar shortcomings are well-known in other research disciplines using
    solvers, and some techniques for resolving them have been suggested by
    \citet{Krings2016Constraint} in the context of constraint logic programming.

\section{Future Work}

    Directions for future work include the specification of more computational
    idioms, usability improvements of the specification languages, and the
    application of dynamic analysis and machine learning to complement the
    static analysis.
    Moreover, constraint specifications could eventually be generated from
    examples, eliminating the difficulty of writing them manually.

\paragraph*{Complementing Dynamic and Machine Learning Approaches}

    The static methods in this thesis could be enhanced with dynamic approaches
    and machine learning.
    Such methods would naturally complement the entirely static reasoning of
    constraint programming, but were outside the scope of this work.

    Dynamic profiling could preselect candidate hot loops, drastically reducing
    the compile time overhead.
    More advanced dynamic analysis could collect program features to be fed into
    machine learning algorithms.
    The use of neural networks to guide compiler optimisations has been
    successfully studied in the relevant literature
    \citep{DBLP:journals/pieee/WangO18}.

    Dynamic methods are also suitable for automating the efficient memory
    transfers required between cooperating hardware in heterogeneous
    computing, and to rule out pointer aliasing.

\paragraph*{Unbounded Number of Variables}

    Related research disciplines found ways around some limitations of their
    underlying solvers.
    For example, Bounded Model Checking \cite{Clarke:2001:BMC:510986.510987}
    uses SAT solvers and involves the unrolling of loops to generate formulas.
    In order to fit the underlying solver, this requires the introduction of a
    certain finite iteration limit $k$.
    However, the checking process can be repeated with increasing values for
    $k$, successively ruling out possible violations.

    The same approach could be applied to constraint solving on SSA.
    The solver would be invoked repeatedly with a gradually increasing number of
    variables.
    When no collect constraint exceeds variable capacity anymore, the
    process would be terminated.
    This would ensure that small solutions are found without unnecessary
    overhead, yet solutions requiring many variables would not be discarded
    entirely.
    No upper limits for collect statements would be required with this scheme.

\paragraph*{Pointer-Chasing Idioms}

    The ability to recognise sparse linear algebra sets the methods of this
    work apart from previous approaches.
    Going beyond a single data access indirection, IDL could be used to capture
    data access patterns that involve pointer chases.
    This would allow it to analyse graph operations, such as depth-first graph
    traversal and the PageRank algorithm.
    Parboil implements the breadth-first search program ``bfs'' that could be
    accelerated this way.

    Moreover, IDL could detect iterations over lists, and the insertion and
    removal of list items, extending work by
    \citet{Manilov:2018:GPI:3178372.3179511}.
    Such patterns are not usually performance-critical, but this would be
    another step toward an understanding of dynamic data structures in compilers.

    \paragraph*{Higher-level Specification Languages}
    Compared to previous approaches, the specification languages CAnDL and IDL
    simplify the implementation of compiler analysis functionality and enable
    the detection of more sophisticated idiomatic structures.
    Nonetheless, knowledge of the compiler internals is still required to write
    correct specifications.
    Furthermore, the precise extent to which the customisation of the
    specification languages to LLVM IR was necessary has not been explored
    thoroughly.

    Future work could investigate languages that abstract away the
    compiler-specific nature of CAnDL and IDL, providing an improved programming
    experience.
    The pseudocode and the corresponding specifications in
    \Cref{csr_lilacwhat_fig,jds_lilacwhat_fig} suggest how this could be
    achieved.
    They show that for restricted domains (e.g.\ SPMV), generating IDL
    from high-level expressions is straightforward.

    \paragraph*{Phase-Ordering for Normalisation}
    The presented methods for constraint programming on LLVM IR relied on
    preceding optimisation passes for the normalisation of the intermediate
    code generated from user programs.
    The relationship between normalisation and optimisation in compiler
    transformation passes poses interesting research questions.

    To allow the specification of idioms on predictable intermediate code
    structures, the solver was invoked after the Clang optimisation pipeline.
    Optimisations such as loop-independent code motion helped eliminate the
    artefacts of superficial implementation decisions by mapping many different
    input programs onto the same optimised intermediate code.

    While this approach was effective in practice, the existing LLVM
    optimisation passes were not designed for this purpose.
    Instead, they aim to maximise runtime performance and minimise code size.
    Incidentally, these two goals often coincide with normalising behaviour.

    However, there are clear exceptions to this rule.
    The outcome of loop unrolling introduces unpredictability, as the decision
    to unroll is based on opaque heuristics and threshold values that depend
    on command-line options.
    Similarly, threshold values for loop unswitching interact with the loop
    inversion transformation in obfuscating ways.
    The surrounding conditionals of inverted loops within loop nests are
    sometimes -- but now always -- propagated out.
    Finally, strength reduction changes the opcodes of instructions in special
    cases.

    These challenges were resolved pragmatically in this research.
    High threshold values were set, some optimisations were disabled
    (loop unrolling, vectorisation), and others were partially reversed
    (strength reduction).

    Instead of pruning the ``-Os'' or ``-O2'' presets, future work could
    establish special-purpose normalisation compilers by re-evaluating the
    phase-ordering from scratch.
    According to a new metric, individual compiler optimisations could be
    evaluated on a spectrum from ``distorting'' to ``normalising''.
    The highly-scoring passes would then selected for a dedicated
    phase-ordering for intermediate code normalisation.

    \paragraph*{Generating Specifications from Examples}
    The automatic generation of specifications from examples is another
    direction for follow-up research.
    Initial attempts were promising \citep{DBLP:conf/IEEEpact/CollieGO19}.
    Using a graph-matching algorithm that operates on SSA code, implementation
    variations of the same idiom were overlayed, exposing the common structure.
    This was guided by a quality metric for the matching.
    Code structures that were shared between the examples were turned into
    constraints, and features that were unique to specific samples were
    discarded.

    Suitable quality metrics for graph matching are critical to the success
    of this approach and could eventually be tuned with machine learning
    approaches, such as neural networks.
    The manually implemented idioms from this thesis would provide suitable
    training data.

    Eventually, the manual curation of example codes that group together into
    computational idioms could become redundant.
    Automatic profiling of large quantities of code would allow the automatic
    identification of all the relevant hot loops.
    The hot loops could then be grouped into computational idioms using
    automatic clustering algorithms that use a distance metric between SSA code
    based on a success score for the previously discussed graph matching
    approach.

\section{Summary}

    This chapter first gave a short summary, outlining the development of the
    approaches in this thesis and putting the achievements into context.
    The core contributions of the work were then listed individually in a
    separate section.
    The critical analysis discussed implementation issues and brought attention
    to details that could profit from further evaluation.

    Interesting challenges remain for future work.
    The suggestions included improvements of the basic approach for constraint
    programming on SSA compiler intermediate representation code, enhanced
    usability of the specification languages, and the application to novel domains.
