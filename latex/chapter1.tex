\section{The Emergence of Heterogeneous Computing}

    For several decades, from the 1970s until the early 2000s, advances in
    processor development followed Moore's Law \citep{4785860} and
    Dennard Scaling \citep{1050511}.
    The density of integrated circuits doubled every two years,
    and this allowed increased clock frequencies while maintaining
    steady power consumption.
    These conditions enabled continual processor improvements that were driven
    primarily by rising clock frequencies and microarchitectural refinements,
    enabling ever faster computations.

    The physics-driven nature \citep{Hutcheson2018Moore} of these advances in
    hardware has had important implications for software development.
    Not only did the performance of computers improve exponentially, but these
    performance gains were in the form of direct speedups, available to all the
    already existing programs.
    The primary interfaces between software and hardware - the instruction set
    architectures of processors - evolved gradually and with few
    paradigmatic changes \citep{8310168}.
    This is best exemplified by the pervasive x86 instruction set architecture,
    which still retains backward compatibility with its initial version from
    1978, and dominates desktop processors to this day.
    Therefore, software developers and users could rely on ever-increasing
    performance from hardware progress alone, without any intervention.

\subsection{Via Multi-Processing to Heterogeneity}

    This continual progress started breaking down around 2005 with the apparent
    end of Dennard Scaling \citep{6307773}.
    While the shrinking of transistors continued, this no longer enabled
    proportionally increased clock frequencies with the same power budget.
    Instead, processor designers started using the increasing transistor
    budget for additional processor cores.
    This shift toward multi-processing has left a deep mark on software
    development.
    New programming paradigms and languages, annotation systems, programming
    interfaces, libraries and compiler techniques are still being developed to
    address the challenges of parallel computing.

    In recent years, transistor scaling has slowed significantly.
    In response to the breakdown of both Dennard Scaling and Moore's Law, the
    hardware industry has turned toward architectural innovation
    \citep{7878935}.
    In particular, there is a trend toward specialised processor cores that work
    in tandem as a heterogeneous system.
    Such dedicated processor cores outperform general-purpose cores on
    specific tasks.
    Furthermore, as heat dissipation has become a major challenge,
    simultaneously powering all transistors is often unviable, meaning that
    different processor cores have to be masked out dynamically at runtime
    (``dark silicon'') \citep{6307773}.
    Disabling a subset of a homogeneous multi-core system renders some of its
    cores redundant.
    However, specialised cores can improve the system performance overall even
    if they are disabled most of the time and only used for the specific tasks
    at which they excel.

\section{The Diminished Role of Traditional Compilers}

    Heterogeneous computing can help overcome the scaling limitations of
    homogeneous, general-purpose processors.
    However, it also poses a challenge to the associated software ecosystem,
    putting into question many of the achievements in portability and longevity
    of programs that are taken for granted in modern computing \citep{8719512}.
    Existing software does not automatically benefit from entirely new
    accelerator designs in the way that it profited from the continuous
    improvements of established architectures.
    Where programs previously performed better on each succeeding hardware
    generation, new accelerators arrive with novel and incompatible interfaces.

    In particular, this new hardware landscape greatly diminishes the scope of
    responsibilities and impact that traditional compilers for languages such as
    C, C++ and Fortran can have.
    Such compilers used to be responsible for orchestrating
    program execution on the entirety of available computing resources.
    They are now generally limited to only targeting the relatively small
    homogeneous fraction of processor cores directly.

\subsection{Libraries and Domain-Specific Languages}

    In order to reach peak performance on rapidly evolving and highly parallel
    hardware, novel programming paradigms built around libraries and
    domain-specific languages have emerged.
    They succeed in utilising heterogeneous hardware in situations where
    traditional compilers fail.

    Two examples display the diversity of these approaches.
    Firstly, \citet{Ragan-Kelley2013Halide} developed the domain-specific
    language Halide for image processing.
    The Halide toolchain is able to generate fast code for heterogeneous
    platforms by focusing on a well-understood class of computations and by
    using a restrictive program representation.
    After tuning programs for particular platforms, it outperforms
    hand-optimised code, demonstrating the advantage of domain-specific compiler
    optimisation under circumstances of constrained semantics.

    Secondly, Basic Linear Algebra Subprograms (BLAS)
    \citep{2002:USB:567806.567807}, a standard for library function interfaces
    going back to \citet{Lawson:1979:BLA:355841.355847} in the 1970s, have been
    implemented for most accelerators.
    These implementations are widely used and offer unrivalled performance.
    Some versions are provided directly by hardware vendors to support
    accelerators \citep{mkl,cublas,clblas,apl,qml}, while others originate as
    academic projects \citep{Wang:2013:AAG:2503210.2503219}.
    These competing implementations use a plethora of approaches to achieve as
    close to peak performance as possible.
    These includes manually written assembly code, but also highly advanced
    code generation techniques, custom program representations and many more.

\subsection{The Consequences of the Decline of Compilers}

    Domain-specific languages and library interfaces make the full performance
    of heterogeneous systems accessible to programs.
    These success stories, however, leave significant problems unaddressed.
    The adoption costs are high, requiring application rewrites for
    accelerators.
    This coincides with often uncertain long-term prospects and minimal
    cross-platform portability.
    Even in the case of the agreed-upon BLAS standard -- arguably the best case
    scenario -- adoption of novel implementations is non-trivial in practice,
    due to the frequently encountered interface extensions for managing device
    handlers and memory synchronisation.
    For academic-backed domain-specific languages like Halide, on the other
    hand, complete rewrites are required in entirely novel software ecosystems,
    with an unclear future of support.

    On homogeneous systems, programmers could rely on compilers for existing
    programming languages to evolve in lockstep with processor development.
    For example, even decades-old C++ source code compiles into efficient
    programs for the newest generation of x86 processors.
    Libraries and domain-specific languages are useful on homogeneous systems
    for achieving absolute maximum performance, but they become essential only
    in the context of heterogeneity.
    Therefore, the pervasive requirement for domain-specific languages and
    libraries only arises because compilers are unable to map programs onto
    specialised cores.
    Instead, compilers are increasingly downgraded to merely coordinating the
    launch of core workloads in the form of computational kernels, which are
    then executed as separate and opaque programs.

    It may appear that, for example, a C++ or Java compiler cannot
    be provided for a typical graphics processor.
    After all, most graphics processors have hardware limitations that prevent
    them from implementing the entire language standard.
    Indeed, many accelerators are even further from Turing complete.
    However, this should not prevent a partial compiler.
    Such a partial compiler would compile fitting program parts for accelerators
    and utilise a fallback system for the remainder of the program.
    After all, that is precisely the result achieved with libraries and
    domain-specific languages, and likely the desirable outcome.
    Despite the apparent convenience of such an approach, the next section gives
    reasons why such a scheme has not been championed so far.

\section{Host Compilers and Kernel Compilers}
\label{sec:hostkernel}

    While mainstream compilers for languages such as C/C++, Fortran or Java
    generally fail to exploit the full performance of heterogeneous systems, a
    specific class of compilers already plays an essential role in
    targeting heterogeneous hardware.
    Many of the kernel programs that remain opaque to the application compilers
    are themselves products of other, specialised compilers.
    This necessitates a distinction between {\em host compilers} and {\em kernel
    compilers}.
    This distinction is mirrored by the differences between {\em host languages}
    and {\em kernel languages}.
    Host compilers translate full programs written in host languages -- such as
    the previously mentioned C/C++, Fortran and Java -- while kernel
    compilers handle small computational kernels that are expressed in dedicated
    domain-specific programming languages.
    The two classes of compilers have developed differently.
    Kernel compilers successfully apply many of the techniques that are severely
    limited on host compilers
    \citep{Murphy2014LimitsOD,Maleki:2011:EVC:2120965.2121464}.
    They reason automatically about parallelism
    \citep{Steuwer:2017:LFD:3049832.3049841}, are more successful at accurately
    modelling data dependencies, and incorporate iterative compilation
    techniques \citep{Ansel:2014:OEF:2628071.2628092}.

    This is made possible by a combination of factors that uniquely apply to
    kernel compilers:
    Smaller programs allow for more expensive compilation techniques;
    more restrictive languages and intermediate representations allow for
    stronger reasoning;
    abstractions in kernel compilers can be customised for the exact hardware
    architecture of accelerators;
    and domain knowledge from areas such as image processing can be directly
    embedded in custom compiler technology, without requiring their validity
    on generic programs.
    Furthermore, kernel compilers often run in more controlled environments,
    with less need for predictability, reproducibility, and stability.
    The range of input programs might even be small enough to ship them in an
    already compiled form, making the compiler merely a tool in a
    library-implementation process.

    As host compilers operate under less forgiving conditions, it is
    unsurprising that they have lagged behind these developments.
    Because they cannot rely on such a restricted environment, host compilers
    are unable to match the optimisation capabilities of kernel compilers.
    Domain-specific compilers and languages, as well as libraries, have
    therefore not been championed only because their restricted program
    spaces match the capabilities of particular heterogeneous accelerators.
    The restricted environment also makes them intrinsically more powerful and
    able to generate better code.

    Hybrid approaches, such as OpenCL, reinforce these hypotheses.
    The language is domain-specific in its expression of parallelism, but
    otherwise designed to be quite general-purpose.
    Compilers are provided for many structurally different processors.
    However, as a consequence of being a relatively unrestricted language, the
    compilers often underperform and the language is notoriously lacking in
    performance portability \citep{Falch:2015:MLB:2863697.2864570}.
    Partial compilers that require the same compromises as OpenCL -- being
    unable to utilise the restricted nature of input programs -- would surely
    suffer the same shortcomings.

\subsection{The Spectrum of Specialisation}

    General-purpose programming languages, domain-specific languages and
    accelerator libraries exist on a spectrum of specialisation.
    \Cref{specialgradient} provides some intuition about this spectrum by
    gauging the places of popular languages and compute libraries in it.
    On the far left are versatile general-purpose languages, such as C++ and
    Fortran.
    Moving right, flexibility is gradually lost, with BLAS libraries at
    the end providing only fixed-function computations.
    However, the restricted semantics allow for increased optimisation
    capabilities in return.

    It is necessary to clarify what is meant by optimisation potential in this
    context and how it differs from raw performance.
    C++ is, of course, very suitable for manual optimisation and renowned as a
    low-overhead, high-performance language.
    However, it is difficult for compilers to apply deep structural changes to
    C++ programs correctly.
    Maintaining the semantics of BLAS routines, on the other hand, is viable
    with many structurally different implementation approaches.
    Therefore, C++ is fast because of its low overhead, and because C++
    programmers write programs with performance in mind.
    By contrast, BLAS is fast because its semantics are so constrained that
    implementations can be tuned to perfection.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/DSLgradient}
\caption{Domain specific languages on a spectrum between general purpose
         languages and libraries.
         Decreasing flexibility allows for stronger reasoning and greater
         optimisation potential.}
\label{specialgradient}
\end{figure}

    In conclusion, the superior optimisation potential of libraries and
    domain-specific kernel compilers is a consequence of operating in more
    constrained conditions than host compilers.
    While expert programmers can often apply domain knowledge themselves
    when using versatile general-purpose languages, this potential can
    automatically be uncovered by specialised tools.
    The restrictions on input programs imply domain knowledge that is leveraged
    to make stronger assumptions, use more powerful models and generate better
    code.

    {\bf
    More restrictive program models result in additional optimisation
    opportunities and increased portability.
    Compilers use program models that correspond closely to their input
    programming languages.
    This correspondence could be decoupled by automatically recognising program
    parts that adhere to more constrained models, making the powerful
    domain-specific optimisation techniques of kernel compilers available to
    host compilers.
    }

\section{Moving on the Spectrum of Specialisation}

    To combine the positive aspects of both sides of the spectrum of
    specialisation, host compilers need methods to automatically recognise more
    restrictive programming models within general-purpose code.
    Reformulating the adhering program sections in domain-specific
    representations makes the superior domain-specific reasoning from kernel
    compilers applicable also in the host compiler.

    Such an approach of detecting restricted models in general-purpose code has
    previously been used only for specific individual domains, and required
    elaborate hand-implemented compiler analysis functionality.
    For example, the Polly compiler by \citet{Lengauer2012Polly} takes
    arbitrary C/C++ code as input and recognises whether parts of this code
    are expressible in the more restrictive polyhedral model
    \citep{Karp:1967:OCU:321406.321418,benabderrahmane2010polyhedral}.
    The compiler then reformulates the relevant code in this system, which
    makes applicable advanced optimisation techniques that use the available
    domain knowledge of the restricted model for generating faster code
    \citep{Moll:2016:ISS:2892208.2892217,Doerfert2015Polly}, substantiating the
    hypotheses of \Cref{sec:hostkernel}.

\subsection{Contributions of this Thesis}

    This thesis derives a systematic and configurable method for recognising
    constrained program sections during the compilation process.
    The approach emerges from the analysis of a standard program model that host
    compilers use: Static Single Assignment (SSA) form intermediate
    representations.
    These SSA form representations are accessible to mathematical reasoning,
    with their most relevant features based on graph and list structures.
    With this in mind, structural restrictions on intermediate representation
    code -- corresponding to restricted, domain-specific models -- can be
    precisely formulated as mathematical constraints on a model of SSA form
    representations.
    The use of constraint solver techniques then makes automatic detection of
    adhering code regions feasible.

    Instead of manually implementing compiler analysis passes to recognise
    restricted models, a flexible constraint programming language is introduced,
    that can express many different such domain-specific models, and an
    accompanying toolchain is developed that uses these specifications to
    detect code sections that satisfy them automatically.
    This system captures the spectrum from \Cref{specialgradient}.
    On one extreme, sections of code are recognised as implementing
    specific algorithms, like matrix multiplications, that are merely
    parametrised with numbers.
    Next are algorithms that allow parametrisation with an operator.
    Stencil kernels, reduction operations or generalised histogram computations
    fall into this category.
    More generic again are programs that merely follow specific structural
    restrictions, such as code adhering to the polyhedral model.
    \Cref{chapter:candl,chapter:reductions,chapter:idioms} cover each of these
    domains, culminating in a system for the automatic heterogeneous
    acceleration of idiomatic code.

\section{Summary}

    The end of Moore's Law and Dennard Scaling has resulted in a shift away from
    homogeneous multi-processing, toward architectural innovation and
    specialised processor cores.
    The rising heterogeneity of computing resources poses a challenge to
    established compiler toolchains, which often only access the increasingly
    marginal homogeneous fraction of processing power.
    Therefore, programmers depend on domain-specific languages and libraries,
    which leverage domain knowledge from their restricted operating domains, to
    generate efficient code for this new hardware landscape.

    This domain knowledge can be made available to host compilers with
    approaches that automatically specialise general-purpose code and
    reformulate it in more restrictive program models.
    Previous work has established this on individual models, but this thesis
    develops a framework to formulate a wide range of restricted program models
    using a systematic approach based on constraint solving.
    To this purpose, a custom constraint programming language was developed
    and successively extended.
    Starting from a discussion of the underlying assumptions and features of
    Single Static Assignment code, the methodology is developed into a complete
    system, implemented as part of the mature Clang C/C++ compiler.

    In the later chapters, this core system is then applied in combination
    with other techniques in a range of different contexts.
    This includes the rapid prototyping of compiler optimisations, the
    automatic parallelisation of some program structures with indirect memory
    accesses, a new formulation of the polyhedral model and the detection of
    computational structures that cover important bottlenecks in typical
    scientific programs.

    Eventually, the techniques are combined into a fully integrated, extensible
    compiler tool for efficiently mapping sequential programs onto heterogeneous
    systems.
    This develops the thesis all the way from theoretical discussions of
    constraint programming approaches in compilers, to the experimental
    evaluation of real performance improvements on established benchmark suites
    and scientific applications.

\newpage
\section{Structure of the Thesis}

    This thesis is divided into six chapters.
    Following the introduction, the underlying methodology of this work is
    established in {\bf\Cref{chapter:theory}} and put in the context of
    conventional compiler analysis.
    Based on a model of Static Single Assignment (SSA) intermediate
    representations, the chapter derives a novel constraint programming approach
    to express algorithmic program structures as constraint formulas.
    Later sections of the chapter cover implementation decisions, algorithmic
    complexity, and a comparison to established Satisfiability Modulo Theory
    (SMT) problems.
    This forms the methodological basis for
    {\bf\Cref{chapter:candl,chapter:idioms,chapter:reductions}}.

    {\bf\Cref{chapter:literature}} gives a broad overview of the related work.
    This literature survey covers the four main areas of research in which this
    work is placed.
    Firstly, {\em constraint programming} underpins the methodology of this
    thesis.
    Secondly, {\em compiler analysis and auto-parallelisation} provide
    competing approaches against which the results in this thesis are evaluated.
    Thirdly, {\em heterogeneous computing} and the corresponding challenges
    motivate many of the compiler approaches that this research implements.
    Finally, ``{\em computational idiom}'' is an umbrella term for several
    overlapping concepts of algorithmic patterns that capture programming models
    in different positions on the spectrum of specialisation.

    {\bf\Cref{chapter:candl,chapter:idioms,chapter:reductions}}
    are each based on a published research article and elaborate on different
    applications of constraint programming in compilers.

    {\bf\Cref{chapter:candl}} develops a constraint programming language called
    CAnDL and presents an implementation in the LLVM compiler infrastructure.
    CAnDL that automatically generates compiler analysis passes from
    declarative specifications.
    The chapter explores several compiler analysis tasks with CAnDL,
    including the rapid implementation of peephole optimisations and the
    prototyping of graphics shader optimisations.
    This chapter is based on published research in
    \citet{Ginsbach:2018:CDS:3178372.3179515}.

    {\bf\Cref{chapter:reductions}} extends CAnDL into the Idiom Detection
    Langauge (IDL) and develops an auto-parallelising compiler for
    complex reduction and histogram computations built on IDL-powered analysis
    functionality.
    These computations cover many loops with indirect data access that are
    inaccessible to established approaches based on data flow and polyhedral
    analysis.
    The flexibility of constraint programming allows it to still capture such
    irregular computations.
    This chapter is based on published research in
    \citet{ginsbach2017discovery}.

    {\bf\Cref{chapter:idioms}} applies IDL to the detection of algorithmic
    structures that go beyond the scope of traditional compiler analysis:
    stencils, complex reductions and histograms, as well as sparse and dense
    forms of linear algebra.
    The detection in LLVM enables the automatic heterogeneous
    acceleration of sequential code and result in significant speedups on
    established benchmark suites.
    This chapter is based on published research in
    \citet{Ginsbach:2018:AML:3173162.3173182}.
