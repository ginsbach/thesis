    For several decades, from the 1970s until the early 2000s, Moore's Law and
    Dennard Scaling accurately described the progress of processor development.
    The circuit density approximately doubled every two years, yet the power
    consumption and the accompanying thermal limitations remained relatively
    stable.
    This had important implications also for software development.
    Not only did the performance of computers improve exponentially, but these
    performance gains were also immediately available to already existing
    programs.
    The most fundamental contracts between software and hardware - the
    instruction set architectures of processors - evolved without paradigmatic
    changes.
    Software developers and users could therefore trust on ever increasing
    performance from hardware progress alone, without any intervention.
    This is best exemplified by the pervasive x86 instruction set architecture,
    which evolved without breaking backward compatibility from its original
    inception in 1978 and still dominates mainstream processors.

    This seemingly unending progress started breaking down around 2006,
    with the apparent end of Dennard Scaling.
    While the shrinking of transistors continued, this did not coincide with
    increased clock cycles and stable power consumption any longer.
    The resulting paradigmatic shift toward multi-processing left a deep mark on
    software development.
    New programming paradigms and languages, annotation systems, programming
    interfaces, libraries and compiler techniques were developed and continue to
    evolve.
    Despite immenense progress in the field, parallel computing remains an area
    of active research and automatic approaches often fell short.

    In recent years, it has become apparent that multi-processing as well has
    reached its limits, and another paradigm shift is following.
    With the end of both Dennard Scaling and Moore's Law, the hardware industry
    turns toward architectural innovation, trending away from general purpose
    processors and toward the use of dark silicon and specialised hardware.
    Accelerator processors have become widespread, first in the form of general
    purpose graphics processing units (GPGPU) and now increasingly with deep
    learning accelerators, which coincide with the meteoric rise of
    Convolutional Neural Networks and the enormous success of Deep Learning.

    Heterogeneous computing is a natural and necessary reaction to the scaling
    limitations of general purpose processors, but it poses an enormous
    challenge to the surrounding software ecosystem, and it puts into question
    many of the achievements in portability and longevity of programs that are
    taken for granted in modern computing.
    Existing software is not able to automatically benefit from entirely new
    acccelerator designs in the way that it profited from continuous
    microarchitectural improvements.
    Where previously, programs performed better on each succeeding hardware
    generation, with at most a recompilation required, new accelerators arrive
    with entirely novel and incompatible application interfaces.

    In particular, this novel hardware landscape greatly diminishes the
    scope of responsibilities and impact that traditional compilers for
    languages like C/C++ and Fortran used to have.
    Were previously, such compilers were responsible for orchestrating
    program execution on the entirety of available computing resources, they are
    increasingly downgraded to merely coordinating the launch of core workloads
    in the form of computational kernels, which are executed as separate and
    opaque programs on heterogeneous accelerators.
    In order to reclaim their central position in the software ecosystem,
    compilers require some fundamentally new approaches.

    An important distinction has to be made between {\em host compilers} and
    {\em kernel compilers} at this point, as these have developed quite
    distinctly.
    In fact, many of the kernel programs that remain opaque to the host compiler
    are themselved products of specialized kernel compilers.
    Kernel compilers successfully apply many of the techniques that have had
    only limited success for host compilers.
    Most importantly, they reason automatically about parallelism, understand
    data dependencies at a deeper level and they incorporate iterative
    compilation.
    This is made possible by a combination of several factors that apply
    uniquely to kernel compilers:
    Smaller programs allow for more expensive compilation techniques;
    more restrictive languages and intermediate representaions allow for
    stronger reasoning;
    and domain knowledge from areas such as image processing can be directly
    embedded in custom compiler technology, without the need to genrealize to
    fully generic programs.
    Furthermore, kernel compilers often run in relatively controlled
    environments, with less need for predictability, reproducability and
    stability.
    The scope of input programs might even be small enough to leave compilation
    to experts and vendors, which then ship the results in the form of already
    compiled library functions.

    As host compilers do not operate under these conditions, it is unsurprising
    that they have lagged behind these developments.
    Despite immense research efforts, auto-parallelization has ultimately failed
    to conclusively solve the problems of parallel computing.
    Heterogeneous computing adds another dimension of complexity on top of
    homogeneous parallelism, making a traditional compiler approach appear
    unfeasable.
    However, research can profit from the experiences with kernel compilers
    and from understanding the resons why concepts from kernel compilers fail to
    translate into host compilers.

    \section{Learning from Success Stories}

    Compilers need to think out of the box and incorporate novel ideas
    in order to leverage the know-how that has evolved in the surrounding
    software ecosystem.

    Programming models built around libraries and novel domain specific
    languages, kernel compilers and library interfaces have had major successes
    in the areas that established compilers have been weakest - namely reaching
    peak performance on rapidly evolving and highly parallel accelerator
    hardware.
    Two examples stand characteristically for the readth of these successes.
    Firstly, implementations of the Basic Linear Algera Subprograms (BLAS)
    interfaces, a standard dating back to the 1970s, are available for most
    accelerators.
    They are widely used and offer unrivaled performance, with implementations
    from hardware vendors and well as from the scientific community.
    Competing implementations use a plethora of approaches to achieve as close
    to peak performance as possible.
    This includes manually written assembly, as well as highly advanced
    compilation approaches, involving novel program representations, polyhedral
    compilation techniques, iterative compilation and many more.
    Secondly, the domain specific language Halide has developed for the domain
    of image processing.
    It has demonstrated an immense optimization potential for compiler based
    optimization under circumstances of highly constrained semantics.

    These success stories, however, still leave many important problems
    unaddressed.
    They are weak where approaches based on host compilers are strong:
    Adoption costs are significant and they are combined with often uncertain
    long-term prospects and very limited cross-platform portability.
    Even in the case of mostly agreed upon standards such as BLAS --
    arguably the best case scenario -- adoption of novel implementations is
    non-trivial in practice due to even slight differences in the interfaces.
    For academic-backed approaches like Halide on the other hand, complete
    rewrites are requires in entirely novel ecosystems with an entirely unclear
    future of support.

    What is promising therefore is a combination of hand-optimized libraries,
    domain specific languages together with compiler automatisms.
    This requires a disruptive improvement in compiler analysis capabilites.
    The detection of higher level algorithmic structures in compilers has been
    investigated before, but established approaches cannot scale to what
    this challenge requires.
    Syntactic matching on programming languages has become
    unviable for the complexities of both modern programming languages and
    complex code bases.

    Instead, this thesis develops an entirely novel approach based on concepts
    from constraint solving, building a pragmatic methodology for detecting
    complex algorithmic structures -- computational idioms.

\pagebreak
\section{Structure of the Thesis}

    This PhD thesis is divided into six chapters.
    Following the introduction in {\bf\cref{chapter:introduction}}, a broad
    overview of the related work is given in {\bf\cref{chapter:literature}}.
    This literature survey covers the four main research areas that this work
    is placed in.
    Firstly {\em constraint programming}, which is the basis of the methodoloy
    for most of this research.
    Secondly {\em compiler analysis and auto-parallelisation}, against which the
    results in this thesis are evaluated.
    Thirdly {\em heterogeneous computing}, which motivates many of the compiler
    approaches that this research enables.
    Lastly {\em computational idioms}, a term for several overlapping concepts
    of algorithmic patterns.

    {\bf\Cref{chapter:theory}} develops the foundations of the constraint
    programming methodology that is forms the core of the later
    \cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}.
    Based on a novel mathematical model of static single assignment (SSA) form
    compiler representaions, the core of the constraint programming approach
    is derived and some particular challenges are discussed in detail.

    {\bf\Cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}}
    are each based on a published research article and elaborate on different
    applications of constraint programming in compilers.

    {\bf\Cref{chapter:candl}} develops a full-fledged constraint programming
    language called CAnDL, with an implementation in the LLVM compiler
    infrastructure that automatically generates compiler analysis passes from
    declarative descriptions.
    Using CAnDL, the chapter explores severeal complex compiler analysis
    challenges, concluding with a full polyhedral code analysis.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:CDS:3178372.3179515}}.

    {\bf\Cref{chapter:reductions}} develops an auto-parallelising compiler for
    complex reduction and histogram computations using CAnDL-powered analysis
    functionality.
    This covers many computations that are inaccesible to established approaches
    based on data flow or polyhedral analysis and is to demonstrate the greater
    flexibility of constraint programming.
    The work of this chapter has been published as
    {\bf\citet{ginsbach2017discovery}}.

    {\bf\Cref{chapter:idioms}} extends CAnDL into the Idiom Description Language
    (IDL) and applies it to algorithmic concepts that go beyond traditional
    compiler analysis: stencil computations, complex reductions and histograms
    as well as sparse and dense linear algebra.
    The resulting automatic detection passes in LLVM enable automatic
    heterogeneous acceleration of sequential code and result in significant
    speedups on established benchmark suites.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:AML:3173162.3173182}}.

    {\bf\Cref{chapter:lilac}} builds on top of the idiom descriptions for linear
    algebra that were introduced in the previous chapter and demonstrates how
    compiler analysis based on constraint programming can be made accessible to
    non-experts.
    With the novel LiLAC language, constraint programs in IDL are automatically
    generated from a high-level specification for many different sparse linear
    algebra representations.
    This powers a fully integrated compiler approach, handling everything from
    algorithmic detection, library call insertion and run time data transfer
    optimisations.
    The work of this chapter has been published as \citet{lilacpaper}.
