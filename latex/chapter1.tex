    For several decades, from the 1970s until the early 2000s, advances in
    processor development followed Moore's Law and Dennard Scaling.
    The circuit density approximately doubled every two years, yet the power
    consumption and the accompanying thermal limitations remained relatively
    stable.
    This hardware progress had major implications also for software
    development.
    Not only did the performance of computers improve exponentially, but these
    performance gains were immediately available to the already existing
    programs.
    The fundamental contracts between software and hardware - the
    instruction set architectures of processors - evolved without paradigmatic
    changes.
    This is best exemplified by the pervasive x86 instruction set architecture,
    which still retains backward compatibility with its original inception in
    1978 and dominates desktop processors to this day.
    Software developers and users could therefore trust on ever increasing
    performance from hardware progress alone, without any intervention.

    This seemingly unending progress started breaking down around 2006,
    with the apparent end of Dennard Scaling.
    While the shrinking of transistors continued, this did not coincide with
    increased clock cycles and stable power consumption any longer.
    The resulting paradigmatic shift toward multi-processing left a deep mark on
    software development.
    New programming paradigms and languages, annotation systems, programming
    interfaces, libraries and compiler techniques were developed and continue to
    evolve.
    Despite immenense progress in the field, parallel computing remains an area
    of active research and automatic approaches often fell short.

    In recent years, it has become clear that multi-processing on its own has
    also reached its limits, and another change in direction is following
    swiftly.
    In response to the prevailing breakdown of both Dennard Scaling and Moore's
    Law, the hardware industry turns toward architectural innovation, trending
    away from general purpose processors and toward the use of specialised
    hardware and dark silicon.
    Accelerator processors have become widespread, first in the form of general
    purpose graphics processing units and now increasingly as deep learning
    accelerators, coinciding with the meteoric rise in popularity of
    convolutional neural networks due to the success of deep learning.

    This arrival of widespread heterogeneous computing is a natural and
    necessary reaction to the scaling limitations of homogeneous, general
    purpose processors.
    However, it poses an enormous challenge to the surrounding software
    ecosystem, and it puts into question many of the achievements in portability
    and longevity of programs that are taken for granted in modern computing.
    Existing software does not automatically benefit from entirely new
    acccelerator designs in the way that it profited from continuous
    microarchitectural improvements.
    Where previously, programs performed better on each succeeding hardware
    generation, with at most a recompilation required, new accelerators arrive
    with entirely novel and incompatible interfaces.

    \subsection*{The Diminished Role of Host Compilers}

    In particular, the novel hardware landscape greatly diminishes the
    scope of responsibilities and impact that traditional compilers for
    languages such as C, C++ and Fortran used to have.
    Were previously, such compilers were responsible for orchestrating
    program execution on the entirety of available computing resources, they are
    increasingly downgraded to merely coordinating the launch of core workloads
    in the form of computational kernels, which are executed as separate and
    opaque programs on heterogeneous accelerators.
    In order to reclaim their central position in the software ecosystem,
    compilers require fundamentally new approaches.

    An important distinction has to be made between {\em host compilers} and
    {\em kernel compilers} at this point, as these have developed quite
    distinctly.
    In fact, many of the kernel programs that remain opaque to the host compiler
    are themselved products of specialized kernel compilers.
    Kernel compilers successfully apply many of the techniques that have had
    only limited success for host compilers.
    Most importantly, they reason automatically about parallelism, understand
    data dependencies at a deeper level and they incorporate iterative
    compilation.
    This is made possible by a combination of factors that apply uniquely to
    kernel compilers.
    Smaller programs allow for more expensive compilation techniques;
    more restrictive languages and intermediate representaions allow for
    stronger reasoning;
    and domain knowledge from areas such as image processing can be directly
    embedded in custom compiler technology, without requiring their validity
    on generic programs.
    Furthermore, kernel compilers often run in relatively controlled
    environments, with less need for predictability, reproducability and
    stability.
    The scope of input programs might even be small enough to leave compilation
    to experts and vendors, which may share the results in the form of already
    compiled library functions.

    As host compilers operate under less forgiving conditions, it is
    unsurprising that they have lagged behind these developments.
    In particular, auto-parallelization has ultimately failed to conclusively
    solve the problems of parallel computing despite immense research efforts.
    This makes a traditional compiler approach to the even more complex
    problems of heterogeneous parallelism appear unfeasable.
    However, research can profit from the experiences with kernel compilers
    and from understanding why concepts from kernel compilers fail in
    host compilers.

    The distinction between kernel compiler and host compilers is gradual and
    reflected directly in the corresponding programming languages.
    They range from very specific kernel approaches such as Halide over hybrid
    approaches such as OpenCL and CUDA to the well established host compilers of
    languages such as C++, Java and Fortran.
    The more restricted the programming languages are, the more additional
    domain knowledge the compilers can utilise to outperform host compilers.


    \subsection*{Learning from Success Stories}

    In order to successfully target heterogeneous systems, host compilers need
    to leverage the know-how that already exists in the surrounding software
    ecosystem, encapsulated in special purpose libraries and code generators.

    Programming models built around libraries and novel domain specific
    languages, kernel compilers and library interfaces have succeeded in areas
    where established compilers continue to struggle - reaching peak performance
    on rapidly evolving and highly parallel hardware.
    Two examples stand characteristically for the readth of these successes.
    Firstly, implementations of the Basic Linear Algera Subprograms (BLAS)
    interfaces, a standard dating back to the 1970s, are available for most
    accelerators.
    They are widely used and offer unrivaled performance, with implementations
    from hardware vendors and from the scientific community.
    Competing implementations use a plethora of approaches to achieve as close
    to peak performance as possible.
    This includes manually written assembly code, but also highly advanced
    compilation techniques, novel program representations, polyhedral
    compilation, iterative compilation and many more.
    Secondly, the domain specific language Halide has developed for the domain
    of image processing.
    It has demonstrated an immense optimization potential for compiler based
    optimization under circumstances of highly constrained semantics.

    These success stories, however, still leave many important problems
    unaddressed.
    They are weak where approaches based on host compilers are strong:
    Adoption costs are significant and they are combined with often uncertain
    long-term prospects and very limited cross-platform portability.
    Even in the case of mostly agreed upon standards such as BLAS --
    arguably the best case scenario -- adoption of novel implementations is
    non-trivial in practice due to even slight differences in the interfaces.
    For academic-backed approaches like Halide on the other hand, complete
    rewrites are requires in entirely novel ecosystems with an entirely unclear
    future of support.

    What is promising therefore is a combination of hand-optimized libraries,
    domain specific languages together with compiler automatisms.
    This requires a disruptive improvement in compiler analysis capabilites.
    The detection of higher level algorithmic structures in compilers has been
    investigated before, but established approaches cannot scale to what
    this challenge requires.
    Syntactic matching directly on programming languages has become
    unviable for the complexities of both modern programming languages and
    complex code bases.

    Instead, this thesis develops an entirely novel approach based on concepts
    from constraint solving, building a pragmatic methodology for detecting
    complex algorithmic structures -- computational idioms.

    \subsection*{Leveraging the Existing Knowhow}
    Kernel compilers and libraries fundamentally can only reach their superior
    performance and capabilites as opposed to host compilers by leveraging the
    domain knowledge that they get from operating under constraint conditions.
    
    A linear alebra compiler operates under the assumption that the program is
    linear algbera, and the program representation that is used with not even
    allow the expression of any other system.

    Conceptually, the availability of dedicated linear algebra code generators
    means that host compilers should outsource code generation of linear algbera
    to these tools, because a generic approach will never reach the same level
    of sophistication.
    The crucial piece of technology that is missing, is for the compiler to
    automatically find opportunities to express user code in constraint
    domains.

    Conceptiully, some of this has been done before.
    Polyhedral compilers take as input arbitrary code, but they recognise whether
    parts of code are actually expresible in a much more constraint system.
    If possible, they reformulate the code into this system and continue
    optimization with domain knowledge, thus resulting in better code.
    This work proposes a generalization of this concept.

    \subsection{Constraints For the Rescue}
    The core to automatically making code heterogeneous is therefore to specialize it.
    When the compiler can classify code sections as adhering to a more constraint
    model than the generic SSA form, it can then apply domain specific optimization techniques.

    In the most extreme interpretation of this model, sections of code are
    recognised as implementing specific algorithms, like matrix multiplications,
    that are merely parameterized.

    Next are algorithms that allow parameterization with an operator.
    For example, stencil kernels of reductions fall into this catefory.

    More generic again are restricted programs that are nonetheless freely
    programmeable.
    Polyhedral code sections fall under this domain.

    All of these types of code restrictions can be formulated via constraints.
    This thesis will develop a framework to enable this, starting from
    theoretical foundations and resulting in a donain specific programming
    language. that is evaluated on well established benchmark programs.


\pagebreak
\section{Structure of the Thesis}

    This PhD thesis is divided into six chapters.
    Following the introduction in {\bf\cref{chapter:introduction}}, a broad
    overview of the related work is given in {\bf\cref{chapter:literature}}.
    This literature survey covers the four main research areas that this work
    is placed in.
    Firstly {\em constraint programming}, which is the basis of the methodoloy
    for most of this research.
    Secondly {\em compiler analysis and auto-parallelisation}, against which the
    results in this thesis are evaluated.
    Thirdly {\em heterogeneous computing}, which motivates many of the compiler
    approaches that this research enables.
    Lastly {\em computational idioms}, a term for several overlapping concepts
    of algorithmic patterns.

    {\bf\Cref{chapter:theory}} develops the foundations of the constraint
    programming methodology that is forms the core of the later
    \cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}.
    Based on a novel mathematical model of static single assignment (SSA) form
    compiler representaions, the core of the constraint programming approach
    is derived and some particular challenges are discussed in detail.

    {\bf\Cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}}
    are each based on a published research article and elaborate on different
    applications of constraint programming in compilers.

    {\bf\Cref{chapter:candl}} develops a full-fledged constraint programming
    language called CAnDL, with an implementation in the LLVM compiler
    infrastructure that automatically generates compiler analysis passes from
    declarative descriptions.
    Using CAnDL, the chapter explores severeal complex compiler analysis
    challenges, concluding with a full polyhedral code analysis.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:CDS:3178372.3179515}}.

    {\bf\Cref{chapter:reductions}} develops an auto-parallelising compiler for
    complex reduction and histogram computations using CAnDL-powered analysis
    functionality.
    This covers many computations that are inaccesible to established approaches
    based on data flow or polyhedral analysis and is to demonstrate the greater
    flexibility of constraint programming.
    The work of this chapter has been published as
    {\bf\citet{ginsbach2017discovery}}.

    {\bf\Cref{chapter:idioms}} extends CAnDL into the Idiom Description Language
    (IDL) and applies it to algorithmic concepts that go beyond traditional
    compiler analysis: stencil computations, complex reductions and histograms
    as well as sparse and dense linear algebra.
    The resulting automatic detection passes in LLVM enable automatic
    heterogeneous acceleration of sequential code and result in significant
    speedups on established benchmark suites.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:AML:3173162.3173182}}.

    {\bf\Cref{chapter:lilac}} builds on top of the idiom descriptions for linear
    algebra that were introduced in the previous chapter and demonstrates how
    compiler analysis based on constraint programming can be made accessible to
    non-experts.
    With the novel LiLAC language, constraint programs in IDL are automatically
    generated from a high-level specification for many different sparse linear
    algebra representations.
    This powers a fully integrated compiler approach, handling everything from
    algorithmic detection, library call insertion and run time data transfer
    optimisations.
    The work of this chapter has been published as \citet{lilacpaper}.
