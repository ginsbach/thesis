    For several decades, from the 1970s until the early 2000s, processor
    development followed Moore's Law and was able to exploit Dennard Scaling.
    Circuit density approximately doubled every two years, without significantly
    increasing power consumption.
    This had important implications for software development: Not only did the
    performance of computers improve exponentially, but these performance gains
    were also immediately available to already existing programs.
    The fundamental contract between software and hardware of mainstream
    processors - the instruction set architectures - evolved without
    paradigmatic changes.
    Software developers and users could trust on ever increasing clock cycles
    without any intervention

    This seemingly unending progress first broke down around 2006 with the
    apparent end of Dennard Scaling.
    The resulting paradigmatic shift toward multi-processing left a deep mark on
    software development.
    In recent years, it has become apparent that multi-processing as well has
    hit a wall and another paradigm shift is looming.
    With the end of both Dennard Scaling and Moore's Law, the hardware industry
    turned instead toward architectural innovation, with a trend away from
    general purpose processors and toward dark silicon and special purpose
    hardware.
    Accelerator processors have become widespread, first in the form of general
    purpose graphics processing units (GPGPU) and now increasingly with deep
    learning accelerators, that coincide with the meteoric rise of Convolutional
    Neural Networks and the enormous success of Deep Learning.

    Heterogeneous computing is the natural and necessary reaction to the scaling
    limitations of general purpose processors, but it poses an enormous
    challenge to the surrounding software ecosystem and it puts into question
    many of the achievements in portability and longevity of programs that are
    taken for granted in modern computing.
    As opposed to only microarchitectural improvements, existing software is not
    able to automatically profit from entirely new acccelerators designs.
    Where previously binary programs could profit immediately from succeeding
    hardware generations, and compilers were able to achieve portability across
    platforms, novel accelerators arrive with entirely new and incompatible
    application interfaces.

    The novel hardware landscape has greatly diminishmed the scope of influence
    of traditional compilers.
    Were previously, a standard C/C++ compiler was in orchestrating program
    execution on the entirely of available computing resources, it is
    increasingly downgraded to bookeeping, with computational kernels launched
    as separate programs on heterogeneous cores.
    In order to reclaim their central position in the software ecosystem,
    compilers require some fundamentally new approaches.
    While compilers have lagged behind the developments in the hardware domain,
    research into this area can profit from experience with multi-core
    processors.

    Despite immense research efforts, auto-parallelizing compilers have
    ultimately failed to conclusively solve any of the problems of parallel
    computing and have only had major success for specific small-scale numerical
    kernels in combination with auto-tuning.
    Heterogeneous computing adds another dimension of complexity on top of
    homogeneous parallelism, making a traditional compiler approach appear
    unfeasable.
    Instead, compilers need to think out of the box and incorporate novel ideas
    in order to leverage the know-how that has evolved in the surrounding
    software ecosystem.

    Programming models built around libraries and novel domain specific
    languages have had major succes in the areas that established compilers have
    been weakest - namely reaching peak performance on rapidly evolving
    accelerator hardware.
    At the same time, these approaches are weak where compilers are strong:
    they have significant adoption cost combined with often uncertain long-term
    prospects and very limited portability.
    What is promising therefore is a combination of hand-optimized libraries,
    domain specific languages together with compiler automatisms.

    This requires a disruptive improvement in compiler analysis capabilites.
    The detection of higher level algorithmic structures in compilers has been
    investigated before, but established approaches cannot scale to what
    this challenge requires.
    Syntactic matching on programming languages has become
    unviable for the complexities of both modern programming languages and
    complex code bases.

    Instead, this thesis develops an entirely novel approach based on concepts
    from constraint solving, building a pragmatic methodology for detecting
    complex algorithmic structures -- computational idioms.

\pagebreak
\section{Structure of the Thesis}

    This PhD thesis is divided into six chapters.
    Following the introduction in {\bf\cref{chapter:introduction}}, a broad
    overview of the related work is given in {\bf\cref{chapter:literature}}.
    This literature survey covers the four main research areas that this work
    is placed in.
    Firstly {\em constraint programming}, which is the basis of the methodoloy
    for most of this research.
    Secondly {\em compiler analysis and auto-parallelisation}, against which the
    results in this thesis are evaluated.
    Thirdly {\em heterogeneous computing}, which motivates many of the compiler
    approaches that this research enables.
    Lastly {\em computational idioms}, a term for several overlapping concepts
    of algorithmic patterns.

    {\bf\Cref{chapter:theory}} develops the foundations of the constraint
    programming methodology that is forms the core of the later
    \cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}.
    Based on a novel mathematical model of static single assignment (SSA) form
    compiler representaions, the core of the constraint programming approach
    is derived and some particular challenges are discussed in detail.

    {\bf\Cref{chapter:candl,chapter:idioms,chapter:reductions,chapter:lilac}}
    are each based on a published research article and elaborate on different
    applications of constraint programming in compilers.

    {\bf\Cref{chapter:candl}} develops a full-fledged constraint programming
    language called CAnDL, with an implementation in the LLVM compiler
    infrastructure that automatically generates compiler analysis passes from
    declarative descriptions.
    Using CAnDL, the chapter explores severeal complex compiler analysis
    challenges, concluding with a full polyhedral code analysis.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:CDS:3178372.3179515}}.

    {\bf\Cref{chapter:reductions}} develops an auto-parallelising compiler for
    complex reduction and histogram computations using CAnDL-powered analysis
    functionality.
    This covers many computations that are inaccesible to established approaches
    based on data flow or polyhedral analysis and is to demonstrate the greater
    flexibility of constraint programming.
    The work of this chapter has been published as
    {\bf\citet{ginsbach2017discovery}}.

    {\bf\Cref{chapter:idioms}} extends CAnDL into the Idiom Description Language
    (IDL) and applies it to algorithmic concepts that go beyond traditional
    compiler analysis: stencil computations, complex reductions and histograms
    as well as sparse and dense linear algebra.
    The resulting automatic detection passes in LLVM enable automatic
    heterogeneous acceleration of sequential code and result in significant
    speedups on established benchmark suites.
    The work of this chapter has been published as
    {\bf\citet{Ginsbach:2018:AML:3173162.3173182}}.

    {\bf\Cref{chapter:lilac}} builds on top of the idiom descriptions for linear
    algebra that were introduced in the previous chapter and demonstrates how
    compiler analysis based on constraint programming can be made accessible to
    non-experts.
    With the novel LiLAC language, constraint programs in IDL are automatically
    generated from a high-level specification for many different sparse linear
    algebra representations.
    This powers a fully integrated compiler approach, handling everything from
    algorithmic detection, library call insertion and run time data transfer
    optimisations.
    The work of this chapter has been published as \citet{lilacpaper}.
