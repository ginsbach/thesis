
    Four areas of research are of particular relevance to this thesis:
    {\bf Constraint programming and declarative languages} are central to the
    introduced methodology, with the relevant literature including the research
    into constraint programming in the context of program analysis and to a
    lesser extent the design of constraint programming languages.
    The survey of previous approaches to
    {\bf compiler analysis and auto-parallelisation}
    establishes the baselines for the later evaluation sections.
    Related work on {\bf heterogeneous computing} with its many different
    programming paradigmes to overcome the specific challenges of emerging
    hardware will serve to motivate the proposed approaches.
    Lastly, the diverse research landscape around concepts related to
    {\bf computational idioms} will put our idiomatic approach into context.

\section{Constraint Programming and Declarative Languages}

    Declarative Languages, constraint programming, and the application of
    constraints to program analysis problems are well-established in the
    literature.
    Previous work covers query languages, logic programming, applications to
    software security and formal verification, model checking and SMT,
    but also more compiler-centric dataflow analysis and type inference
    problems.
    The limited scope of this section requires a focus on work that is
    particularly relevant for this thesis.

    For this research, constraint programming is most interesting within the
    context of research fields such as program analysis and model checking.
    Crucial background material for this thesis also comes from
    the programming language design community of declarative programming
    languages.
    Prolog and its many extensions and dialects particularly stand out as
    fully-fledged logic programming languages, but parallels can also be drawn
    to querying languages that apply database techniques to static analysis.

    These different fields vary significantly in their interests, motivations,
    and approaches, but the underlying challenges are often similar.
    Notably, the performance of backtracking solvers and the scalability to
    complex problems are a recurring theme.

\subsection{Constraint Programming for Program Analysis}

    \paragraph*{On abstract programming languages}
    Constraint systems have long been used in program analysis.
    \citet{Aiken:1999:ISC:339853.339897} gives a comprehensive overview of
    earlier work.
    The article highlights the crucial ability of constraint-based program
    analysis to separate {\it constraint specification} from
    {\it constraint resolution}.
    This separation enables the scalability of compiler analysis problems also
    in this thesis beyond what could reasonably be implemented with manual
    recognition routines, by offloading the {\it constraint resolution} to an
    external solver.
    However, none of the techniques in the article capture higher-level
    algorithmic concepts.
    Instead, the focus is on classical purposes such as dataflow analysis and
    type inference.

    More recent work on constraint-based program analysis by
    \citet{Gulwani:2008:PAC:1375581.1375616} leverages the advancements in
    off-the-shelf SAT/SMT solvers.
    The analysis problems are lowered to bit-vector formulations, and the
    {\it constraint resolution} is entirely externalised to these separate
    tools.
    The motivation of the approach is mainly to verify program properties,
    as opposed to the aim of sophisticated parallelising code transformation in
    this thesis.
    Furthermore, \Cref{sec:SATcomp} already showed that the limitation to
    SMT backends is inefficient for the resolution of SSA constraint problems.

    \paragraph*{On real intermediate representations}
    There is particularly extensive work in the area of security and formal
    verification of software systems.
    This work includes investigations into using SMT solvers and theorem provers
    that operate on LLVM IR code, among them
    \citet{Zhao:2012:FLI:2103656.2103709}.
    However, the presented model of LLVM IR serves an entirely different purpose
    to the SSA model in this thesis.
    The model provides operational semantics, but cannot be used to detect
    large-scale algorithmic structures in user programs, as is required for
    automatic heterogeneous acceleration.
    Instead, the focus is on formally verifying the correctness of existing
    compiler transformations for all possible user functions.

    Recent domain-specific languages, such as Alive
    \citep{Lopes:2015:PCP:2737924.2737965} also operate on SSA intermediate
    representation.
    LLVM instructions are reformulated on bitvectors, and the
    correctness of conditions is checked with an SMT solver.
    However, Alive only implements a subset of LLVM's integer and pointer
    instructions.
    It has no support for control flow and does not scale to the real
    applications, such as those used in this thesis for evaluation.
    Instead, it is designed for formally verifying already existing
    compiler optimisations that operate on only a handful of integer
    instructions at a time.

    LifeJacket \citep{Notzli:2016:LVP:2931021.2931024} proves the correctness of
    floating point optimisations in LLVM, as does Alive-FP \citep{Menendez2016}.
    Both of these projects are extensons of the SMT-based Alive system.
    They extend the scope of suitable inputs by modeling additional instructions
    as bitvectors, but the fundamental limitations remain and control flow is
    not supported.
    Therefore, only peephole optimisations can be evaluated with these
    approaches.

    \paragraph*{On other program models}
    Other advanced approaches to extracting high-level structures from assembly
    and well structured Fortran code use temporal logic as the foundation of
    their analysis functionality \cite{Mendis2015Helium, Kamil2016Verified}.
    These techniques leverage counter-example guided inductive synthesis (CEGIS)
    to find provably correct translations into a high-level description
    language.
    This high-level language then specialised the code again, exploring the
    optimisation space via powerful optimisations that are enabled by its
    restrictive semantics.
    However, the focus is on a a more restricted set of computations, involving
    only dense memory access.
    While this allows formal reasoning about correctness, is too restrictive to
    model sparse linear algebra.

    \citet{Mullen:2016:VPO:2908080.2908109} verify low-level program
    transformations within the formally verified CompCert compiler
    \citep{CompCert-ERTS-2018}, directly on x86 assembly.
    The approach is fully verified, requiring approximately 30000 lines of
    manually written Coq code and proof lines.
    Further approaches to generating compiler optimisations that focus on formal
    verification instead of programmer productivity include Rhodium
    \citep{Lerner:2005:ASP:1040305.1040335}, PEC
    \citep{Kundu:2009:POC:1543135.1542513} and Gospel
    \citep{Whitfield:1997:AEC:267959.267960}.
    None of these approaches however, tackle the issue of efficiently enabling
    large-scale program transformations.
    Instead, they all focus on small-scale peephole optimisations.

\subsection{Delarative Programming Languages for Program Analysis}

    \paragraph*{Languages for querying program properties}
    From the perspective of language design, the declarative programming
    languages Prolog and SQL are perhaps most influential.
    The two languages differ fundamentally.
    Prolog is a logic programming language that originated in academia for
    analysing natural language \citep{Colmerauer:1993:BP:154766.155362}.
    By contrast, SQL was developed at IBM for managing data in
    relational database management systems
    \citep{Chamberlin:1974:SSE:800296.811515}.
    Nonetheless, specification languages for structures in program code have
    been proposed that take inspiration from both backgrounds.

    The first such specification language was the Omega system by
    \citet{Linton:CSD-83-164}.
    It uses a relational database to store all the relevant properties of a
    program.
    The captured information is based on the abstract syntax tree of programs
    that are implemented in a subset of the Ada programming language.
    Additional edges are inserted to connect common variables of
    successive expressions, given some indication of dataflow between
    instructions.
    The system then allows database-style queries formulated in QUEL
    \citep{Stonebraker:1976:DII:320473.320476}, an SQL-style language.

    The CodeQuest system \citep{Hajiyev:2006:CSS:2171327.2171331} combined the
    approach of Omega and its database-oriented successors with the use of logic
    programming.
    The queries are translated into Datalog, a Prolog derivative that is
    imlemented on top of SQL.
    This allows CodeQuest to be fundamentally more expressive, allowing
    recursive queries that are required for meaningful CFG inspection.
    Nonetheless, the approach is based on querying source langauge features.
    This makes the detection of large-scale algorithmic structures in complex
    programming languages such as C++ infeasible, as demonstrated in
    \Cref{sec:syntacticmatching}.

    \paragraph*{Languages for generating compiler passes}
    \citet{Martin1998} presents a specification language for program analysis
    functionality called PAG that is based on abstract interpretation.
    Domain specific languages for the conception of optimization passes have
    also been studied before using tree rewrites, among them
    \citet{Olmos:2005:CSD:2136624.2136643}.
    \citet{Lipps1989} propose the domain specific language OPTRAN for matching
    patterns in attributed abstract syntax trees of Pascal programs.
    These patterns can then be automatically replaced by semantically
    equivalent, more efficient implementations.
    Generic programming concepts can also be used to generate optimization
    passes, as demonstrated by \cite{Willcock:2009:RGP:1621607.1621611}.
    These schemes however are not able to work at the IR level essential for
    compiler implementation and do not scale beyond simple functions.

    Different to code transformation techniques are based on the LLVM ASTMatcher
    library and LibTooling \citep{be0fa11ddb194bde86a9dab8589b779c}.
    However, these tools are not standalone languages, but implemented as C++
    library functions that provide a sort of embedded domain-specific langauge
    for pattern matching.
    This method is extremely specific to the Clang compiler frontend, and
    exposes the abstract syntax tree of C++ code.
    For deteting complex algorithmic structures, this is an entirely impractical
    approach.

    Another language for implementing compiler optimisations from
    declarative specifications is OPTIMIX \citep{Assmann1996,Assmann98optimix}.
    Similarly to the presented work in \Cref{chapter:candl}, OPTIMIX emphasizes
    developer productivity.
    However, it is based on graph rewrite rules.
    OPTIMIX programs are compiled into C code that performs the specified
    transformation.
    A domain specific language for the generation of optimisation
    transformations was also used in the CoSy compiler \citep{Alt1994}.
    Both OPTIMIX and the CoSy method are simple rewrite engines that have no
    knowledge of global program constraints.

\section{Compiler Analysis and Auto-Parallelisation}

    The overarching motivation for constraint programming on SSA intermediate
    representation in this thesis is the automatic heterogeneous
    paralellelisation of sequential code.
    In order to evaluate this appropriately, traditional compiler analysis
    approaches are used for comparison.

\subsection{Polyhedral Compiler Approaches}

    The polyhedral model is a well established approach for modeling data
    dependencies that has been used by several compilers, with relevant
    publications by \citet{redon1994scheduling, jouvelot1989unified,
    chi1997optimizing, gupta2006simplifying, stock2014framework}.
    Polyhedral optimisers have been implemented in mainstream C/C++ compilers,
    notably are the Polly extensions to LLVM described in
    \cite{Doerfert2015Polly}.
    Recent work by \citet{7429301} has extended the polyhedral model beyond
    affine programs to some forms of sparsity with the PENCIL extensions.
    These can be used to model important features of sparse linear algebra, such
    as counted loops \citep{Zhao:2018:PCF:3178372.3179509}, meaning loops with
    dynamic, memory dependent bounds but statically known strides.
    Such loops are central to sparse linear algebra.
    It uses the PPCG compiler \citep{Verdoolaege:2013:PPC:2400682.2400713} to
    automatically detect relevant code regions, but it relies on well behaved C
    code with all arrays declared in variable-length C99 array syntax.
    Unfortunately such restrictions prevent application to real world programs;
    none of our benchmarks or data sets have this structure.

\subsection{Reduction Parallelism}


    Discovering and exploiting scalar reductions in programs has been studied
    for many years based on dependence analysis and idiom detection
    \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
    Early work focused on well structured Fortran and paid little attention to
    compiler-based detection, a notable exception being. 
    \cite{suganuma1996detection}. 
    In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
    approaches and developed a dynamic test to speculatively exploit reduction
    parallelism.
    Alongside this data dependence based approach, there has also been a
    large body of work exploring mapping of reductions in a polyhedral setting
    \cite{redon1994scheduling, jouvelot1989unified}.

    The treatment of more general reduction operations has received less
    attention.
    Work has focused on the exploitation of reductions rather than discovery
    \cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000},
    examining trade-offs in implementation \cite{yu2006adaptive} or exploitation
    of novel hardware \cite{ravi2010compiler,Huo2011HiPC}.
    In \cite{das2010experiences}, they use dynamic profile analysis to guide
    manual analysis and show there is potential for finding generalized
    reductions.
    In \cite{kim2012dynamic} they explore the use of dynamic analysis further,
    but state that detecting reductions on arrays is challenging.

    More recently, extensions to the polyhedral framework have been proposed,
    allowing it to capture reduction computations
    \cite{chi1997optimizing, gupta2006simplifying, stock2014framework}.
    Such efforts are described in \cite{Doerfert2015Polly}.
    The authors discuss and implement a reduction-enabled scheduling approach as
    part of Polly and use the Polybench benchmark suite to evaluate it,
    achieving speedups of up to 2.21x.
    However as shown in our evaluation section, such schemes are fragile in the
    presence of non static control flow.

    The difficulty in automatically detecting reductions has led to language or
    annotation based approaches where it is the user's responsibility to mark
    reductions in the program \cite{deitz2002high}.
    An annotation approach is described in \cite{Reddy2016Reduction}, based on
    the Platform-Neutral Compute Intermediate Language
    (\cite{baghdadi2015PENCIL}).
    This used the code generator in
    (\cite{Verdoolaege:2013:PPC:2400682.2400713}) to generate CUDA and OpenCL
    code for multiple compute platforms.

    Work has focused on exploitation rather than discovery
    \cite{Gutierrez:2000,gutierrez2003optimization,gutierrez2008analytical}, examining trade-offs in implementation \cite{yu2006adaptive}
     or exploitation of novel hardware \cite{ravi2010compiler,Huo2011HiPC}.
    Recent work \cite{ginsbach2017discovery} shows that more complex reductions can be 
    detected, but this is tied to  an  ad~hoc non-portable code generation phase. 



\subsection{Linear Algebra Compilers}

    Compiler management of indirect memory accesses was already studied in the
    1980s using an inspector-executor model for distributed-memory machines in
    \citet{Baxter:1989:RPS:72935.72967}.
    Here the location of read data was discovered at runtime and appropriate
    communication automatically inserted.
    Later work by \citet{pottenger1995idiom,fisher1994parallelizing,
    rauchwerger1999lrpd,suganuma1996detection} was focused on efficient runtime
    dependence analysis and the parallelization of more general programs.
    However, the performance achieved by these approaches is modest due to
    runtime overhead and falls well short of library performance.

\paragraph*{Idiom Detection}
    Idiom based optimization based on AST manipulation, as described by
    \citet{Pinter:1994:POP:177492.177494}, has fallen out of fashion.
    More systematic approaches based on static single assignment representation
    \cite{lattner2004llvm} and polyhedral representations
    \cite{benabderrahmane2010polyhedral} however were more recently
    investigated.
    They were largely based on syntactic pattern matching and not robust in the
    presence of complex control and dataflow.
    More recently, work by \citet{Andion2015Compilation} describes a compiler
    based parallelization approach for heterogeneous computing, based on an
    diomatic intermediate representation called KIR.  
    It is not clear how such an approach would work on general C/C++ programs.

\paragraph*{Polyhedral Approaches}
    Polyhedral compilers \cite{Baskaran:2010:ACC:2175462.2175482,Verdoolaege:2013:PPC:2400682.2400713} perform advanced loop optimizations and have been used for the generation of fast GPU kernels.
    More recently, extensions to the polyhedral framework have been proposed, allowing it to capture reduction computations \cite{chi1997optimizing, gupta2006simplifying, stock2014framework}.
    Such efforts are described in \cite{Doerfert2015Polly}, but they are fragile in the presence of non static control flow.

\section{Heterogeneous Computing}

    Heterogeneous computing has been a particularly active field of research
    since the widespread adoption of GPUs for general purpose computations in
    the last decade.
    This includes research from both software and hardware perspectives.
    The hardware research investigates the most promising directions of
    diversification for processors in heterogeneous systems.
    
    However, the related work in the context of this research is from
    the software perspective.
    Therefore, this section focuses on the different programming approaches that
    are championed for programming exiting heterogeneous hardware.
    These broadly fall into three categorised: library approaches,
    domain-specific languages, and compiler-based methods.

\paragraph*{Domain specific Languages}
    Domain specific languages have received much attention in recent years,
    ranging from SPIRAL \citep{ofenbeck13spiral}, a DSL for Fast Fourier
    Transforms, over Lift \citep{steuwer15rewrite, SteuwerRD17,HagedornSSGD18}
    to UFL \citep{Alnaes:2014:UFL:2594412.2566630}, a DSL for partial
    differential equations.
    Stencils in particular have received much attention
    \citep{Mullapudi:2015:PAO:2694344.2694364,HagedornSSGD18}, the best known of
    which is Halide \citep{Ragan-Kelley2013Halide}.
    DSLs to exploit complex reductions are less studied.
    \citet{Reddy2016Reduction} introduce a type of DSL via annotations
    that allow expression of complex reductions.
    This was based on the  Platform-Neutral Compute Intermediate Language by
    \citet{baghdadi2015PENCIL}.
    The Matrix multiplication idimos is well supported by
    specific libraries \citep{clblas,mkl,cublas}.

\paragraph*{Generation of Performance Portable Code for Heterogeneous Hardware}
    Recent research has highlighted the challenges of generating code that
    performs well on different heterogeneous hardware architectures.
    PetaBricks by \citet{PhothilimthanaARA13} is one of the first languages to
    address this performance portability challenge by encoding algorithmic
    choices which are then empirically evaluated and automatically taken by the
    compiler.
    Similarly \cite{MuralidharanRHG16} explores automatic selection of code
    variants using machine learning.
    In a similar spirit, Lift~\cite{steuwer15rewrite} uses rewrite rules to
    explore optimization choices automatically.

\paragraph*{Functional Code Generation Approaches}
    There exist multiple functional approaches for generating code for
    heterogeneous hardware.
    \citet{chakravarty11accelerating,mcdonell13optimising} propose Accelerate,
    a domain specific language embedded in Haskell that generates efficient GPU
    code.
    Recently, \citet{collins14nova} introduced NOVA, a new functional language
    targeted at code generation for GPUs, and Copperhead
    \cite{catanzaro11copperhead}, a data parallel language embedded in Python.
    Delite~\cite{brown11heterogeneous,chafi11domain} is a system that enables
    the creation of domain-specific languages using functional parallel patterns
    and targets multi-core CPUs or GPUs.
    In contrast to these approaches, we require no rewriting of legacy programs.

\subsection{Domain Specific Languages}

    Many domain specific languages have been proposed for efficient and
    performant programming of heterogeneous systems.
    They allow implementers to restrict the compiler and runtime away from
    general purpose programming concepts that are difficult to support on
    specific hardware.
    Domain specific languages can be stand alone with an entire tool chain and
    runtime ecosystem of be embedded in existing languages, such as C++ or
    Scala.
    DSLs range in complexity from onlymarginally more flexible than library
    interfaces to full-fledged programming languages such as OpenCL and CUDA.

    There have been multiple domain specific libraries proposed specifically
    for linear algebra computations.
    Many of these contain some degree of autotuning functionality to achieve
    good performance across different platforms.
    The Delite langauge of \citet{Sujeeth:2014:DCA:2601432.2584665} can be used
    as an intermediate representation to construct domain specific languages for
    heterogeneous platforms.
    The approach is tightly integrated with the Scala programming language
    and does not offer a readily available end-to-end solution.
    Halide, as proposed by \citet{Ragan-Kelley:2013:HLC:2499370.2462176}
    was designed for image processing, but is flexible enough to also allow the 
    formulation of matrix multiplications and other computations.
    \citet{Suriana:2017:PAR:3049832.3049863} demonstrates that this can extend
    to reduction computations as well.
    Its core design decision is the scheduling model that allows the separation
    of the computation schedule and the actual computation.
    There has been work on automatically tuning the schedules, e.g.\ 
    \citet{Mullapudi:2016:ASH:2897824.2925952}, but in general the computational
    burden is put on the application programmer.

    The LIFT programming language by \citet{Steuwer:2015:GPP:2858949.2784754}
    provides composable constructs from functional programming to implement
    data-parallel algorithms and operations.
    It is particularly suitable for dense linear algebra applications
    \citep{Steuwer:2016:MMB:2968455.2968521}, but extensions to sparsity exist.
%    \citep{b5f556d505d746109608f8db3cbce4ac}
    The MILK programming model \citep{Kiriansky:2016:OIM:2967938.2967948}
    extends C++ with pragmas to annotate indirect memory accesses.
    This allows low level optimizations that are applicable to sparse linear
    algebra.
    The authors report performance gains of up to 3x, but the approach is unable
    to utilize the much greater potential of heterogeneous compute and requires
    detailed programmer intervention.

\subsection{Libraries}

    The most established way of encapsulating fast linear algebra is via
    library implementations, generally based on the BLAS interface
    specification \cite{2002:USB:567806.567807}.
    These are generally very fast on their specific hardware platforms, but
    require application programmer effort and offer little performance portability.
    Implementations of dense linear algebra are available for most suitable
    hardware platforms, such as cuBLAS \cite{cublas} for NVIDIA GPUs, clBLAS
    \cite{clblas} for AMD GPUs and the Intel MKL library \cite{mkl} for Intel
    CPUs and accelerators.

    There are fewer implementations of sparse linear algebra, but they exist for
    the most important platforms, including cuSPARSE \cite{cusparse} for NVIDIA
    GPUs and clSPARSE \cite{clsparse} built on top of OpenCL.
    Several BLAS implementations attempt platform independent acceleration and
    heterogeneous compute, among them \citet{Wang:2016:BHP:2925426.2926256,
    10.1007/978-3-319-64203-1_33, Diego2017Multi}. 

%\paragraph{Graphs as sparse linear algebra}
%Graph processing is an important computational domain, especially in the context
%of large social media graphs \cite{5470687, 6408680}.
%The duality between graph processing and sparse linear algebra is well studied
%\cite{doi:10.1137/1.9780898719918} and has lead to the development of the
%GraphBLAS interface standard \cite{6670338,7761646}.
%GraphBLAS attempts to define a standard set of building blocks for graph
%algorithms that use the language of linear algebra.

\subsection{CPU-GPU data transfer optimisations}

    Data transfers between CPU and GPU have been studied extensively as
    important bottlenecks for parallelization efforts.
    Previous work \cite{Jablin:2011:ACC:1993316.1993516} established a system
    for automatic management of CPU-GPU communication.
    The authors of \cite{Lee:2009:OGC:1594835.1504194} implemented a system to
    move OpenMP code to GPUs, optimizing data transfers using data flow
    analysis.
    However, this approach performs a direct translation of the code, without
    optimizing it for the specific performance characteristics of GPUs.

%% EArly scalar detection sc
%Discovering and exploiting scalar reductions in programs has been
%studied for many years based on dependence analysis and idiom detection
% \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
%Early work focused on well structured Fortran and
%paid little attention to compiler-based detection, a notable exception being. 
%\cite{suganuma1996detection}. 
%In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
% approaches and developed  a dynamic test to speculatively exploit
% reduction parallelism.
%%%Early ploy detection
%Alongside this data dependence based approach, there has also been a
% large body of work exploring mapping of reductions in a
% polyhedral setting \cite{redon1994scheduling, jouvelot1989unified}
%
%%% Expoitation of genearlise reductiosns 
% The treatment of
%more general reduction operations has received  less attention
%Work has focused on exploitation rather than discovery
%\cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000}, examining trade-offs in implementation \cite{yu2006adaptive}
% or exploitation of novel hardware \cite{ravi2010compiler,Huo2011HiPC}
%In \cite{das2010experiences}, they use dynamic profile analysis to
%guide manual analysis and show there is potential for finding
%generalized reductions. In \cite{kim2012dynamic} they explore the use
%of dynamic analysis further, but state that detecting reductions on arrays is
%challenging.
%
%
%
%%% LLVM out there \cite{lattner2004llvm}
%%% What NAS:\cite{seo2011performance}
%
%%% Vaguely related C;
%
%
%%% New polyhedral approaches
%
%More recently, extensions
%to the polyhedral framework have been proposed, allowing  it to capture
%reduction computations \cite{chi1997optimizing, gupta2006simplifying,
%stock2014framework} Such efforts are described
%in \cite{Doerfert2015Polly}.  The authors discuss and implement a
%reduction-enabled scheduling approach as part of Polly and use the
%Polybench benchmark suite to evaluate it, achieving speedups of up to
%2.21x. However as shown in our evaluation section, such schemes are
%fragile in the presence of non static control flow.
%
%%% Language Poly and non-polly
% The difficulty in automatically detecting
%reductions has led to language or annotation based approaches where it
%is the user's responsibility to mark reductions in the
%program \cite{deitz2002high}.
%%% Another approach to incorporate the reduction idiom into the Polyhedral Framework using annotations is described in
%An annotation approach is described in 
%\cite{Reddy2016Reduction}, based on the Platform-Neutral Compute Intermediate Language (\cite{baghdadi2015PENCIL}).
%This used the code generator
%in (\cite{Verdoolaege2013Polyhedral}) to generate CUDA and OpenCL code for multiple compute platforms.
%
%
%
%%% New dynamic approaches
%There has also been recent work following on
%from \cite{rauchwerger1999lrpd} in using more aggressive
%speculation and dynamic analysis \cite{aguilar2015unified} to exploit reduction parallelism.
%The authors of \cite{Han2010Speculative} present an approach for the parallelization of a wide class of scalar reductions.
%They start from the observation that many reductions in real benchmark programs are not detected by current static analysis approaches.
%They propose a hardware assisted speculative parallelization approach for likely runtime reductions, denominated `partial reduction variables'.
%Candidates for speculative parallelization are determined by searching for update-chains in the data flow graph.
%The approach is evaluated on some of the SPEC2000 benchmarks using a simulator.
%They achieve up to $46\%$ speedup by including speculative reductions.
%This approach
%based on update chains in the dependence graph requires hardware
%speculation support to check dependences but is unable to detect
%histogram reductions.
%
%Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a complex
%system featuring compiler support and a runtime to enable speculative
%parallelization.  The core approach is the privatization of memory
%for each thread and an exception mechanism with recovery routines for
%accesses that violate parallelism.  The authors explicitly allow for
%reduction parallelism that involves only a single scalar associative
%and commutative operator.  The implementation approach is to first
%profile the program for hot loops and then to classify all variables
%accessed in those loops into different groups.  A transformation pass
%then identifies corresponding \texttt{malloc} and \texttt{free} calls
%and replaces them by thread local allocations.  In a similar way
%all\texttt{load} and \texttt{store} instructions are replaced by local
%accesses.  At runtime, the system uses manual page table switching and
%memory protection to minimize runtime overhead.  The evaluation is
%done on a limited set of five benchmark programs, yielding a geometric
%mean speedup of 11.4x on a 24 core machine.  The runtime overhead on
%these five programs varies between $<1\%$ and $>50\%$.  Despite this
%complexity they only exploit simple scalar reductions.
%
%In \cite{Andion2015Compilation}, it describes a compiler based
%parallelization approach for heterogeneous computing that is based on
%an idiomatic intermediate representation called KIR.  This
%intermediate representation is based on the concept
%of \mbox{diKiernels}, which constitute algorithmic building blocks and
%are used to automatically generate OpenMP and OpenHMPP code.  The
%authors propose a system that detects diKernels in conventional
%compiler IR and concatenates them to form contiguous sections of KIR.
%Individual examples of diKernels are scalar reductions and irregular
%assignments. It is not clear how such an approach would work on general
%'C' programs.
%

\section{Computational Idioms}

    The concept of computational idioms has been observed in different contexts
    and remains a rather vague concept.
    While terms such as \texttt{reduction}, \texttt{stencil} and
    \texttt{linear algebra} are commonly used, the concrete concepts can be
    surprisingly vague, although previous work has established several formal
    approaches.
    This chapter not try to create new formal definitions, but instead want to
    give an overview of the literature that sheds different perspectives on this
    topic.

    The basic observation is that software programs don't cover the space of
    possible programs evenly, instead, they tend to be structured among certain
    design principles.
    The same is true algorithmically and particularly for performance intensive
    applications.

\section{CGO related}


    There has also been recent work extending on \citet{rauchwerger1999lrpd}
    with more aggressive speculation and dynamic analysis
    \citep{aguilar2015unified} to exploit reduction parallelism.
    \citet{Han2010Speculative} present an approach for the
    parallelization of a wide class of scalar reductions.
    They start from the observation that many reductions in real benchmark
    programs are not detected by current static analysis approaches.
    They propose a hardware assisted speculative parallelisation approach for
    likely runtime reductions, denoted `partial reduction variables'.
    Candidates for speculative parallelization are determined by searching for
    update-chains in the data flow graph.
    The approach is evaluated on some of the SPEC2000 benchmarks using a
    simulator.
    They achieve up to $46\%$ speedup by including speculative reductions.
    This approach
    based on update chains in the dependence graph requires hardware
    speculation support to check dependences but is unable to detect
    histogram reductions.

    Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a
    complex system featuring compiler support and a runtime to enable
    speculative parallelization.
    The core approach is the privatization of memory for each thread and an
    exception mechanism with recovery routines for accesses that violate
    parallelism.
    The authors explicitly allow for reduction parallelism involving only a
    single scalar associative and commutative operator.
    The implementation approach is to first profile the program for hot loops
    and then to classify all variables accessed in those loops into different
    groups.
    A transformation pass then identifies corresponding \texttt{malloc} and
    \texttt{free} calls and replaces them by thread local allocations.
    In a similar way all\texttt{load} and \texttt{store} instructions are
    replaced by local accesses.
    At runtime, the system uses manual page table switching and memory
    protection to minimize runtime overhead.
    The evaluation is done on a limited set of five benchmark programs, yielding
    a geometric mean speedup of 11.4x on a 24 core machine.
    The runtime overhead on these five programs varies between $<1\%$ and
    $>50\%$.
    Despite this complexity they only exploit simple scalar reductions.

    \citet{Andion2015Compilation} describes compiler based
    parallelisation for heterogeneous computing that is based on
    an idiomatic intermediate representation called KIR.
    This intermediate representation is based on the concept of
    \mbox{diKiernels}, which constitute algorithmic building blocks and are used
    to automatically generate OpenMP and OpenHMPP code.
    The authors propose a system that detects diKernels in conventional compiler
    IR and concatenates them to form contiguous sections of KIR.
    Examples of diKernels are scalar reductions and irregular
    assignments.
    It is not clear how such an approach would work on general 'C' programs.
