
    Four areas of research are of particular relevance to this thesis:
    {\bf Constraint programming and specification languages} are central to the
    introduced methodology, with the relevant literature including the research
    into constraint programming in the context of program analysis and
    the design of specification languages.
    The survey of previous approaches to
    {\bf compiler analysis and auto-parallelisation}
    establishes the baselines for the later evaluation sections.
    Related work on {\bf heterogeneous computing} with its many different
    programming paradigms to overcome the specific challenges of emerging
    hardware will serve to motivate the proposed approaches.
    Lastly, the diverse research landscape around concepts related to
    {\bf computational idioms} puts the detected algorithmic structures in this
    thesis into context.

\section{Constraint Programming and Specification Languages}

    Declarative Languages, constraint programming, and the application of
    constraints to program analysis problems are well-established in the
    literature.
    Previous work covers query languages, logic programming, applications to
    software security and formal verification, model checking and SMT,
    but also more compiler-centric dataflow analysis and type inference
    problems.
    The limited scope of this section requires a focus on work that is
    particularly relevant for this thesis.

    For this research, constraint programming is most interesting within the
    context of research fields such as program analysis and model checking.
    Crucial background material for this thesis also comes from
    the programming language design community of declarative programming
    languages.
    Prolog and its many extensions and dialects particularly stand out as
    fully-fledged logic programming languages, but parallels can also be drawn
    to querying languages that apply database techniques to static analysis.

    These different fields vary significantly in their interests, motivations,
    and approaches, but the underlying challenges are often similar.
    Notably, the performance of backtracking solvers and the scalability to
    complex problems are a recurring theme.

\subsection{Constraint Programming for Program Analysis}

    \paragraph*{Constraints on abstract languages}
    Constraint systems have long been used in program analysis.
    \citet{Aiken:1999:ISC:339853.339897} gives a comprehensive overview of
    earlier work, highlighting the crucial ability of constraint-based program
    analysis to separate {\it constraint specification} from
    {\it constraint resolution}.
    This separation is critical also for this thesis, as it enables the
    scalability of compiler analysis problems beyond what could reasonably be
    implemented with manual recognition routines.
    The constraint specification can be brief by offloading the
    {\it constraint resolution} to an external solver.
    However, none of the techniques in the article capture higher-level
    algorithmic concepts like computational idioms.
    Instead, the focus is on more basic compiler analysis problems, such as
    dataflow analysis and type inference.

    More recent work on constraint-based program analysis by
    \citet{Gulwani:2008:PAC:1375581.1375616} leverages the advancements in
    modern off-the-shelf SAT/SMT solvers.
    The analysis problems are lowered to bit-vector formulations, and the
    {\it constraint resolution} is entirely externalised to these separate
    tools.
    The motivation of the approach is mainly to verify program properties,
    as opposed to the  sophisticated parallelising code transformation in
    this thesis.
    Furthermore, \Cref{sec:SATcomp} showed that the confinement to
    conventional SMT backends is inefficient for the resolution of
    SSA constraint problems.

    \citet{Kundu:2009:POC:1543135.1542513} use constraints to verify the
    correctness of program transformations with their system for
    Parameterized Equivalence Checking (PEC).
    This system is built on previous work on translation validation, checking
    the validity of transformations on concrete input programs by comparing the
    semantics before and after modification.
    PEC implements a hybrid approach that allows some aspects of the program
    to be underspecified, yet does not check the soundness of transformations
    in full generality, for all possible inputs.
    The checking is done via a custom solver for the generated constraint
    problems.
    The hybrid nature allows the system also to validate program transformations
    that significantly modify the control flow, e.g.\ loop unswitching.
    However, it cannot discover transformation opportunities, only verify them
    after the transformation was applied.

    \paragraph*{Constraints on compiler IR code}
    There is previous work on applying constraint-based program analysis on
    real compiler intermediate representations, mostly in the area of security
    and formal verification of software systems.
    This includes investigations into using SMT solvers that operate on LLVM IR
    code, which is the basis for the implementations in this thesis.
    \citet{Zhao:2012:FLI:2103656.2103709} build a model of LLVM IR for such
    solvers.
    However, this model serves an entirely different purpose to the SSA model
    in this thesis.
    The model provides operational semantics, but cannot be used to detect
    large-scale algorithmic structures in user programs, as is required for
    automatic heterogeneous acceleration.
    Instead, the focus is on formally verifying the correctness of existing
    compiler transformations for all possible user functions.

    Recent domain-specific languages, such as Alive
    \citep{Lopes:2015:PCP:2737924.2737965} also operate directly on LLVM IR.
    The individual instructions are reformulated on bitvectors, and the
    correctness of conditions is checked with an SMT solver.
    However, Alive only implements a subset of LLVM's integer and pointer
    arithmetic instructions.
    It has no support for control flow and does not scale to the real
    applications, such as those used in this thesis for evaluation.
    Instead, it is designed for formally verifying already existing
    compiler optimisations that operate on only a handful of integer
    instructions at a time.
    Alive is meant to improve compilers, not user programs.

    LifeJacket \citep{Notzli:2016:LVP:2931021.2931024} proves the correctness of
    floating-point optimisations in LLVM, as does Alive-FP \citep{Menendez2016}.
    Both of these projects are extensons of the SMT-based Alive system.
    They extend the scope of the system to model a much wider range of
    additional instructions as bitvectors, enabling the verification of more
    compiler optimisations.
    Botth LifeJacket and Alive-FP successfuly identify invalid optimising
    transformations in real compilers.
    However, the fundamental limitations from Alive remain, and control
    flow is not supported.
    Therefore, only peephole optimisations can be evaluated with these
    approaches.

    The Alive-Infer approach \citep{Menendez:2017:ADP:3062341.3062372} builds on
    Alive, but goes beyond the verification of existing optimisations.
    The tool uses an SMT solver to automatically generate preconditions that
    need to hold for transformations to be applied.
    This moves the Alive system away from verifying optimisations and
    closer to automatically detecting algorithmic structure in parts of user
    programs.
    However, Alive-Infer still requires the separate specification of the actual
    transformation.
    It just generates additional conditions and does not cover control flow.

    \paragraph*{Constraints on other program models}
    Other advanced approaches to extracting high-level structures
    program program using constraints and verification systems have been
    proposed.
    \citet{Mendis2015Helium, Kamil2016Verified} use temporal logic as the
    foundation to formulate the conditions for reformulating well-structured
    Fortran and assembly code in restrictive models.
    These techniques leverage counter-example guided inductive synthesis to find
    provably correct translations into the high-level description language
    Halide.
    The Halide compiler specialises the code again, exploring the
    optimisation space via powerful optimisations that are enabled by its
    restrictive semantics.
    However, the focus is on a a restricted class of computations, involving
    only dense memory access.
    This allows formal reasoning about correctness, but is too restrictive for
    interesting computational idioms, such as sparse linear algebra.

    \citet{Mullen:2016:VPO:2908080.2908109} verify low-level program
    transformations that are implemented for the formally verified CompCert
    compiler \citep{CompCert-ERTS-2018}, directly on x86 assembly.
    Instead of an automatic verification after modeling optimisations
    as SMT problems, the presented Peek system was checked with the interactive
    theorem prover Coq.
    This required approximately 30000 lines of manually written Coq code and
    proof lines.
    Some of the transformations consider rudimentary control flow constraints,
    but they cannot scale to computational idioms.

\subsection{Delarative Programming Languages for Program Analysis}

    \paragraph*{Languages for querying program properties}
    From the perspective of language design, the declarative programming
    languages Prolog and SQL are perhaps most influential.
    The two languages differ fundamentally.
    Prolog ("programmation en logique") is a logic programming language that
    originated in academia for analysing natural language
    \citep{Colmerauer:1993:BP:154766.155362}.
    By contrast, SQL ("Structured Query Language") was developed at IBM for
    managing data in relational database management systems
    \citep{Chamberlin:1974:SSE:800296.811515}.
    Nonetheless, specification languages for structures in program code have
    been designed that take inspiration from both backgrounds.

    The first such specification language was the Omega system by
    \citet{Linton:CSD-83-164}.
    It uses a relational database to store all the relevant properties of a
    program.
    The captured information is based on the abstract syntax tree of programs
    that are implemented in a subset of the Ada programming language.
    Additional edges are inserted to connect common variables of
    successive expressions, given some indication of dataflow between
    instructions.
    The system then allows database-style queries formulated in QUEL
    \citep{Stonebraker:1976:DII:320473.320476}, an SQL-style language.

    The CodeQuest system \citep{Hajiyev:2006:CSS:2171327.2171331} combined the
    approach of Omega and its database-oriented successors with the use of logic
    programming.
    The queries are translated into Datalog, a Prolog derivative that is
    imlemented on top of SQL.
    This allows CodeQuest to be fundamentally more expressive, allowing
    recursive queries that are required for meaningful CFG inspection.
    Nonetheless, the approach is based on querying source langauge features.
    This makes the detection of large-scale algorithmic structures in complex
    programming languages such as C++ infeasible, as demonstrated in
    \Cref{sec:syntacticmatching}.

    \paragraph*{Languages for generating compiler passes}
    Custom specification languages for generating compiler analysis and
    transformation passes have been presented in the literature.
    \citet{Martin1998} presented a specification language for program analysis
    functionality called PAG based on abstract interpretation.
    The generated functionality was integrated into C and Fortran compilers via
    a well-specified interface and applied succesfully to real benchmark codes.
    However, the tool is focused on relatively simple compiler optimisations
    such as constant propagation.

    Domain-specific languages for the conception of transformation passes
    include \citet{Olmos:2005:CSD:2136624.2136643}.
    The proposed Stratego system uses rewrite rules that apply tree
    transformations to the abstract syntax tree of the source program.
    However, this was only evaluated on the obscure Octave language, and the
    general applicability on large-scale applications remains unclear.

    \citet{Lipps1989} proposed the domain specific language OPTRAN for matching
    patterns in attributed abstract syntax trees of Pascal programs.
    These patterns can then be automatically replaced by semantically
    equivalent, more efficient implementations.
    With the focus on Pascal, it remains unclear how well the proposed concepts
    translate to the complex C++ programs with pointer calculations that were
    used for the evaluation of this thesis.

    Another language for implementing compiler optimisations from
    declarative specifications is OPTIMIX \citep{Assmann1996,Assmann98optimix}.
    Similarly to the presented work in \Cref{chapter:candl}, OPTIMIX emphasizes
    developer productivity.
    However, it is based on graph rewrite rules.
    OPTIMIX programs are compiled into C code that performs the specified
    transformation.
    A domain specific language for the generation of optimisation
    transformations was also used in the CoSy compiler \citep{Alt1994}.
    Both OPTIMIX and the CoSy method are simple rewrite engines that have no
    knowledge of global program constraints.

    Different code transformation techniques use LibASTMatchers
    and LibTooling \citep{be0fa11ddb194bde86a9dab8589b779c} from the
    LLVM project.
    These tools do not provide a standalone language, but LibASTMatchers is
    implemented in C++ as an embedded domain-specific language for pattern
    matching.
    The approach is very specific to the Clang compiler frontend, and exposes
    the abstract syntax tree (AST) of the compiler frontend directly.
    There are more than a thousand classes implementing different AST nodes in
    Clang, introducing considerable complexity for any non-trivial pattern.
    Therefore, this is an entirely impractical approach for deteting complex
    algorithmic structures such as computational idioms.

    \citet{Willcock:2009:RGP:1621607.1621611} propsed a complex system for
    generating generic optimisation passed using concepts from generic
    programming.
    However, such schemes are not able to work at the IR level of established
    compiler frameworks, such as LLVM.
    Instead, they require program rewrites by the user.

    \citet{Whitfield:1997:AEC:267959.267960} presented Gospel, a frameword and
    specification language for the exploration of properties of code-improving
    transformations.
    The project furthermore includes the Genesis tool, which automatically
    generates transformers as specified in Gospel.
    Several standard optimisations were implemented with Gospel and Genesis,
    such as constant folding and common subexpression elimination.
    Similar approaches to generating compiler optimisations from specification
    languages include Rhodium \citep{Lerner:2005:ASP:1040305.1040335}.
    The language expresses optimisations using explicit dataflow facts, which
    are manipulated by local propagation and transformation rules.
    The transformations are applied to a custom intermediate language and can
    be proven correct with a theorem prover.
    Neither Gospel nor Rhodium provide means to tackke the issue of efficiently
    enabling large-scale program transformations.

\section{Compiler Analysis and Auto-Parallelisation}

    The core motivation of the work in this thesis is the automatic
    heterogeneous paralellelisation of sequential code.
    The derived methods are evaluated on two metrics: how broadly they apply to
    real code, and how big their performance impact is when applied.
    In order to evaluate this appropriately, several other compiler analysis
    and parallelisation approachs are used for comparison.
    This section gives an overview of the the relevant research landscape.

\subsection{Compilation with the Polyhedral Model}

    The polyhedral model \citep{Karp:1967:OCU:321406.321418} is an established
    mathematical framework for modeling, analysing, and transforming
    well-behaved loop nests.
    Iterations in loop nests are treated as lattice points in a multidimensional
    grid.
    The iteration space can then be transformed with affine maps, potentially
    new uncovering parallelisation opportunities.
    This basic approach has been applied extensively in different compilers.
    Furthermore, the required conditions have been relaxed in different ways,
    allowing the application of the approach to a broader variety of input code.

    \paragraph*{Polly in LLVM}
    Polyhedral optimisers have been implemented as extensions to mainstream
    C/C++ compilers.
    Most notably, \citet{Lengauer2012Polly} presented the Polly extensions for
    LLVM.
    Polly reconises parts of LLVM IR that can be expressed in the polyhedral
    model and transforms then into that representation.
    Polyhedral optimisations can then be applied with PLuTo
    \citep{Bondhugula:2008:PAP:1375581.1375595}, before the model is
    translated back into optimised LLVM IR for further treatment by the core
    compiler.
    This enables the seamless application of polyhedral techniques to
    large-scale applications without source code changes via the many
    frontends of the LLVM infrastructure.
    However, this impacts only code that the tool can translate
    into a polyhedral representation.

    Polly-ACC \citep{polly-acc} is an extension to the Polly compiler that
    provides code generation for heterogeneous hardware.
    The tool uses the recognition functionality of standard Polly to detect
    code sections in LLVM IR that can be represented in the polyhedral model.
    These code sections are optimised with established polyhedral
    transformation techniques from \citet{Lengauer2012Polly}.
    However, the optimised polyhedral code sections are then translated into
    CUDA code to be executed on the GPU.
    This results in significant speedups of some benchmark programs, but the
    impact remains limited to code that fits the polyhedral model.

    \citet{Doerfert2015Polly} extend the applicability of polyhedral
    transformations within the Polly compiler to a broader set of input
    programs.
    Dependencies between iterations that originate from reduction variables
    cannot be eliminated with affine transformations.
    Therefore, they prohibit doall parallelism in a way that standard Polly is
    unable to resolve.
    However, the reduction-enabled scheduling approach for Polly can parallelise
    such loop despite the reduction dependencies.
    This significantly improved the speedup that Polly achieves on benchmark
    programs that contain reductions.

    \citet{Doerfert:2017:OLO:3049832.3049864} propose another approach to
    extending the applicability of polyhedral code transformations.
    The approach allows some conditions that are required for the the legal
    application of transformations to remain unproven at compile time.
    These conditions are then checked at runtime, providing a fall back to the
    original code when assumptions are not met.
    The checks that this work allows to be delayed include the absence of
    aliasing, finite loop boundaries, and in-bounds memory accesses.
    This allowed Polly to cover 3.9$\times$ as many loops in the SPEC and NPB
    benchmarks at a negligible runtime overhead.

    \paragraph*{Other polyhedral tools}
    The Polyhedral Parallel Code Generator (PPCG)
    \citep{Verdoolaege:2013:PPC:2400682.2400713} is a source-to-source compiler
    that takes sequential C programs as input and generates optimised CUDA
    kernels to target GPU acceleration.
    The extraction of polyhedral code sections from the C input is based on the
    Polyhedral Extraction Tool \citep{Verdoolaege12polyhedralextraction}.
    The extraction method can automatically detect relevant code regions, but it
    is implemented on syntax level and relies on purpose-built C code with all
    arrays declared in variable-length C99 array syntax.
    This is not robust enough to reliably cover larger programs from benchmark
    collections such as NPB or Parboild, which are used for evaluation in this
    thesis.

    C-to-CUDA \citep{Baskaran:2010:ACC:2175462.2175482} is another compiler that
    offers heterogneous acceleration of sequential C code by representing it in
    the polyhedral model.
    However, the focus is on code generation and the application of optimising
    transformations.
    The automatic recognition in the abstract syntax tree of parallel loops that
    can be represented in the polyhedral model remains ad-hoc and handles only a
    small set of benchmarks.

    Recent work by \citet{7429301} has extended the polyhedral model beyond
    affine programs to some forms of sparsity with the
    Platform-Neutral Compute Intermediate Language (PENCIL).
    This intermediate language is intended for heterogeneous systems and
    provides backends for accelerator programming.
    This platforms provides extensions, which can be used to model important
    features of sparse linear algebra, such as counted loops
    \citep{Zhao:2018:PCF:3178372.3179509}, meaning loops with dynamic, memory
    dependent bounds but statically known strides.
    Such loops are central to sparse linear algebra.
    However, the article by \citet{7429301} only proposes the language, a full
    implementation is not provided.

    Tiramisu \citep{Baghdadi:2019:TPC:3314872.3314896} is a polyhedral framework
    for targeting heterogeneous hardware, providing backends for CPUs, GPUs,
    distributed architectures and FPGAs.
    Optimisations are performed on four layers of intermediate representation,
    resulting in performance that almost matches dedicated library functions.
    However, the tool does not detect polyhedral code sections in existing
    source code.
    Instead, it requires the programmer to manually implement the algorithms
    with a dedicated C++ API.

    \citet{Zhang:2016:CTG:3018843.3018849} propose an extension to polyhedral
    approaches that allows the capturing of some sparse linear algebra
    calculations in the polyhedral model.
    They introduce a novel non-affine split transformation for this purpose.
    Using the inspector-executor model, the approach achieved significant
    speedup on some benchmark programs.
    However, the paper does not address the automatic recognition of sparse
    linear algebra routines within existing programs.
    Furthermore, the approach is not evaluated against state-of-the art
    library implementations such as Intel MKL and cuSPARSE.

    Many approaches have been proposed for parallelising loop nests with
    reduction variables in the polyhedral model, among them
    \citet{jouvelot1989unified,redon1994scheduling,chi1997optimizing,
    gupta2006simplifying,stock2014framework}.

\pagebreak
\subsection{Reduction Parallelism}

    Discovering and exploiting scalar reductions in programs has been studied
    for many years based on dependence analysis and idiom detection
    \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
    Early work focused on well structured Fortran and paid little attention to
    compiler-based detection, a notable exception being. 
    \cite{suganuma1996detection}. 
    In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
    approaches and developed a dynamic test to speculatively exploit reduction
    parallelism.

    The treatment of more general reduction operations has received less
    attention.
    Work has focused on the exploitation of reductions rather than discovery
    \cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000},
    examining trade-offs in implementation \cite{yu2006adaptive} or exploitation
    of novel hardware \cite{ravi2010compiler,Huo2011HiPC}.
    In \cite{das2010experiences}, they use dynamic profile analysis to guide
    manual analysis and show there is potential for finding generalized
    reductions.
    In \cite{kim2012dynamic} they explore the use of dynamic analysis further,
    but state that detecting reductions on arrays is challenging.

    The difficulty in automatically detecting reductions has led to language or
    annotation based approaches where it is the user's responsibility to mark
    reductions in the program \cite{deitz2002high}.
    An annotation approach is described in \cite{Reddy2016Reduction}, based on
    the Platform-Neutral Compute Intermediate Language
    (\cite{baghdadi2015PENCIL}).
    This used the code generator in
    (\cite{Verdoolaege:2013:PPC:2400682.2400713}) to generate CUDA and OpenCL
    code for multiple compute platforms.

    There has also been recent work extending on \citet{rauchwerger1999lrpd}
    with more aggressive speculation and dynamic analysis
    \citep{aguilar2015unified} to exploit reduction parallelism.
    \citet{Han2010Speculative} present an approach for the
    parallelization of a wide class of scalar reductions.
    They start from the observation that many reductions in real benchmark
    programs are not detected by current static analysis approaches.
    They propose a hardware assisted speculative parallelisation approach for
    likely runtime reductions, denoted `partial reduction variables'.
    Candidates for speculative parallelization are determined by searching for
    update-chains in the data flow graph.
    The approach is evaluated on some of the SPEC2000 benchmarks using a
    simulator.
    They achieve up to $46\%$ speedup by including speculative reductions.
    This approach
    based on update chains in the dependence graph requires hardware
    speculation support to check dependences but is unable to detect
    histogram reductions.

    Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a
    complex system featuring compiler support and a runtime to enable
    speculative parallelization.
    The core approach is the privatization of memory for each thread and an
    exception mechanism with recovery routines for accesses that violate
    parallelism.
    The authors explicitly allow for reduction parallelism involving only a
    single scalar associative and commutative operator.
    The implementation approach is to first profile the program for hot loops
    and then to classify all variables accessed in those loops into different
    groups.
    A transformation pass then identifies corresponding \texttt{malloc} and
    \texttt{free} calls and replaces them by thread local allocations.
    In a similar way all\texttt{load} and \texttt{store} instructions are
    replaced by local accesses.
    At runtime, the system uses manual page table switching and memory
    protection to minimize runtime overhead.
    The evaluation is done on a limited set of five benchmark programs, yielding
    a geometric mean speedup of 11.4x on a 24 core machine.
    The runtime overhead on these five programs varies between $<1\%$ and
    $>50\%$.
    Despite this complexity they only exploit simple scalar reductions.

\subsection{Linear Algebra Compilers}

    Compiler management of indirect memory accesses was already studied in the
    1980s using an inspector-executor model for distributed-memory machines in
    \citet{Baxter:1989:RPS:72935.72967}.
    Here the location of read data was discovered at runtime and appropriate
    communication automatically inserted.
    Later work by \citet{pottenger1995idiom,fisher1994parallelizing,
    rauchwerger1999lrpd,suganuma1996detection} was focused on efficient runtime
    dependence analysis and the parallelization of more general programs.
    However, the performance achieved by these approaches is modest due to
    runtime overhead and falls well short of library performance.

    \citep{Spampinato:2016:BLA:2854038.2854060}

\paragraph*{KIR}

    \citet{Andion2015Compilation} describes compiler based
    parallelisation for heterogeneous computing based on
    an idiomatic intermediate representation called KIR.
    This intermediate representation is based on the concept of
    \mbox{diKiernels}, which constitute algorithmic building blocks and are used
    to automatically generate OpenMP and OpenHMPP code.
    The authors propose a system that detects diKernels in conventional compiler
    IR and concatenates them to form contiguous sections of KIR.
    Examples of diKernels are scalar reductions and irregular
    assignments.
    It is not clear how such an approach would work on general 'C' programs.


\paragraph*{Idiom Detection}
    Idiom based optimization based on AST manipulation, as described by
    \citet{Pinter:1994:POP:177492.177494}, has fallen out of fashion.
    More systematic approaches based on static single assignment representation
    \cite{lattner2004llvm} and polyhedral representations
    \cite{benabderrahmane2010polyhedral} however were more recently
    investigated.
    They were largely based on syntactic pattern matching and not robust in the
    presence of complex control and dataflow.
    More recently, work by \citet{Andion2015Compilation} describes a compiler
    based parallelization approach for heterogeneous computing, based on an
    diomatic intermediate representation called KIR.  
    It is not clear how such an approach would work on general C/C++ programs.

\pagebreak
\section{Heterogeneous Computing}

    Heterogeneous computing has been a particularly active field of research
    since the widespread adoption of GPUs for general purpose computations in
    the last decade.
    This includes research from both software and hardware perspectives.
    The hardware research investigates the most promising directions of
    diversification for processors in heterogeneous systems.
    
    However, the related work in the context of this research is from
    the software perspective.
    Therefore, this section focuses on the different programming approaches that
    are championed for programming exiting heterogeneous hardware.
    These broadly fall into three categorised: library approaches,
    domain-specific languages, and compiler-based methods.

\subsection{Domain Specific Languages}

    Many domain specific languages have been proposed for efficient and
    performant programming of heterogeneous systems.
    They allow implementers to restrict the compiler and runtime away from
    general purpose programming concepts that are difficult to support on
    specific hardware.
    Domain specific languages can be stand alone with an entire tool chain and
    runtime ecosystem of be embedded in existing languages, such as C++ or
    Scala.
    DSLs range in complexity from onlymarginally more flexible than library
    interfaces to full-fledged programming languages such as OpenCL and CUDA.

    There have been multiple domain specific libraries proposed specifically
    for linear algebra computations.
    Many of these contain some degree of autotuning functionality to achieve
    good performance across different platforms.
    The Delite langauge of \citet{Sujeeth:2014:DCA:2601432.2584665} can be used
    as an intermediate representation to construct domain specific languages for
    heterogeneous platforms.
    The approach is tightly integrated with the Scala programming language
    and does not offer a readily available end-to-end solution.
    Halide, as proposed by \citet{Ragan-Kelley:2013:HLC:2499370.2462176}
    was designed for image processing, but is flexible enough to also allow the 
    formulation of matrix multiplications and other computations.
    \citet{Suriana:2017:PAR:3049832.3049863} demonstrates that this can extend
    to reduction computations as well.
    Its core design decision is the scheduling model that allows the separation
    of the computation schedule and the actual computation.
    There has been work on automatically tuning the schedules, e.g.\ 
    \citet{Mullapudi:2016:ASH:2897824.2925952}, but in general the computational
    burden is put on the application programmer.

    The LIFT programming language by \citet{Steuwer:2015:GPP:2858949.2784754}
    provides composable constructs from functional programming to implement
    data-parallel algorithms and operations.
    It is particularly suitable for dense linear algebra applications
    \citep{Steuwer:2016:MMB:2968455.2968521}, but extensions to sparsity exist.
    The MILK programming model \citep{Kiriansky:2016:OIM:2967938.2967948}
    extends C++ with pragmas to annotate indirect memory accesses.
    This allows low level optimizations that are applicable to sparse linear
    algebra.
    The authors report performance gains of up to 3x, but the approach is unable
    to utilize the much greater potential of heterogeneous compute and requires
    detailed programmer intervention.

    Spatial \citep{Koeplinger:2018:SLC:3192366.3192379}.

\paragraph*{Generation of Performance Portable Code for Heterogeneous Hardware}
    Recent research has highlighted the challenges of generating code that
    performs well on different heterogeneous hardware architectures.
    PetaBricks by \citet{PhothilimthanaARA13} is one of the first languages to
    address this performance portability challenge by encoding algorithmic
    choices which are then empirically evaluated and automatically taken by the
    compiler.
    Similarly \cite{MuralidharanRHG16} explores automatic selection of code
    variants using machine learning.
    In a similar spirit, Lift~\cite{steuwer15rewrite} uses rewrite rules to
    explore optimization choices automatically.

\paragraph*{Functional Code Generation Approaches}
    There exist multiple functional approaches for generating code for
    heterogeneous hardware.
    \citet{chakravarty11accelerating,mcdonell13optimising} propose Accelerate,
    a domain specific language embedded in Haskell that generates efficient GPU
    code.
    Recently, \citet{collins14nova} introduced NOVA, a new functional language
    targeted at code generation for GPUs, and Copperhead
    \cite{catanzaro11copperhead}, a data parallel language embedded in Python.
    Delite~\cite{brown11heterogeneous,chafi11domain} is a system that enables
    the creation of domain-specific languages using functional parallel patterns
    and targets multi-core CPUs or GPUs.
    In contrast to these approaches, we require no rewriting of legacy programs.

\paragraph*{Libraries}
    The most established way of encapsulating fast linear algebra is via
    library implementations, generally based on the BLAS interface
    specification \cite{2002:USB:567806.567807}.
    These are generally very fast on their specific hardware platforms, but
    require application programmer effort and offer little performance portability.
    Implementations of dense linear algebra are available for most suitable
    hardware platforms, such as cuBLAS \cite{cublas} for NVIDIA GPUs, clBLAS
    \cite{clblas} for AMD GPUs and the Intel MKL library \cite{mkl} for Intel
    CPUs and accelerators.

    There are fewer implementations of sparse linear algebra, but they exist for
    the most important platforms, including cuSPARSE \cite{cusparse} for NVIDIA
    GPUs and clSPARSE \cite{clsparse} built on top of OpenCL.
    Several BLAS implementations attempt platform independent acceleration and
    heterogeneous compute, among them \citet{Wang:2016:BHP:2925426.2926256,
    10.1007/978-3-319-64203-1_33, Diego2017Multi}. 

%\paragraph{Graphs as sparse linear algebra}
%Graph processing is an important computational domain, especially in the context
%of large social media graphs \cite{5470687, 6408680}.
%The duality between graph processing and sparse linear algebra is well studied
%\cite{doi:10.1137/1.9780898719918} and has lead to the development of the
%GraphBLAS interface standard \cite{6670338,7761646}.
%GraphBLAS attempts to define a standard set of building blocks for graph
%algorithms that use the language of linear algebra.

\paragraph*{CPU-GPU data transfer optimisations}

    Data transfers between CPU and GPU have been studied extensively as
    important bottlenecks for parallelization efforts.
    Previous work \cite{Jablin:2011:ACC:1993316.1993516} established a system
    for automatic management of CPU-GPU communication.
    The authors of \cite{Lee:2009:OGC:1594835.1504194} implemented a system to
    move OpenMP code to GPUs, optimizing data transfers using data flow
    analysis.
    However, this approach performs a direct translation of the code, without
    optimizing it for the specific performance characteristics of GPUs.

%% EArly scalar detection sc
%Discovering and exploiting scalar reductions in programs has been
%studied for many years based on dependence analysis and idiom detection
% \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
%Early work focused on well structured Fortran and
%paid little attention to compiler-based detection, a notable exception being. 
%\cite{suganuma1996detection}. 
%In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
% approaches and developed  a dynamic test to speculatively exploit
% reduction parallelism.
%%%Early ploy detection
%Alongside this data dependence based approach, there has also been a
% large body of work exploring mapping of reductions in a
% polyhedral setting \cite{redon1994scheduling, jouvelot1989unified}
%
%%% Expoitation of genearlise reductiosns 
% The treatment of
%more general reduction operations has received  less attention
%Work has focused on exploitation rather than discovery
%\cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000}, examining trade-offs in implementation \cite{yu2006adaptive}
% or exploitation of novel hardware \cite{ravi2010compiler,Huo2011HiPC}
%In \cite{das2010experiences}, they use dynamic profile analysis to
%guide manual analysis and show there is potential for finding
%generalized reductions. In \cite{kim2012dynamic} they explore the use
%of dynamic analysis further, but state that detecting reductions on arrays is
%challenging.
%
%
%
%%% LLVM out there \cite{lattner2004llvm}
%%% What NAS:\cite{seo2011performance}
%
%%% Vaguely related C;
%
%
%%% New polyhedral approaches
%
%More recently, extensions
%to the polyhedral framework have been proposed, allowing  it to capture
%reduction computations \cite{chi1997optimizing, gupta2006simplifying,
%stock2014framework} Such efforts are described
%in \cite{Doerfert2015Polly}.  The authors discuss and implement a
%reduction-enabled scheduling approach as part of Polly and use the
%Polybench benchmark suite to evaluate it, achieving speedups of up to
%2.21x. However as shown in our evaluation section, such schemes are
%fragile in the presence of non static control flow.
%
%%% Language Poly and non-polly
% The difficulty in automatically detecting
%reductions has led to language or annotation based approaches where it
%is the user's responsibility to mark reductions in the
%program \cite{deitz2002high}.
%%% Another approach to incorporate the reduction idiom into the Polyhedral Framework using annotations is described in
%An annotation approach is described in 
%\cite{Reddy2016Reduction}, based on the Platform-Neutral Compute Intermediate Language (\cite{baghdadi2015PENCIL}).
%This used the code generator
%in (\cite{Verdoolaege2013Polyhedral}) to generate CUDA and OpenCL code for multiple compute platforms.
%
%
%
%%% New dynamic approaches
%There has also been recent work following on
%from \cite{rauchwerger1999lrpd} in using more aggressive
%speculation and dynamic analysis \cite{aguilar2015unified} to exploit reduction parallelism.
%The authors of \cite{Han2010Speculative} present an approach for the parallelization of a wide class of scalar reductions.
%They start from the observation that many reductions in real benchmark programs are not detected by current static analysis approaches.
%They propose a hardware assisted speculative parallelization approach for likely runtime reductions, denominated `partial reduction variables'.
%Candidates for speculative parallelization are determined by searching for update-chains in the data flow graph.
%The approach is evaluated on some of the SPEC2000 benchmarks using a simulator.
%They achieve up to $46\%$ speedup by including speculative reductions.
%This approach
%based on update chains in the dependence graph requires hardware
%speculation support to check dependences but is unable to detect
%histogram reductions.
%
%Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a complex
%system featuring compiler support and a runtime to enable speculative
%parallelization.  The core approach is the privatization of memory
%for each thread and an exception mechanism with recovery routines for
%accesses that violate parallelism.  The authors explicitly allow for
%reduction parallelism that involves only a single scalar associative
%and commutative operator.  The implementation approach is to first
%profile the program for hot loops and then to classify all variables
%accessed in those loops into different groups.  A transformation pass
%then identifies corresponding \texttt{malloc} and \texttt{free} calls
%and replaces them by thread local allocations.  In a similar way
%all\texttt{load} and \texttt{store} instructions are replaced by local
%accesses.  At runtime, the system uses manual page table switching and
%memory protection to minimize runtime overhead.  The evaluation is
%done on a limited set of five benchmark programs, yielding a geometric
%mean speedup of 11.4x on a 24 core machine.  The runtime overhead on
%these five programs varies between $<1\%$ and $>50\%$.  Despite this
%complexity they only exploit simple scalar reductions.
%
%In \cite{Andion2015Compilation}, it describes a compiler based
%parallelization approach for heterogeneous computing that is based on
%an idiomatic intermediate representation called KIR.  This
%intermediate representation is based on the concept
%of \mbox{diKiernels}, which constitute algorithmic building blocks and
%are used to automatically generate OpenMP and OpenHMPP code.  The
%authors propose a system that detects diKernels in conventional
%compiler IR and concatenates them to form contiguous sections of KIR.
%Individual examples of diKernels are scalar reductions and irregular
%assignments. It is not clear how such an approach would work on general
%'C' programs.
%

\pagebreak
\section{Computational Idioms}

    The concept of computational idioms has been observed in different contexts.
    Software design patterns are a way of understanding program code
    strucutres as specialised implementation of a class of standard approaches
    in the discipline of software engineering.
    Terms such as {\em map and reduce}, {\em stencil code}, and
    {\em linear algebra} are commonly used when designing libraries and
    domain-specific languages.
    Scientific computing is mostly concerned with the architectural
    implications of specific memory access patterns that are instrinsic to
    the choice of certain algorithmic approaches.

    The concrete concepts are partially overlapping and can be vague.
    This section tries to demarcate a meaningful conception of
    {\it computational idioms} by comparison with the existing literature of the
    different domains with related concepts.
    The shared basic observation is that software programs don't cover the space
    of possible programs evenly.
    Instead, they tend to be structured among certain design principles.
    This is true in particular for performance intensive programs and the
    core bottlenecks of large applications.

\subsection{Higher-Order Functions}

    Functional programming languages -- such as OCaml or Haskell -- represent
    abstract algorithmic choices and common programming patterns as higher-order
    functions \citep{Hughes:1989:WFP:63410.63411}.
    These are functions that are parameterised with other functions.
    Examples of higher-order functions are {\it map}, which applies a function
    to each element in a data structure, and {\it fold} / {\it reduce}, which
    accumulates the elements in a data structure with a reduction operator.

    Common computational workloads can be expressed as instances of
    higher-order functions.
    For example, the popularity of the MapReduce framework
    \citep{Dean2008MapReduce} stems from the observation that many big data
    workloads exhibit characteristics that can be expressed efficiently with
    combinations of {\it map} and {\it reduce}.
    The framework provides an idiomatic approach to the development of big
    data applications, enabling shorter development times and more predictable
    performance.

    The use of computational idioms for automatic heterogeneous acceleration
    requires a more restrictive view of types than what is common in
    functional programming languages.
    For example, the {\it reduce} operator allows the implementation of
    the insertion sort algorithm, as well as a simple sum over an array of
    floating-point values.
    These two algorithms do not share parallelisation opportunities.
    Therefore, the detection of {\it reduce} instances is insufficient for
    enabling compiler parallelisation approaches.
    However, more restrictive version of {\it reduce} are suitable for
    compiler detection.
    \Cref{chapter:reductions} studies in detail the class of Complex Reduction
    and Histogram Computations, which is formulated as a computational idiom.
    This restricted class of {\it reduce} calculations shares a common
    parallelisation approach.

\subsection{Parallel Dwarfs}

    The {\it Berkeley Dwarfs} are a collection of 13 computational methods
    that together comprise a large portion of the most common parallel computing
    workloads \citep{Asanovic06thelandscape}.
    Each Dwarf is a computational pattern that is common in important
    applications.
    The core observation of the authors is that the nature of the dwarfs has
    persisted more or less identical for many years, even as concrete
    applications changed.
    The Dwarfs are inspired by numerical computations that arise in the
    scientific computing community, although the authors claim that the
    knowledge from this domain may prove useful in other areas as well.

    The dwarfs are studied from the perspective of architecture requirements,
    not with automatic compiler support in mind.
    Therefore, some of the dwarfs are specified too broadly for use as
    computational idioms in this paper.
    However, dense linear algebra, sparse linear algebra, and strcutured grid
    computations (stencils) are important idioms in \Cref{chapter:idioms}.

\subsection{Algorithmic Skeletons}

    Another abstraction that relates to computational idioms as used in this
    thesis is the notion of Algorithmic Skeletons \citep{Cole1991Algorithmic}.
    This concept was introduced to classify the behaviour of parallel programs
    according to their organisation of workload distribution.
    The motivation of this approach was to enable the introduction of new,
    higher-level programming models and tools for parallel programming.
    Higher-order functions from functional programming were a major inspiration,
    observing a lack of similar abstractions on more mainstream programming
    langauges.
    Among the Algorithmic Skeletons are Fixed Degree Divide \& Conquer and the
    Task Queue.

    The concept of Algorithmic Skeletons has been used to implement many
    programming frameworks and libraries.
    The eSkel library was sketched by \citet{Cole2004Bringing}, providing a
    higher-level programming model based on skeletons on top of C and MPI.
    Skandium \citep{Leyton2010Skandium} is a parallel skeleton library for
    multi-core architectures.
    Eden \citep{Loogen2005Parallel} provides skeletons for parallel programming
    in Haskell.
    SkelCL \citep{Steuwer2011SkelCL} provides implementations of algorithmic
    skeletons that target GPUs via CUDA.
    This is implemented in C++, providing templated versions of higher-order
    functions such as {\it map}, {\it reduce}, and {\it zip}.
    Finally, the Thread Building Blocks library
    \citep{Reinders2007Intel} implements Slgorithmic Skeletons.

    The definitions for Algorithmic Skeletons are not specified formally to
    enable automated reasoning, but are drafted for human understanding and to
    guide the design of libraries and DSLs.
    This abstraction level is similar to the Berkley Dwarfs.
    However, Algorithmi Skeletons were heavily inspired by higher-order
    functions and similarly describe the algorithmic structure of computations.
    This distinguishes them from the Berkeley Dwarfs, which are more focused on
    mathematical domains and architectural requirements.
