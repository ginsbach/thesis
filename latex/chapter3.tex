
    The research contributions of this thesis are based on a novel constraint
    programming approach on compiler intermediate representation.
    This chapter introduces the theoretical underpinnings of this approach,
    based on a mathematical model of static single assignment form (SSA).
    With the help of this model, concepts that are typically discussed on a
    programming language level are transferred onto the structurally simple,
    but semtantically expressive class of SSA compiler intermediate
    representations.

    Traditional abstract syntax tree (AST) pattern matching approaches here turn
    into more subtle constraint problems.
    This needs careful consideration to handle search space explosion, but at
    the same time enables much more powerful detection capabilities and the
    expression of higher level algorithmic structures that are impossible to
    define syntactically on programming language level.
    Later chapters will evaluate these claims in detail by building a complete
    constraint programming language on top of the theory in this chapter, and
    by then using it to solve real compiler analysis problems.

    After defining a mathematical foundation of SSA form and introducing
    formalisms for the expression of contraint problems on top of it, several
    standard definitions from compiler theory are then reformulated in the the
    framework.
    This includes common graph properties and the domination concepts as well
    as control flow structures such as single entry single exit blocks.
    All of this lies the foundations for the succeeding chapter.

    As a last part, a practical application of constraint programming to compiler
    intermediate representation requires efficient solver techniques and a
    section in the chapter will discuss strategies for limiting compile time
    explosions and explore analogies of the described model to standard
    Satisfiability Modulo Theory (SMT) problems.
    This will give further insights into performance improvements and put the
    work into a wider context.

\section{Introduction}

    During compilation, modern compilers for proceedural languages, such as
    C/C++, Fortran or JavaScript, typically use a range of different
    representations of the user program.
    They can be grouped into categories and reflect the requirements of
    the compilation stages they are used in.
    \linebreak
    {\bf Front end representations} reflect source language properties,
    often in the form of an AST, and contain information about syntactic choices
    of the programmer that are irrelevant to program logic.
    These representations scale in complexity with the source language and
    obscure the underlying semantics by not resolving complex language features
    such as operator overloading.
    \linebreak
    {\bf Back end representations} are based on a model of the target hardware.
    These representations typically approach an assembly style format that
    exposes specific instruction set architectures of the hardware;
    Back end representations are concerned with problems that are removed from
    the algorithmic core of the user program, such as register allocation and
    instruction scheduling.
    \\
    {\bf Static single assignment} (SSA) form has emerged as a suitable
    representation in the middle end, which is traditionally responsible for
    applying complex optimizing transformations.
    SSA abstracts away the complexities of both the source language and the
    target architecture, instead focusing on a relatively simple semantic
    description of the user program in oder to enable reliable analysis and
    platform independent reasoning.
    It is these properties that also make it the suitable target for expressing
    algorithmic structures.

    Static single assignment form is not a mathematically codified standard.
    The precise instruction set, syntax and type systems of the many different
    static single assignment form intermediate representations vary considerably
    depending on the requirements of the source languages (static or dynamic)
    and the operating constraints (just-in-time or ahead-of-time).
    Some prominent examples of compilers that utilize SSA for their
    optimisation passes are {\bf clang/clang++} (LLVM IR), {\bf gcc} (GIMPLE),
    {\bf v8 Crankshaft} (Hydrogen) and {\bf SpiderMonkey} (IonMonkey/MIR).
    Despite their many difference, they share the same fundamental paradigmes
    and we can mostly abstract away the differences as implementation specific
    details here.

    Fundamentally, a program in single static assignment form is made up of
    functions, that are represented as sequences of instructions, that are
    grouped into basic blocks and that operate on virtual registers.
    Control flow is handled via jump instructions at end of basic blocks.
    The virtual registers, within a function, can be assigned only at a single
    static location each, which we will see later has very useful implications
    and is the eponymous feature of SSA form.
    As opposed to syntactic representations, SSA form is thus a heavily
    searialized program representation, where expressions have been turned into
    lists of instructions and control flow structures into lists of basic
    blocks.
    The following sections will will discuss this in more detail and show how
    the serial nature and hence the enumerability of SSA makes constraint
    programming concepts very naturally applicable.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/ssaoverview}
\caption{Structural overview of SSA programs}
\label{fig:ssaoverview}
\end{figure}

\subsection{Static Single Assignment Form}

    SSA representations of programs are highly serialised, as shown in
    \autoref{fig:ssaoverview}:
    Programs are fundamentally lists of functions, functions are fundamentally
    lists of basic blocks and basic blocks are fundamentally lists of
    instructions.
    Functions and basic blocks are labelled, in order to be used as call or jump
    targets and functions have an argument list.

    Individual instructions operate on an abstract machine, which provides an
    unlimited number of identical registers and a well defined instruction set.
    Each instruction has a finite amount of input arguments and an opcode that
    refers to a specific operation to be performed.
    Instruction arguments can be registers, constants or globals and
    instructions can write their result into a single output register.
    Control flow is expressed as branch instructions, which direct the execution
    conditionally or unconditionally to other basic blocks.
    Branch instructions are only allowed at the end of a basic block and can
    only target the beginnings of basic blocks.

    The static single assignemnt property stipulates that within a function,
    no register can be written at more than a single static location, i.e.\ it
    is a function argument or is written by at most one of the instructions in
    the function.
    This makes the data dependencies between the instructions explicit and
    means that registers can be identified directly with the instructions that
    write to them.
    The registers themselves can therefore be considered implicit, with only the
    data flow between instructions required to recover them.

    In the presence of dynamic control flow behaviour in the program, most
    simply in the case of a conditional branch, the static single assignment
    property can only be maintained using phi instructions.
    These are particular pseudo-instructions that encapsulate assignments that
    can only dynamically be determined.

\section{Deriving a Mathematical Model}

\begin{figure}[p]
\centering
\includegraphics[width=\textwidth,height=1.5\textwidth]{figures/ssamathmodel.pdf}

\vspace{9.31595pt}
\caption{Compiler-generated SSA code is first decomposed into data flow, control
         flow and per-value attributes.
         Mathematical representations of the data are then introduced with
         notation.}
\label{fig:derivemaths}
\end{figure}

    This section derives a mathematical model of programs in single static
    assignemnt form.
    Using this model as a sound basis and mathematical notation, common compiler
    analysis problems are then reformulated.
    Later chapters will use this to define computational idioms formally, and to
    implement automatic compiler tools.
    It is not the aim here to introduce a formal operational semantics, or more
    generally to derive a model for the execution of SSA programs.
    Instead, the section will investigate the static structure, focusing on
    clear notation of the commonalities of existing SSA intermediate
    representations.

    The previous section showed how programs in SSA representations can be
    disected into several components, including control flow, data flow and
    instruction specifications.
    \autoref{fig:derivemaths} shows how this can be taken further in order to
    extract a more mathematical approach.
    At the top of the figure, different textual representations of a simple
    vector dot product are shown.
    The version of the top right is in an SSA intermediate representation: LLVM
    IR as generated by the clang compiler.

    In the middle column of \autoref{fig:derivemaths}, the information
    contained in the SSA representation is split into three components:
    Firstly, the we need a set of per-instruction properties, such as
    instruction opcodes, types and the values of constants that are used.
    Secondly, we capture the control flow graph.
    Thirdly, we capture the detailed data flow graph of the program.
    This data structure contains information about how the results of previous
    instructions are used as arguments to successive instructions.
    As we discussed in the previous section, the SSA property ensures that this
    is enough information to make the concrete register usage implicit.

    We can see this \autoref{fig:derivemaths}.
    The single static assignment property implies that it is statically knows
    at each point, which instruction produced the value in each register.
    Therefore, we can immediately identify registers with their producting
    instruction.
    Therefore we can entirely capture the interaction between instructions in
    two graphs: the control flow graph and the data flow graph.
    This is shown in in middle section of the figure.
    The data flow graph entirely replaces the concept of registers and instead
    models directly how the results from instructions are used as arguments
    in succeeding instructions.
    The control flow graph models the possible paths through the program.

    Note that we entirely removed the concepts of basic blocks and registers
    here.
    However, both can be recovered easily from the graph representations and
    hence we lost no information going from the textual representation towards
    the graph representations.

    Finally, we can model this entirely mathematical.
    We can see at the bottom of the figure, how we can model the entire function
    body as a tuple
    \begin{align*}
        \mathcal{F}=(T_\mathcal{F},I_\mathcal{F},C_\mathcal{F},P_\mathcal{F},G_\mathcal{F},CDG_\mathcal{F},DFG_\mathcal{F})\text{.}
    \end{align*}

    Note that the same model can be used abstractly for other single static
    assignment forms, only the type and opcode information as encoded via
    $Types_\text{LLVM}$ and $Opcodes_\text{LLVM}$ are specific to LLVM and need
    to be replaced.

\begin{definition}{Mathematical model of SSA programs}{ssamodel}
    An {\em SSA model} of a program is a tuple
    \begin{align*}
        \mathcal{F}=(T_\mathcal{F},I_\mathcal{F},C_\mathcal{F},P_\mathcal{F},G_\mathcal{F},CDG_\mathcal{F},DFG_\mathcal{F})
    \end{align*}
    comprising a {\em type model}
    $T_\mathcal F\subset\mathbb N\times Types$,
    an {\em instruction model}
    $T_\mathcal F\subset\mathbb N\times Opcodes$,
    a {\em constant model}
    $C_\mathcal F\subset\mathbb N\times\mathbb R$,
    a {\em parameter model}
    $P_\mathcal F\subset\mathbb N$,
    a {\em data flow model}
    $$ and a control flow model $$.

\end{definition}


\section{Constraint Programming}

    With a mathematical strucure in place, properties of SSA form programs can
    now be formulated as constraint problems.
    The notation for this will be as follows: An SSA constraint problem
    $C=(V,P)$ is made up of a finite set of {\em variables} $V$ and a boolean
    predicate $P$ such that
    \begin{align*}
        P\colon\mathbb N^V\times\mathcal M\mapsto\{\text{true}, \text{false}\}
    \end{align*}
    where $\mathcal M$ is the set of SSA programs.
    We can then define the set of {\em constraint solutions} for a given
    constraint problem and a specific function in SSA form $\mathcal F$ as
    \begin{align*}
        S(C, \mathcal F) = \{s\in\mathbb N^V\mid P(s,\mathcal F)=\text{true}\}.
    \end{align*}
    The interesting aspect of this defition is now the structure of $P$.
    In particular, this section will concern it self with how meaningful
    predicates can be composed from small building blocks.
    This internal structure will then also lead to efficient solver approaches.

\subsection{Example}
    Consider the task of detecting in a program all simple loop counters.
    Such loop counters show up in LLVM IR as data flow cycles of a phi node
    and an add instruction.
    We express the constraint problem $C=(V,P)$ with a set of three variables
    $V=\{\text{phi}, \text{update}, \text{step}\}$ and the following predicate:
    \begin{align*}
        P(x,\mathcal F)=&(x_\text{phi},x_\text{update},1)\in DFG_\mathcal{F}\land\\
                        &(x_\text{step},x_\text{update},2)\in DFG_\mathcal{F}\land\\
                        &(x_\text{step}, phi)\in I_\mathcal{F}\land\\
                        &(x_\text{step}, add)\in I_\mathcal{F}\land\\
                        &x_\text{step}\in C_\mathcal{F}^*\land\\
                        &(x_\text{update},x_\text{phi},1)\in DFG_\mathcal{F}
    \end{align*}


\subsection{Computational of SSA Constraint Problems}

    Predicates for useful SSA constraint problems are composed by logical
    connectors from a set of basic {\em atomic} predicates.
    The atomic predicates are defined directly on the mathematical model and
    they often only utilize a very small set of variables, typically one or two.
    In the previous example, this was true for all the atomic constraints.

    Via the logical connectors $\land$ and $\lor$, the atomic predicates
    interact on their shared variables.

    There are some important compiler analysis problems that can not not be
    decomposed with logical connectors into atomic predicates using only one
    or two variables.
    They are mostly concerned with graph properties and we will discuss some of
    them in detail later in this chapter.
    For the purposes of constraint solving, these are more difficult to handle.

\subsection{Efficient Solving of SSA Constraint Problems}

    Assuming that the SSA model of a program is represented in some data
    structure and a function that implements the boolean predicate $P$ of a
    SSA constraint problem $C=(V,P)$, it is not obvious how to compute the set
    of constraint solutions.
    This is because the set $\mathbb N^V$ is infinite and hence a brute force
    algorithm that evaluates $P$ on every point in $\mathbb N^V$ would never
    terminate.

    Instead, an algorithmic solution requires an intelligent way to encode the
    boolean predicate.
    It is intuitive and well established in literature to use backtracking for
    solving constraint problems, so firstly this will be reformulated
    recursively as follows.
    The finite set of $V$ can be enumerated $V=\{v_0,v_1,\dots,v_n\}$.

    Then consider $C_k=(V_k,P_k)$, where $P_k$ is defined as
    \begin{align*}
        P_k(x)=\left\{\begin{array}{}\text{true} if P(x,y)=true for some y\\
                                     \text{false} otherwise\end{array}\right.
    \end{align*}

    Consider the following purely algebraic reformulation.
    \begin{align*}
        S_k=\{\emptyset\}\\
        S_{k+1}(C,\mathcal F)=&\{s\in\mathbb N^V_k\mid P(s,\mathcal F)=\text{true}\}\\
                             =&\{
    \end{align*}


\subsection{Important Graph Properties}

    With our established notation, we can now transfer standard compiler
    analysis problems into this more formal language.
    Most of these are based on graph theoretic considerations, so we
    will firstly need to recapitulate some graph theory basics.
    Firstly, there is the notion of {\em cuts} of graphs, that we will introduce
    here in a hybrid version of edge based and vertex based modelling.

    \begin{definition}{Connections and Cuts}{def:cuts}
        Consider an adjacency set $E\subset\mathbb{N}\times\mathbb{N}$ of a
        directed graph and let $a,b\in\mathbb{N}$.
        \newline
        A {\em connection} between $a$ and $b$ in $E$ is a subset
        $A\subset\mathbb{N}$ such that a finite sequence $c_1,\dots,c_n$
        exists with
        \begin{gather*}
            a=c_1\hspace{1cm}c_2,\dots,c_{n-1}\in A\hspace{1cm}b=c_n\\
            (c_k,c_{k+1})\in E\hspace{1em}\text{for all}\hspace{1em}k=1,\dots,n-1.
        \end{gather*}
        A {\em cut} between $a$ and $b$ in $E$ is a subset $B\subset E$
        such that no {\em connection} between $a$ and $b$ in $E\setminus B$
        exists.
        We define the {\em set of cuts} between $a$ and $b$ in $E$ as
        \begin{align*}
            \text{Cuts}_E(a,b):=\{B\subset E\mid B\text{ is {\em cut} between $a$ and $b$ in $E$}\}
        \end{align*}
    \end{definition}

    These notions are quite intuitive, two vertices in a graph have a connection
    if one can reach the other via the available edges and by ``cutting'' these
    edges, they are no longer connected.

    These definitions are very useful in order to identify crucial properties of
    data and control flow graphs.
    Most standard is the the definition of a dominator in the control flow
    graph: An instruction $d$ is said to dominate another instruction $n$ if
    every path from the entry node to $n$ through the control flow graph must
    go through $d$.
    In our model this is of course equivalent to the following:

    \begin{definition}{Dominator}{def:dominator}
        Consider an instruction $n$ in a function $\mathcal F$.
        A {\em dominator} of $n$ in $\mathcal{F}$ is an instruction $d$ such
        that $\{(d,m)\mid(d,m)\in CFG_\mathcal{F}^*\}$ is a {\em cut} between $1$ and $n$ in $CFG_\mathcal{F}^*$.
    \end{definition}

    Another important definition is the concept of control dependence.
    Control dependence models the behaviour of conditional control flow.
    Instructions that are executed only in some control flow paths are control
    dependent on the conditional branches that preceed them.

    \begin{definition}{Control Dependence}{cdg}
        Consider instructions $a,b$.
        We say that an $b$ is control dependent on $a$ if a instructions
        $c,c'$ exist such that $(a,c),(a,c')\in CFG_\mathcal{F}^*$ and
        \begin{align*}
            \{(a,c)\}\in{}&{}\text{Cuts}_E(a,b)\\
            \{(a,c')\}\notin{}&{}\text{Cuts}_E(a,b)\text{.}
        \end{align*}
        We define the {\em control dependence graph} as follows
        \begin{align*}
            CDG_\mathcal{F}:=\{(a,b)\in\mathbb{N}^2\mid b\text{ control dependent on }a\}
        \end{align*}
    \end{definition}

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth,height=1.5\textwidth]{figures/schaubild2.pdf}

    \vspace{27.38136pt}
    \caption{Computation of the control dependence graph.}
    \label{fig:pdg}
\end{figure}


\subsection{Control Dependence Example}

    The control dependence graph is a function of the control flow graph, as is
    directly apparent from \autoref{def:cdg}.
    We can see how an example control dependence graph is computed in
    \autoref{fig:pdg}, from the control flow graph of the \texttt{dot} function
    in \autoref{fig:derivemaths}.
    From the definition it is immediately obvious that we need to only consider
    conditional branches as origins of control dependence.

    We can consider the two conditional branches $2$ and $16$ independently.
    On the right, we consider only $2$.
    We check the defining property: On the top of the figure, all the
    instructions that are not reachable from $2$ without the edge $(2,4)$ are in
    grey.
    Below this, all instructions not reachable from $2$ without $(2,3)$ are
    grey.
    We see that $4,5$ are always reachable and $1$ is never reachable, these are
    therefore not control dependent on $2$.
    All the other instructions are control dependent on $2$.

    Once we have computed this for all conditional branches, we take the union
    on graphs and get the complete control dependence graph of the function.
    Note what this graph represents:
    Once the loop in the function has been unrolled, it contains a conditional
    and a loop.
    Eveything within the body of the conditional is control dependent on $2$.
    Everythig within the loop as well as everything afterwards is control
    dependent on $16$.

\subsection{Phi Dependence Graph}

    Phi nodes are fundamental in single static assignment form and need special
    care.
    The value that a phi node takes depends on from where a phi node was
    reached.
    We need to encapsulate this in a graph.

    \begin{definition}{Phi Dependence Graph}{def:pog}
        Let $p$ a phi node and $c$ a conditional branch instruction.
        We say that the outcome of $p$ depends on $c$ if there is a branch
        instruction $b$ that reaches $p$ such that $b$ is control dependent on
        $c$.

        This defines the {\em phi dependence graph} $\Phi DG_\mathcal{F}$.
    \end{definition}

\subsection{Program Dependence Graph}

    After the control flow, data flow and control dependence graph, we lastly
    introduce the {\em program dependence graph}.
    It is the most exhaustive tool that we have to describe how values depend on
    each other.

    \begin{definition}{Program Dependence Graph}{def:pdg}
        The {\em program dependence graph} is defined as the union of data flow
        and control dependence graphs.
        \begin{align*}
            PDG_\mathcal{F}:=DFG_\mathcal{F}^*\cup CDG_\mathcal{F}^*\cup\Phi DG_\mathcal{F}\text{.}
        \end{align*}
    \end{definition}

    With the program dependence graph, we can now define subsections of the
    program that are self-contained and can be separated into their own
    function.
    This works even if they contain complicated control flow.
    Firstly, we need a definition of an interface.

    \begin{definition}{Interface}{def:interface}
        Let $a\in CFG_\mathcal{F}^*$ and $b_1,\dots,b_n\in DFG_\mathcal{F}^*$.
        Furthermore let $A\subset\mathbb{N}$ a set of instructions.

        We say that $(b_1,\dots,b_n)$ is an interface to $A$ if it is a cut
        between $o$ and $A$ in $PDG_\mathcal{F}$ for any of the following $o$:
        \begin{itemize}
            \item $o$ is a paramter
            \item $o$ is impure
        \end{itemize}
    \end{definition}


\newpage
\subsection{Interface Example}

    We will now consider a non-trivial example.
    Consider this snppet of C code, implementing a function that performs a
    simple square root approximation on each element in an array of double
    precision floating point values.

\begin{figure}[ht]
\begin{lstlisting}[language=C]
void map_sqrt(size_t length, double* array)
{
    for(int i = 0; i < length; i++)
    {
        double root = 1.0;
        for(int i = 0; i < 10; i++)
            root = 0.5*(root+array[i]/root);

        array[i] = root;
    }
}
\end{lstlisting}
\caption{{\bf map$\circ$sqrt}: Apply an appriximate sqare root function to each
         element in a vector.}
\end{figure}

    Coneptually, we should be able to disentangle the square root function from
    the control flow of the outer loop.
    This is possible with the preceeding definition of {\em interfaces}.


\newpage
In single static assignment form, this code looks as follows:

\begin{lstlisting}[language=LLVM]
define void @map_sqrt(i64, double*) {
  %3 = icmp eq i64 %0, 0
  br i1 %3, label %5, label %4

; <label>:4:
  br label %6

; <label>:5:
  ret void

; <label>:6:
  %7 = phi i64 [ %10, %8 ], [ 0, %4 ]
  br label %12

; <label>:8:
  %9 = getelementptr double, double* %1, i64 %7
  store double %19, double* %9
  %10 = add nuw i64 %7, 1
  %11 = icmp eq i64 %10, %0
  br i1 %11, label %5, label %6

; <label>:12:
  %13 = phi i64 [ 0, %6 ], [ %20, %12 ]
  %14 = phi double [ 1.0, %6 ], [ %19, %12 ]
  %15 = getelementptr inbounds double, double* %1, i64 %13
  %16 = load double, double* %15
  %17 = fdiv double %16, %14
  %18 = fadd double %14, %17
  %19 = fmul double %18, 5.0
  %20 = add nuw nsw i64 %13, 1
  %21 = icmp eq i64 %20, 10
  br i1 %21, label %8, label %12
}
\end{lstlisting}

    In this example, the set $\{\%9\}$ is an interface to $\{(\%19,\%store)\}$.

\section{Formulating Constraint Problems}

    With this mathematical background, it is now possible to derive constraint
    programming on top of LLVM code.

    Consider the following definitions of simple binary predicates:

    \begin{align*}
     is\_branch\_inst(\mathcal F, n):= (n,\text{\bf br})\in T_\mathcal{F}\\
     is\_control\_edge(\mathcal F, n, m):= (n,m)\in CFG_\mathcal{F}^*\\
     is\_control\_dom(\mathcal F, n, m):= \\
    \end{align*}

    We can then use these predicates to define more complex constructs, such as
    single entry, single exit (SESE) regions.

    \begin{definition}{}{}{}
        A single entry single exit region is a tuple $a,b,c,d\in\mathcal N$ such
        that the following properties hold:
        \begin{align*}
            is\_control\_edge(\mathcal{F},a,b)\\
            is\_control\_edge(\mathcal{F},c,d)\\
            is\_control\_dom(\mathcal{F},c,d)\\
            is\_control\_postdom(\mathcal{F},d,c)\\
        \end{align*}
    \end{definition}