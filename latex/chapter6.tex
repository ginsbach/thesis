
    The previous chapter introduced a computational idiom --
    Complex Reduction and Histogram Computations (CReHCs) -- and showed how the
    Idiom Description Language (IDL) enables its discovery and automatic
    parallelisation.
    This chapter takes a broader view of automatic idiom detection that also
    includes common sparse and dense linear algebra computations and stencil
    codes.
    Instead of implementing bespoke parallelisation passes, the chapter
    follows the vision laid out in the introduction, using the detection results
    to relevant program parts into stronger models.
    Existing tools are then used to leverage the implied domain knowledge.

    The goal of this chapter is to take abstract algorithmic concepts, which are
    conventionally explored outside the context of compiler analysis --
    computational idioms -- and to formalize them as IDL specifications,
    enabling detection and manipulation in optimising compilers.
    Heterogeneous acceleration will serve as the motivation for this effort.
    As many scientific codes are structured around idiomatic performance
    bottlenecks, efforts that focus on computational idioms can greatly
    improve performance, especially with accelerators that were designed
    with similar computations in mind.
    The focus is therefore on calculations that are well supported by
    accelerators and their software ecosystems: linear algebra,
    stencil codes and CReHCs.

    The IDL specifications are used to build a prototype compiler that
    automatically detects the idioms and uses them to circumvent the code
    generator with libraries and domain specific languages:
    BLAS implementations, cuSPARSE, clSPARSE, Halide and Lift.
    The funcionality is directly accessible in a modified version of the widely
    used Clang C/C++ compilers.
    The evaluation can therefore be performed on the well established benchmark
    suites NAS and Parboil, where 60 idiom instances are detected.
    In those cases where idioms are a significant part of the sequential
    execution time, the generated heterogeneous code achieves 1.26$\times$ to
    over 20$\times$ speedup on integrated and external GPUs.

\section{Introduction}

    Heterogeneous accelerators provide the potential for superior performance.
    However, realising this potential in practice is difficult and requires
    significant programmer effort.
    Programs have to be partially rewritten to target heterogeneous systems,
    using a diverse range of broad and narrow interfaces.
    General-purpose languages such as OpenCL \citep{nvidia11guide} provide
    some portability across heterogeneous devices, but the achieved performance
    often disappoints \citep{lee09openmp}.
    Despite the functional portability, rewrites are required in practice to
    achieve competitive performance.
    Optimised numerical libraries provide more reliable performance, but they
    are more specialised and often provided by hardware vendors without
    portability in mind \citep{clblas,cublas,cusparse,clsparse}.
    More narrow domain-specific languages (DSLs) have been proposed by
    \citet{Ragan-Kelley2013Halide,Franchetti09OL,Rompf:2012:LMS:2184319.2184345}
    among others in attempts to deliver both portability and performance.
    However, this class is quickly evolving, and with most of these DSLs being
    academic projects, the adoption and long term support remain unclear.

    Hardware is becoming increasingly heterogeneous, most recently with the
    development of deep neural network accelerators such as the Google TPU
    \citep{jouppi2017datacenter}.
    This means that library or DSL based programming is likely to become far
    more common.
    The problems with this trend that arise due to the aforementioned tradeoffs
    are evident:
    Firstly, application developers have to learn multiple specialised DSLs and
    vendor-specific libraries if they want good performance.
    Secondly, they will have to rewrite their existing applications to use them.
    Thirdly, this ties code into an ecosystem with unclear future support
    that might soon become obsolete.
    This situation is a severe impediment to the wide-spread efficient
    exploitation of heterogeneous hardware.

    The ideal would be a compiler that automatically maps existing code to
    heterogeneous hardware, with full performance and requiring no directions
    from the application programmer.
    While this is unrealistic in general, the chapter presents a system that
    approximates such a general-purpose system by utilising know-how that is
    already available and encapsulated in the existing backend interfaces.
    Instead of implementing code generation for each heterogeneous
    accelerator, the system maps user code to heterogeneous hardware using the
    existing libraries and domain-specific languages, effectively outsourcing
    the code generation to hardware and domain specialists.

    The approach is based on detecting specific {\em computational idioms} in
    application code that correspond to the functionality of existing interfaces
    -- libraries and DSLs -- for heterogeneous acceleration.
    In addition to the Complex Reduction and Histogram Computations (CReHCs)
    introduced in \Cref{chapter:reductions}, the focus is on
    sparse and dense linear algebra, as well as stencils.
    The covered idioms are a reflection of both the most relevant program
    bottlenecks and the available interfaces.
    Some computational idioms are more widely supported than others, potentially
    pointing to gaps in the accelerator landscape, but some backend was
    available for each of them.
    The Idiom Detection language (IDL) then enabled the automatic detection of
    idioms.

    Once detected, the idioms are mechanically translated into the appropriate
    DSL or replaced with a library call.
    This optimised code, or the pre-built optimised library, is then linked into
    the original program.
    As backends, the libraries cuSparse, clSparse, cuBLAS, clBLAS for
    sparse and dense linear algebra and the DSLs Halide
    \cite{Ragan-Kelley2013Halide} and Lift \cite{SteuwerRD17} were used in the
    evaluation.
    The Lift language is a data-parallel functional language that supports
    generalised reductions as well as stencils and linear algebra.
    The wide range of backends allows the freedom to target many APIs per idiom
    and pick the implementation that best suits the target platform.

    New computational idioms can be easily added thanks to the flexibility of IDL.
    This also provides a powerful means of determining whether a proposed
    heterogeneous interface matches existing code, without touching the core
    compiler.
    The idioms addressed in this paper can be expressed in less than 500 lines
    of IDL code.
    The approach is also highly robust, has been applied to the entire NPB
    and Parboil benchmark suites and was evaluated on three platforms.

    The chapter presents a novel approach that:
    \begin{itemize}
    \item Proposes the Idiom Description Language (IDL) for detecting idiomatic
          code sections that can be accelerated by domain-specific backends.
    \item Implements several common computational idioms in IDL to automatically
          discover opportunities for accelerator exploitation.
    \item Efficiently translates and maps the detected idioms to APIs for
          heterogeneous systems.
    \end{itemize}

    The work most similar in approach discovers stencil computations and maps to
    the Halide DSL for acceleration.
    The Helium tool \citep{Mendis2015Helium} recovers stencils from
    image-processing binaries.
    This requires large scale dynamic analysis of binary traces and replacing
    them with Halide calls. 
    This is significantly extended by \citet{Kamil2016Verified}, detecting
    stencils in Fortran code.
    The focus of that work is on inferring post invariants based on syntax
    guided synthesis in translation to Halide.
    However, it uses a narrow approach to selecting code snippets and relies on
    well-structured Fortran with occasional user annotations.
    The IDL approach is distinct in its use of an external programming language
    for the flexibility of describing arbitrary idioms.
    This allows an unbounded set of idioms to be considered and is not
    restricted to stencils. 

    To summarise, this chapter presents an automatic approach that discovers
    idioms in legacy code and maps them to heterogeneous platforms via libraries
    and DSLs.
    The tool was applied to 21 C/C++ programs from the NPB and Parboil benchmark
    suites, where it detected more reductions, stencils, matrix multiplications
    and sparse matrix-vector computations than existing schemes.
    For the programs where idioms dominate execution time, accelerator code
    was generated and evaluated on 3 platforms: a multi-core CPU, an integrated
    APU, and an external GPU.
    Overall, 60 idioms were detected, and they dominated execution time in
    10 programs.
    Speedup results for the accelerated code ranged from 1.26$\times$ to over
    20$\times$.

\section{Overview}

    The approach is automatic and has been implemented inside the LLVM compiler
    infrastructure.
    It takes arbitrary sequential C/C++ programs as input.
    Using the Clang compiler, the input source code is compiled into a Static
    Single Assignment (SSA) intermediate representation (LLVM IR).
    In this representation, the specified idioms are identified and replaced
    with calls to specific APIs.
    Finally, the code generated by the LLVM compiler and the output of the idiom
    specific code generators/libraries are linked together into a binary,
    producing an optimised program.
    LLVM was chosen as it is the best supported SSA-based compiler;
    the methodology could easily be transferred to other infrastructures such as
    GCC.

\subsection{Compiler Flow}

    The structure of the approach is described in more detail in
    \Cref{fig:methodology}.
    The compiler takes two programs as inputs: the first is the source code of
    the user program, the second describes the computational idioms that are to
    be detected specified in IDL.
    The same idioms, of course, can be detected across many user programs.
    Therefore, the IDL program does not have to change from one run to the next.

\begin{figure}[p]
    \centering
    \includegraphics[width=\linewidth]{figures/compiler_flow.pdf}
    \caption{Workflow of the IDL acceleration system:
             The IDL solver extracts idiomatic loops in the optimised LLVM IR
             of user programs.
             These loops are extracted and replaced with shim function calls.
             Domain-specific code generators implement the calls as library
             objects, or they are taken directly from pre-generated vendor
             libraries for idioms not containing kernel functions.}
    \label{fig:methodology}
\end{figure}

    The source code of the user program is compiled into optimised LLVM IR code,
    and the idiom description is parsed and represented internally as a C++
    object, as previously described in \Cref{chapter:theory},
    \Cref{subsec:impl}.
    The C++ representation of the constraints and the LLVM IR code
    are then passed as inputs to the backtracking solver, which detects all
    cases where the idioms can be found in the LLVR IR.

    The recognised idioms and the LLVM IR code are then passed on to the
    transformation phase of the system.
    The sections of code corresponding to computational idioms are extracted
    and then reformulated for the appropriate heterogeneous backends.
    For libraries, this means replacing the code covered by the idiom with a
    library call. 

    For DSL interfaces, the process is a little more involved.
    The user code captured by idioms is extracted and replaced with functions
    calls to shim interfaces.
    The extracted code features are translated into the appropriate DSL and
    passed on to the external DSL compiler, which optimises it and generates a
    library object that implements the required function interfaces.
    The translation to DSL mainly involves representing the kernel functions.
    Idioms without kernel functions correspond to fixed DSL programs and
    require no additional work.
    The generated code is then linked with the object code from the main program.

    Determining the best heterogeneous APIs to use for a given platform and the
    best idioms to exploit will become an essential consideration as the number
    of idioms and APIs grows.
    Currently, for this chapter, all applicable libraries and DSLs were
    evaluated, and the best-performing versions selected after profiling.

\subsection{Accelerating Sparse Linear Algebra}

    Sparse linear algebra is central to many scientific codes and increasingly
    important as a basis for graph algorithms and data analytics
    \cite{Kepner2015GraphsMA}, but they contain indirect data access that limits
    compiler optimisation.
    Instead, programmers rely on library implementations that are hand optimised
    and utilise accelerator hardware.
    This reliance on libraries comes at a cost, however, as it ties programs
    into vendor-specific software ecosystems and results in non-portable code.
    The IDL approach offers an alternative by recognising sparse linear algebra
    operations in compiler intermediate representation and then incorporating
    domain-specific backends without source code changes.

    The code at the top of \Cref{fig:spmvexample1} is the performance
    bottleneck of the ``Conjugate Gradient'' benchmark program in the NAS
    Parallel Benchmarks, with the corresponding LLVM~IR code underneath.
    This bottleneck loop implements a standard operation from sparse linear
    algebra, namely the multiplication of a sparse matrix in
    Compressed Sparse Row (CSR) format with a dense vector.
    This computation is supported on accelerator hardware, using well-optimised
    libraries such as cuSPARSE and clSPARSE.
    However, compilers are unable to recognise and accelerate the computation
    automatically.

    The structure of this sparse linear algebra computation has several features
    that make it unsuitable for most established compiler optimisations:
    Firstly, the iteration domain of the nested loop is memory dependent
    (line 3), and secondly, there is indirect memory access (line~4).
    This makes the iteration domain of the loop nest as a whole non-polyhedral
    and the access structure to memory non-affine.
    Under these conditions, simple data dependence models, but also
    sophisticated analysis based on the polyhedral model, fail.

    IDL can express this sparse idiom, as derived in \Cref{sec:idioms},
    \Cref{csr_lilacwhat_fig}.
    The ``Conjugate Gradient'' LLVM IR code, together with the ``SPMV-CSR''
    IDL specification, are input to the constraint solver, which outputs a
    constraint solution, as shown in \Cref{fig:spmvexample2}.
    In the solution, different values for the LLVM IR have been assigned to all
    IDL variables in the ``SPMV-CSR'' specification.

    \Cref{fig:spmvexample3} shows how this solution is used to generate a
    call to a cuSPARSE procedure.
    The individual solution variables are inserted into the {\tt cusparseDcsrmv}
    code template as function arguments. 
    The original code is then cut out and replaced with this function call.
    In practice, this involves a shim function that manages the device context
    and memory transfers from and to the GPU.
    Finally, the cuSPARSE library is linked with the object code produced by the
    Clang compiler, resulting in a speedup of $17\times$ on a GPU as described
    in more detail in\Cref{sec:idiomresults}.

    Central to this approach is the ability to detect computational idioms
    reliably.
    The next section derives in detail the formulation of linear algebra -- both
    sparse and dense -- and stencils in the Idiom Detection Language.

\begin{figure}[p]
    \input{latex/figure_spmvexample}
\end{figure}

\section{Specification of Idioms in IDL}
\label{sec:idioms}

    The specification of computational idioms in IDL requires a careful
    handling of the arising complexity, using the modularity functionality that
    IDL provides.

    Control flow constructs, memory access patters as well as basic data flow
    patterns are specified independently and then combined together in order to
    define the complete constraint specifications.

\subsection{Sparse Linear Algebra}
At the heart of our approach is a simple language to specify sparse and dense
linear algebra operations.
This serves two purposes in our LiLAC system: Firstly, it is used to generate
a detection program for finding the computation in user code.
Secondly, it identifies the variables that are arguments to the library, thus
defining the harness interface.

The key challenge in the design of this language was to stay simple enough
to allow automatic generation of robust detection functionality, yet to be able
to capture interesting functionality.
Crucial for sparse linear algebra routines is capturing the many different
memory access patterns, the control flow on the other hand is very rigid.

\subsection{Sparse Matrix Variations in LiLAC-What}
Sparse matrices can be stored in different formats.
In this section we introduce two of them and show how LiLAC-What can express
the corresponding computations.

\begin{figure}[p]
\lstset {
 basicstyle=\linespread{1.0}\small\ttfamily
}
\begin{lstlisting}[language=IDL, label={spmvbase}, caption=
   {Skeleton of the sparse matrix-vector product (SPMV) constraint
    specification in IDL: The precise sparse access patterns are specific to
    chosen storage formats for sparse matrices.}]
Constraint SPMV
( inherits For at {outer_loop} and
  inherits DotProductFor at {inner_loop} and
  {outer_loop.begin} strictly
      control flow dominates {inner_loop.begin} and
  {outer_loop.end} strictly
      control flow post dominates {inner_loop.end} and
([\dots])
\end{lstlisting}
\end{figure}

\begin{figure}[p]
\hfill
\begin{minipage}[b]{0.3\linewidth}
\includegraphics[width=0.95\linewidth]{figures/csrorder.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
\begin{align*}
\text{\bf val} =& \begin{bmatrix}
1\ \ 1\ \ 2\ \ 2\ \ \text{-}1\ \ 3\ \ 2\ \ 2\ \ \text{-}1\ \ 1\\
\end{bmatrix}\\[4mm]
\text{\bf col\_ind} =& \begin{bmatrix}
0\ \ 2\ \ 1\ \ 3\ \ 1\ \ 2\ \ 3\ \ 3\ \ 2\ \ 4\\
\end{bmatrix}\\[4mm]
\text{\bf row\_ptr} =& \begin{bmatrix}
0\ \ 2\ \ 4\ \ 7\ \ 8\ \ 10\\
\end{bmatrix}
\end{align*}
\end{minipage}
\hfill

\vspace{0.8em}
\hrule
\vspace{0.3em}

\includegraphics[width=\linewidth]{figures/spmvcsrwhat.pdf}
\vspace{-1.5em}
\lstset {
 basicstyle=\linespread{1.0}\small\ttfamily
}
\begin{lstlisting}[language=IDL,firstnumber=7]
([\dots])
  inherits VectorStore
      with {outer_loop}          as {scope}
       and {outer_loop.iterator} as {input_index} at {output} and
  inherits VectorRead
      with {outer_loop}          as {scope}
       and {inner_loop.src1}     as {value}
       and {inner_loop.iterator} as {input_index} at {val} and
  inherits VectorRead
      with {outer_loop}      as {scope}
       and {inner_loop.src2} as {value}
       and {col_ind.value}   as {input_index} at {vector} and
  inherits VectorRead
      with {outer_loop}          as {scope}
       and {inner_loop.iterator} as {input_index} at {col_ind} and
  inherits ReadForLoopRanges
     with {outer_loop}          as {scope}
      and {inner_loop}          as {for}
      and {outer_loop.iterator} as {input_index} at {row_ptr})
End
\end{lstlisting}
\caption{Compressed Sparse Row in IDL:
         The top section of the figure shows the different involved arrays.
         The pseudocode in the middle row of the
         figure then directly gives a completion of \Cref{spmvbase}.
         Walking through the expressions and emitting IDL code on-by-one is
         sufficient.}
\label{csr_lilacwhat_fig}
\end{figure}

\begin{figure}[p]
\hfill
\begin{minipage}[b]{0.3\linewidth}
\includegraphics[width=0.9\linewidth]{figures/jdsorder.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.65\linewidth}
\footnotesize
\begin{align*}
\text{\bf perm} =& \begin{bmatrix}1\ \ 2\ \ 0\ \ 4\ \ 3\\\end{bmatrix}\\[-0.75mm]
\text{\bf val} =& \begin{bmatrix}\text{-}1\ \ 1\ \ 2\ \ \text{-}1\ \ 2\ \ 3\ \ 1\ \ 2\ \ 1\ \ 2\\\end{bmatrix}\\[-0.75mm]
\text{\bf col\_ind} =& \begin{bmatrix}1\ \ 0\ \ 1\ \ 2\ \ 3\ \ 2\ \ 2\ \ 3\ \ 4\ \ 3\\\end{bmatrix}\\[-0.75mm]
\text{\bf jd\_ptr} =& \begin{bmatrix}0\ \ 5\ \ 9\ \ 10\\\end{bmatrix}\\[-0.75mm]
\text{\bf nzcnt} =& \begin{bmatrix}3\ \ 2\ \ 2\ \ 2\ \ 1\end{bmatrix}
\end{align*}
\end{minipage}
\hfill

\vspace{0.8em}
\hrule
\vspace{0.3em}

\includegraphics[width=\linewidth]{figures/spmvjdswhat.pdf}
\vspace{-1.5em}
\lstset {
 basicstyle=\linespread{1.0}\small\ttfamily
}
\begin{lstlisting}[language=IDL,firstnumber=7]
([\dots])
  inherits VectorStore
      with {outer_loop} as {scope}
       and {perm.value} as {input_index} at {output} and
  inherits VectorRead
      with {outer_loop}          as {scope}
       and {outer_loop.iterator} as {input_index} at {perm} and
  inherits VectorRead
      with {outer_loop}      as {scope}
       and {inner_loop.src1} as {value}
       and {tmp1.value}      as {input_index} at {val} and
  inherits Addition
      with {jd_ptr.value}        as {input}
       and {outer_loop.iterator} as {addend} at {tmp1} and
  inherits VectorRead
      with {outer_loop}          as {scope}
       and {inner_loop.iterator} as {input_index} at {jd_ptr} and
  inherits VectorRead
      with {outer_loop}      as {scope}
       and {inner_loop.src2} as {value}
       and {col_ind.value}   as {input_index} at {vector} and
  inherits VectorRead
      with {outer_loop} as {scope}
       and {tmp1.value} as {input_index} at {col_ind} and
  inherits ReadForLoopIterations
     with {outer_loop}          as {scope}
      and {inner_loop}          as {for}
      and {outer_loop.iterator} as {input_index} at {read_range}
\end{lstlisting}
\caption{Jagged Diagonal Storage in IDL:
         The top section of the figure shows the different involved arrays.
         The pseudocode in the middle row of the
         figure then directly gives a completion of \Cref{spmvbase}.
         Walking through the expressions and emitting IDL code on-by-one is
         sufficient.}
\label{jds_lilacwhat_fig}
\end{figure}

\subsubsection{Compressed Sparse Row}
For Compressed Sparse Row (CSR) \cite{doi:10.1137/1.9780898718003}, all non-zero
entries are stored in a flat array \textbf{val}.
The \textbf{col\_ind} array stores the column position for each value.
Finally, the \textbf{row\_ptr} array stores the beginning of each row of the
matrix as an offset into the other two arrays.
The number of rows in the matrix is given directly by the length of the
\textbf{row\_ptr} array minus one, however the number of columns is not
explicitly stored.
In \autoref{csr_lilacwhat_fig}, a 5x5 matrix is shown represented in this
format.

\subsubsection{Jagged Diagonal Storage}
For Jagged Diagonal Storage (JDS) \cite{doi:10.1137/0910073}, the rows of the
matrix are permuted such that the number of nonzeros  per row  decreases. The
permutation is stored in a vector \textbf{perm}, the number of nonzeros in
\textbf{nzcnt}.
The nonzero entries are then stored in an array \textbf{val} in the following
order: The first nonzero entry in each row, then the second nonzero entry in
each row etc.
The array \textbf{col\_ind} stores the column for each of the values and
\textbf{jd\_ptr} stores offsets into \textbf{val} and \textbf{col\_idx}.
The product of a sparse matrix in JDS format with a dense vector is specified 
in LiLAC-What at the bottom of \autoref{jds_lilacwhat_fig}.

\subsection{Dense Linear Algebra}

    The generalized matrix multiplication idiom is described in \Cref{fig:gemm}.
    The control flow is captured by three nested for loops.
    Inside these loops, the memory access is characterized by three matrix
    accesses, each with a different subset of the loop iterators.
    The corresponding \texttt{MatrixRead} and \texttt{MatrixWrite} idioms model
    generic access to matrices allowing strides, transposed matrices etc.
    The actual computation is encapsulated by the \texttt{DotProductLoop} idiom.
    This also contains the linear combination with factors \texttt{alpha} and
    \texttt{beta} that is part of the generalized matrix multiplication.

    The sparse matrix vector multiplication defined in \Cref{csr_lilacwhat_fig}
    is different to the other idioms in that
    the control flow of the skeleton of the idiom does not consist of perfectly
    nested for loops.
    Instead, the iteration space of the inner loop is read from an array using
    the \texttt{ReadRange} idiom.
    The actual computation that SPMV performs is a dot product and thus it uses
    the same \texttt{DotProductLoop} idiom as
    \texttt{GEMM} but the memory access pattern is different, with indirect
    memory access in \texttt{indir\_read}.

\begin{figure}[h]
\begin{lstlisting}[language=IDL,
                   label={fig:gemm},caption={IDL specification of GEMM}]
Constraint GEMM
( inherits ForNest(N=3) and
  inherits MatrixStore
      with {iterator[0]} as {col}
       and {iterator[1]} as {row}
       and {begin} as {begin} at {output} and
  inherits MatrixRead
      with {iterator[0]} as {col}
       and {iterator[2]} as {row}
       and {begin} as {begin} at {input1} and
  inherits MatrixRead
      with {iterator[1]} as {col}
       and {iterator[2]} as {row}
       and {begin} as {begin} at {input2} and
  inherits DotProductLoop
      with {loop[2]}        as {loop}
       and {input1.value}   as {src1}
       and {input2.value}   as {src2}
       and {output.address} as {update_address})
End
\end{lstlisting}
\end{figure}

\subsection{Stencils}
    \Cref{fig:stencilcompute} shows the base version of the stencil idiom.
    Stencils consist of a loop nest with a multi-dimensional memory access to
    store the updated cell value.
    The updated value is computed by a kernel function using a number of
    values that are constrained by
    \texttt{StencilRead}, which specifies multidimensional array access
    with only constant offsets in all dimensions.

\begin{figure}[t]
\begin{lstlisting}[language=IDL,
                   label={fig:stencilcompute},caption={IDL specification of simple stencil}]
Constraint Stencil
( inherits ForNest and
  inherits PermMultidStore
      with {iterator} as {input}
       and {begin} as {begin} at {write} and
  collect i
  ( inherits StencilRead
      with {write.input_index} as {input}
       and {kernel.input[i]} as {value}
       and {begin} as {begin} at {reads[i]}) and
  {kernel.output} is first argument of {write.store} and
  inherits KernelFunction
      with {begin}      as {outer}
       and {body.begin} as {inner} at {kernel})
End
\end{lstlisting}
\end{figure}

\subsection{Not Syntactic Pattern Matching}
The idiom descriptions may at first appear to be shallow syntactic pattern matching.
In fact, because it operates on the IR level, it can detect idioms that are written in superficially distinct style but are semantically equivalent.
For example, there are two syntactically distinct programs in \Cref{fig:gemmexamples}, which in fact are both implementations of general matrix multiplication.
The IDL in \Cref{fig:gemm} discovers they are both instances of GEMM and they can both be replaced with an API call to GEMM.

\begin{figure}[ht]
\begin{lstlisting}[language=MyCpp]
for (int mm = 0; mm < m; ++mm) {
  for (int nn = 0; nn < n; ++nn) {
    float c = 0.0f;
    for (int i = 0; i < k; ++i) {
      float a = A[mm + i * lda]; 
      float b = B[nn + i * ldb];
      c += a * b;
    }
    C[mm+nn*ldc] =
        C[mm+nn*ldc] * beta + alpha * c;
  }
}
\end{lstlisting}
\begin{lstlisting}[language=MyCpp,label={fig:gemmexamples},caption=
   {Two matching instances of GEMM}]
for(int i = 0; i < 1000; i++)
    for(int j = 0; j < 1000; j++) {
        M3[i][j] = 0.0f;
        for(int k = 0; k < 1000; k++)
           M3[i][j]+=M1[i][k]*M2[k][j]; }
\end{lstlisting}
\end{figure}

    There are limitations to this semantic matching.
    In particular, the use of low level manual optimizations that circumvent the
    usual IR representation, {\em e.g.}  SIMD compiler intrinsics, would distort
    the algorithms beyond recognition by our system.
    In practice, this is rarely encountered.

\newpage
\phantom{placeholder}
\newpage

\section{Compilation Process and Implementation}
\label{sec:compilation}

    Idiom definitions are compiled to C++ functions that perform idiom
    recognition on LLVM IR.
    In a first step, the compiler eliminates
    $\left<\text{\tt inheritance}\right>$, $\left<\text{\tt forall}\right>$,
    $\left<\text{\tt forsome}\right>$, $\left<\text{\tt if}\right>$,
    $\left<\text{\tt rename}\right>$ and $\left<\text{\tt rebase}\right>$.
    They are replaced with simpler $\left<\text{\tt conjunction}\right>$ and
    $\left<\text{\tt disjunction}\right>$ constructs.
    This also involves removing all parameterizations from the formula and
    flattening all variable names.
    Next, variables are collected and ordered to assist constraint solving.
    The ordering impacts performance, as it determines how well the search space
    is pruned. 
    For each variable, all the constraints associated with the variable are
    assembled.

    The compiler then emits C++ code which is passed to a generic solver based on \cite{ginsbach2017discovery} to search for idiom instances.
    This solver is based on standard backtracking.
    As shown in the results section, this increases compilation time, but the overhead is modest.

\section{Targeted Heterogeneous APIs}

    After idiom detection, we must transform the user program to exploit the
    relevant API.
    Two types of heterogeneous APIs are currently targeted: libraries and domain
    specific languages with their optimizing compilers.

    \subsection{Domain Specific Libraries}
    Libraries provide narrow interfaces but are often highly optimised.
    For example, the cuBLAS library is only suitable for a limited set of dense
    linear algebra operations and only works on Nvidia GPUs, but its
    implementation provides outstanding performance.
    For sparse linear algebra we use the vendor provided cuSPARSE, clSPARSE, and
    MKL libraries.
    For dense BLAS routines cuBLAS, clBLAS, CLBlast, and MKL are used.

    \subsection{Domain Specific Code Generators}
    Domain Specific Languages provide wider interfaces than libraries and allow
    problems to be expressed as composition of dedicated language constructs.
    An optimizing compiler then specializes the program for the target hardware.
    We currently support Halide and Lift as domain specific code generators.

    \paragraph*{Halide}~\cite{Ragan-Kelley2013Halide} is a language and
    optimizing compiler targeted at image processing applications.
    Optimised code is generated for CPUs as well as GPUs.
    Halide separates the functional description of the problem from the
    description of the implementation.
    This involves a separate execution \emph{schedule}.
    This allows retargeting of Halide programs to different platforms.
    We translate some of the stencil idioms and linear algebra idioms into
    Halide.
    Stencils involving control flow in their computations are not easily
    expressible in Halide.

    \paragraph*{Lift}~\cite{steuwer15rewrite, SteuwerRD17, HagedornSSGD18} is an
    optimizing code generator based on rewrite rules.
    The Lift language consists of functional parallel patterns such as
    \emph{map} and \emph{reduce} which  express a range of parallel
    applications.
    For this work we translate stencil idioms, complex reductions and linear
    algebra idioms to Lift.

\section{Translating Computational Idioms}

    This section describes how the detected idioms are mapped to the previously
    described library APIs domain specific languages.
    The two types of APIs (library interfaces and domain specific languages) are
    treated individually.

\subsection{Library}

    For library call interfaces, the original code is removed and an appropriate
    function call is inserted.
    The solution that is generated by the solver using the IDL program contains
    both the IR instructions to remove as well as the arguments that are to be
    used for the function call.

    For example, in the case of the {\tt GEMM} program that was shown in
    \Cref{fig:gemm}, the original code is removed by deleting the IR
    instruction at {\tt output.store\_instr} explicitly, which captures the
    store instruction of the {\tt MatrixStore} subprogram.
    The remaining cleanup is left to the standard dead code elimination pass.
    The arguments that specify the matrix dimensions are taken from
    {\tt ForNest} in combination with the stride and offset determined by
    {\tt MatrixRead} and {\tt MatrixWrite}.

    The mapping of solution variables to the arguments of the generated function
    call needs to be implemented individually for each backend, as we have no
    way to describe it in IDL itself.
    Once the code is replaced, LLVM continues with code generation as usual.

\subsection{DSL}

    For domain specific languages, the situation is a bit more involved.
    Reduction, histogram and stencil idioms are higher order functions that
    contain a kernel function or reduction operator that has to be represented
    for the DSL.

    For each combination of idiom and DSL there is a parameterized
    skeleton program.
    This skeleton is then specialized for the appropriate data types and numeric
    parameters as well as the kernel function or reduction operator.

    Numerical parameters are picked from the constraint solution in the same way
    that was described previously for library call interfaces.
    Also from the constraint solution, we have the loop body that contains the
    kernel function or reduction operator, as well as the input values and the
    result value used.
    We use this information to cut out the kernel function that is then used to
    generate code appropriate for the DSL backends:

\paragraph*{Lift}  expects stencil kernels or reduction operators to be sequential C code with a specific function interface that
is used internally by Lift when generating OpenCL code.
We therefore implemented a rudimentary LLVM IR to C backend for generating this function.

\paragraph*{Halide} is a language embedded in C++, it requires a syntax tree of the kernel functions built using a class hierarchy.

\begin{figure}[ht]
\begin{lstlisting}[language=LIFT,escapechar=|,
                   label={fig:liftmxm},caption=
   {Example of matrix multiplication in Lift}]
float mult(float x, float y) { return x*y; }
float add(float x, float y) { return x+y; }

gemm_in_lift(A, B, C, alpha, beta) {
 map(fun(a_row, c_row) {
  map(fun(b_col, c) {
   map(fun(ab){ add(mult(alpha, ab), mult(beta, c))},
    |\label{line:dot}|reduce(add, 0.0f, map(mult, zip(a_row, b_col))))
  }, zip(transpose(B), c_row))
 }, zip(A, C))
}
\end{lstlisting}
\end{figure}

\paragraph*{}
After code for the DSLs is generated, it is passed to the DSL code generator.
\Cref{fig:liftmxm} shows an example of the Lift code generated for GEMM (\texttt{gemm\_in\_lift}).
It performs a dot product (expressed in line~\ref{line:dot} using the Lift skeletons \texttt{zip}, \texttt{map}, and \texttt{reduce}) for each row of matrix A (\texttt{a\_row}) and column of matrix B (\texttt{b\_col}).
This code is compiled by Lift into optimised OpenCL code.

Finally, we again replace the idiom code in the user's code with a call to the code generated by the DSL and continue once again with LLVM code generation.

\subsection{Aliasing}

    Since idiom detection works statically, we are unable to fully rule out
    aliasing of pointers, which can make transformations unsound.
    For dense linear algebra this is easily solved with some basic run time
    checks for non-overlapping memory.
    However, for sparse linear algebra this is not as straightforward and in
    corner cases our approach is unsound.
    In practice this did not cause problems on any of the benchmark programs,
    however this means that optimizations based on these techniques will have to
    provide appropriate feedback to the programmer.

\section{Experimental Setup}

    \paragraph*{Benchmarks}
    We applied our approach to all of the sequential C/C++ versions of the NAS
    Parallel Benchmarks.
    We use the SNU NPB implementation by the Seoul National University,
    containing the original 8 NAS benchmarks plus two of the newer unstructured
    components UA and DC.
    We also evaluated our approach on all Parboil benchmarks, giving 21 programs
    in total. 

    \paragraph*{Platform and Evaluation}
    We use an AMD A10-7850K APU with a multi-core CPU and an integrated Radeon
    R7 GPU on the same die using driver version 1912.5, as well as an Nvidia GTX
    Titan X as an external GPU using driver version 375.66.
    We report the median runtime of 10 executions for each program.

    \paragraph*{Alternative detection approaches}
    \hspace{0.2cm}There are no easily available compilers to compare against
    that perform idiom detection.
    Instead, we consider two parallelizing compilers and examine
    whether they detect idioms as part of their parallelization approach.
    As their goal is parallelization and not idiom detection, this should be
    borne in mind in the results section.

    Polly \cite{Doerfert2015Polly} is an LLVM based polyhedral compiler
    capable of finding parallel loops and reductions in Static Control
    Parts (SCoP) of programs.  This allows comparison against
    another approach that uses the same compiler infrastructure.
    We gathered the SCoPs that Polly detected with the options
    \texttt{-O3 -mllvm -polly -mllvm -polly-export} and manually inspected
    the reported SCoPs for stencil like parallel loops and reduction operations.
    When Polly captured such a loop as a SCoP, we counted it as an idiom
    detection, although Polly itself has no concept of idioms.
    This gives an optimistic estimate as to what idiom coverage a polyhedral
    based approach can achieve.

    The Intel C++ Compiler (ICC) is a mature industry strength compiler that
    provides a detection mechanism for parallelizing reduction idioms based on
    data dependency analysis.
    We use the \texttt{-parallel -qopt-report} command line options and checked
    in the optimization report files whether the corresponding loop is
    considered parallelizable.

\newpage
\section{Results}
\label{sec:idiomresults}

    The approach was evaluated in several steps.
    Firstly, the number of detected idioms and their distribution over the
    benchmark programs was established.
    During this analysis, the run time of the IDL-enabled Clang compiler was
    measured, and the compile time overhead of the solver over standard
    compilation evaluated.
    Next, the run time coverage of the idioms was determined for each
    benchmark program, to see where exploitation might be beneficial.

    Where run time coverage was substantial, speedups over sequential C code
    are reported.
    Detailed results capture the performance of each trageted backend interface.
    Finally, the evaluation includes comparison to the handwritten OpenMP and
    OpenCL implementations that are included with the benchmark suites as
    reference implementations.
    These provide a suitable estimator for the upper bound of available
    performance.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/asplosplots/detection.pdf}
  \caption{The different computational idioms found in all benchmarks.}
  \label{detection-figure}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/asplosplots/coverage.pdf}
  \caption{Runtime coverage of the detected idioms in all benchmarks.}
  \label{coverage-figure}
  \vspace{0.5em}
\end{figure}

\subsection{Idiom Detection}

    \Cref{tab:detection} shows the number of computational idioms found by IDL,
    Polly, and ICC.
    Polly found 3 scalar reductions and 6 stencils while ICC recognised only
    28 scalar reduction.
    Polly was unable to perform idiom specific optimizations on GEMM.
    Other approaches did not detect any histograms or sparse matrix operations,
    because such code involves indirect and thus non-affine memory accesses.
    This fundamentally contradicts assumptions that these tools rely on and is
    not merely an implementation artifact.
    Our IDL approach detects 60 idioms overall with the compile time cost shown
    in figure \Cref{tab:compiletimecost}.
    On average, the compilation time is increased by 82\%, which can be reduced
    further by optimizing the solver.

    \Cref{detection-figure} shows the different idioms detected across
    the  benchmarks. We detect both scalar
    and histogram reductions as well as stencils, dense matrix operations
    and sparse matrix-vector multiplication.
    Polly and ICC are only capable of detecting simple scalar
    reductions, but we are able to detect histogram reductions, {\em e.g.} in
    the \emph{histo} benchmark as well.  For stencils, Polly detects two
    in \emph{lbm} and \emph{stencil} while our approach
    detects all the stencils in \emph{lbm}, \emph{stencil} and \emph{MG}.
    Unlike any existing approach, we detect sparse matrix-vector
    operations in {\emph CG} and {\emph spmv} as well as dense matrix
    operations in \emph{sgemm}. It is worth repeating, however, that both
    Polly and ICC are parallelizing compilers, not idiom recognition
    tools.

\begin{table}[t]
\centering
  \begin{tabular}{lP{2.16cm}P{2.16cm}P{2.16cm}P{2.163cm}P{2.163cm}}
  \toprule
  \hspace{1.18cm} & Scalar\newline{}Reduction & Histogram\newline{}Reduction & Stencil & Matrix~Op. & Sparse\newline{}Matrix~Op.\\
  \midrule
  Polly &  3  &  --- &   5  &  --- & --- \\
  ICC   &  28 &  --- &  --- &  --- & --- \\
  IDL   &  45 &   5  &   6  &   1  &  3  \\
  \bottomrule
\end{tabular}
\caption{Idioms detected by IDL, ICC, Polly}
\label{tab:detection}
\end{table}

\subsection{Runtime Coverage}

    To determine if the detected idioms are actually important,
    \Cref{coverage-figure} shows the percentage of time spent in the detected
    computational idiom.
    This data shows that either the detected idioms have a low runtime
    contribution or they dominate almost the entire execution.
    \emph{EP} is the only exception where about 50\% of the runtime is spent
    inside a detected histogram reduction.
    We focus on the 10 programs which spend a significant amount of time in the
    detected idioms, as only these can reasonably expect a performance gain
    using our approach.

\subsection{Performance Results}

\begin{table}[t]
\centering
\begin{tabular}{lcccccccccccc}
  \toprule
  & \hspace{0.17mm}BT\hspace{0.17mm}
  & \hspace{0.17mm}CG\hspace{0.17mm}
  & \hspace{0.17mm}DC\hspace{0.17mm}
  & \hspace{0.17mm}EP\hspace{0.17mm}
  & \hspace{0.17mm}FT\hspace{0.17mm}
  & \hspace{0.17mm}IS\hspace{0.17mm}
  & \hspace{0.17mm}LU\hspace{0.17mm}
  & \hspace{0.17mm}MG\hspace{0.17mm}
  & \hspace{0.17mm}SP\hspace{0.17mm}
  & \hspace{0.17mm}UA\hspace{0.17mm}
  & \hspace{0.17mm}bfs\hspace{0.17mm}
  & \hspace{0.17mm}cutcp\hspace{0.17mm} \\
  \midrule
without IDL    & 1.9 & 0.5 & 1.0 & 0.3 & 0.6 & 0.3 & 1.9 & 0.8 & 1.6 & 2.7 & 0.4 & 0.4 \\[0.25em]
with IDL       & 4.0 & 0.8 & 1.6 & 0.6 & 1.2 & 0.5 & 3.9 & 4.5 & 3.2 & 7.3 & 0.5 & 0.6 \\[0.75em]
overhead in \% & 116 &  77 &  57 &  77 &  93 &  62 & 103 & 484 &  97 & 169 &  30 &  65 \\
  \bottomrule
\end{tabular}
\vspace{5mm}

\begin{tabular}{lccccccccc}
  \toprule
  & \hspace{0.44mm}histo\hspace{0.44mm}
  & \hspace{0.44mm}lbm\hspace{0.44mm}
  & \hspace{0.44mm}mri-g\hspace{0.44mm}
  & \hspace{0.44mm}mri-q\hspace{0.44mm}
  & \hspace{0.44mm}sad\hspace{0.44mm}
  & \hspace{0.44mm}sgemm\hspace{0.44mm}
  & \hspace{0.44mm}spmv\hspace{0.44mm}
  & \hspace{0.44mm}stencil\hspace{0.44mm}
  & \hspace{0.44mm}tpacf\hspace{0.44mm} \\
  \midrule
without IDL    & 0.2 & 0.3 & 0.2 & 0.2 & 0.4 & 0.6 & 0.3 & 0.2 & 0.2 \\[0.25em]
with IDL       & 0.2 & 0.6 & 0.4 & 0.3 & 0.6 & 0.7 & 0.7 & 0.2 & 0.4 \\[0.75em]
overhead in \% &  35 &  87 & 100 &  52 &  58 &  24 & 115 &  36 &  54 \\
  \bottomrule
\end{tabular}
\caption{Compile time cost in seconds}
\label{tab:compiletimecost}
\end{table}

\paragraph*{Speedup vs. Sequential}
\Cref{fig:speedup-figure} shows the end-to-end speedup obtained by accelerating idioms with heterogeneous APIs on a CPU, an integrated GPU, and an external GPU.
All results include data transfer overhead to and from the GPUs.
Here the best performing API is shown;
\Cref{tab:detailed-results} provides detailed results for all APIs. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/asplosplots/speedup_vs_sequential_wide.pdf}
  \caption{Speedup over sequential:
           Results for the best-performing backend on each platform are shown.
           The red bars indicate a manual modification for minimising redundant
           data transfers.}
  \label{fig:speedup-figure}
  \vspace{1.5em}
  \centering
  \includegraphics[width=\textwidth]{figures/asplosplots/comparison.pdf}
  \caption{IDL versus manual expert parallelisation:
           Speedup over the sequential baseline was measured for IDL
           (selecting best performing backend; red bars) and the
           handwritten reference OpenCL and OpenMP implementations
           (provided by the benchmark developers; yellow bars).}
  \label{fig:speedup-figure-2}
  \vspace{0.5em}
\end{figure}

    For five benchmarks we obtain moderate speedups from 1.26$\times$ for
    \emph{histo} up to 4.5$\times$ for \emph{IS}.
    All of these benchmarks besides \emph{MG} have a scalar or histogram
    reduction as their performance bottleneck and are, therefore, not
    computationally expensive.
    Interestingly, we can see that for different benchmarks, different hardware
    is beneficial:
    for \emph{tpcaf}  the CPU is the best platform, beating the GPU  for which
    the data transfer time dominates;
    for \emph{MG} and \emph{histo} the integrated GPU strikes the right balance
    between computational power while avoiding the movement of data to the
    external GPU;
    and, finally, for \emph{EP} and \emph{IS} the data transfer to the GPU pays
    off exploiting the high GPU internal memory bandwidth.
    These results emphasize the significance of heterogeneous code generation
    flexibility.

For five of the benchmarks we achieve significantly higher performance gains, from 17$\times$ for \emph{CG} and up to over 275$\times$ for \emph{sgemm}.
These benchmarks are computationally expensive and the external GPU is always the fastest architecture by a considerable margin.

\begin{landscape}
\newlength{\txtwd}
\newcommand{\msb}[1]{\settowidth{\txtwd}{#1}{\tiny\ttfamily\bfseries \hfill #1}}
\newcommand{\ms}[1]{\settowidth{\txtwd}{#1}{\tiny\ttfamily \hfill #1}}
\addtolength{\tabcolsep}{-0.64mm}
\begin{table}[p]
  \centering
  \small
  \begin{tabular}{|l||cccccc|ccccc|cccc|}
  \hline
  & \multicolumn{6}{c|}{\bfseries\large CPU} & \multicolumn{5}{c|}{\bfseries\large iGPU} & \multicolumn{4}{c|}{\bfseries\large GPU} \\
  & MKL & libSPMV & Halide & clBLAS & CLBlast & Lift & clSPARSE & libSPMV & clBLAS & CLBlast & Lift & cuSPARSE & libSPMV & cuBLAS & Lift \\
  \hline
  \hline
   CG      & \msb{1504.21} & --- & --- & --- & --- & --- & \msb{644.02} & --- & --- & --- & --- &  \msb{113.51} & --- & --- & --- \\[3mm]
   EP      & --- & --- & --- & --- & --- &  \msb{32762.50}  & --- & --- & --- & --- & \msb{30983.40}  & --- & --- & --- & \msb{24680.70} \\[3mm]
   IS      & --- & --- & \msb{426.95} & ---  & --- & \ms{1765.61}  &  --- & --- & --- & --- & \msb{547.28}  & --- & --- & --- & \msb{99.95} \\[3mm]
   MG      & --- & --- & --- & --- & --- &  \msb{4699.63}  & --- & --- & --- & --- & \msb{1439.58}  & --- & --- & --- & \msb{2211.56} \\[3mm]
   histo   & --- & --- & --- & --- & --- &  \msb{27.42}  & --- & --- & --- & --- & \msb{17.20}  & --- & --- & --- & \msb{19.54} \\[3mm]
   lbm     & --- & --- & --- & --- & --- &  \msb{6457.93}  & --- & --- & --- & --- & \msb{5335.09}  & --- & --- & --- & \msb{590.60} \\[3mm]
   sgemm   & \msb{53.50} & --- & --- & \ms{1661.75} & \ms{660.44} & \ms{1339.15}  & --- & --- & \msb{14.73} & \ms{19.03} & \ms{15.04}  & --- & --- & \msb{5.99} & \ms{7.87} \\[3mm]
   spmv    & --- & \msb{218.17} & --- & --- & --- & --- & --- &\msb{102.233} & --- & --- & --- & --- &\msb{18.437} & --- &  ---\\[3mm]
   stencil & --- & ---& \msb{5760.81} & --- & --- & \ms{21951.80}  & --- & ---& --- & --- & \msb{2261.48} & --- & ---& --- & \msb{279.38} \\[3mm]
   tpacf   & --- & ---& --- & --- & --- & \msb{19276.40}  & --- & ---& --- & --- & \msb{61111.90} & --- & ---& --- & \msb{23358.20} \\
  \hline
\end{tabular}
\caption{Detailed performance results for each heterogeneous backend interface:
         The run time of benchmark program is measured in milliseconds for every
         compatible combination in each platform.
         The fastest implementations for each benchmark and target hardware are
         highlighted in bold.}
\label{tab:detailed-results}
\end{table}
\end{landscape}


The red highlighting in the plot indicates an important runtime optimization:
redundant data transfers for the iterative \emph{CG}, \emph{lbm}, \emph{spmv} and \emph{stencil} benchmarks.
All of these benchmarks execute computations inside a for loop and do not require access to the data on the CPU between iterations.
We manually applied a straightforward lazy copying technique by flagging memory objects to avoid redundant transfers, similar to~\cite{jablin11automatic}.
As can be seen this runtime optimization is crucial for achieving high performance for these benchmarks.

\paragraph*{API performance comparison}
\Cref{tab:detailed-results} shows a breakdown of the performance of each API on each program and platform.
Not all APIs
target all platforms, {\emph{e.g.} cuSPARSE only targets NVIDIA GPUs and in the
case of Halide, the current version that we have access to failed to generate
valid GPU code for any of the benchmarks we tried.
The best performing API is highlighted in bold in the table entries.
The \emph{spmv} benchmark uses an unusual sparse matrix format, so that we
implemented a custom library libSPMV for this benchmark.

On the multicore CPUs, the Intel MKL library gives the best linear algebra performance, outperforming the other libraries and Lift.
Halide achieves good performance for the NPB \emph{IS} and Parboil \emph{stencil} benchmarks on the CPU, outperforming Lift due to its more advanced vectorization capabilities.
In the programs where scalar reductions dominate, Lift performs well.
On the iGPU, clBLAS provides a better matrix-multiplication implementation than either CLBlast or Lift.
On the external GPUs, the libraries provide better linear algebra implementations, while Lift performs well on stencils and reductions.

\paragraph*{Speedup vs. Handwritten Parallel Implementations}
\Cref{fig:speedup-figure-2} shows the performance of our approach compared to hand-written reference OpenMP and OpenCL implementations.
For some of the benchmarks, the parallel versions are significantly modified using different algorithms beyond the domain of automation.
We can see that for benchmarks where the handwritten implementation does not make algorithmic changes (\emph{CG}, \emph{histo}, \emph{lbm}, \emph{sgemm}, \emph{spmv}, \emph{stencil}), we achieve comparable -- or better -- performance.
For four benchmarks (\emph{EP}, \emph{IS}, \emph{MG}, and \emph{tpacf}) it is beneficial to parallelize the entire application -- which is beyond the scope of this paper. Future work will examine outer loop parallelism as an idiom to exploit.

For the \emph{sgemm} and \emph{stencil} benchmarks we improved the baseline implementation provided by the benchmarks as these had extremely poor performance.
A simple interchange of two loops improved performance by almost 20 times.

\paragraph*{Summary}

    60 idioms were detected across the benchmark suites and significant
    performance improvements were achieved by targeting different heterogeneous
    APIs for those benchmarks where idioms dominate execution time.

\section{Conclusion}

    This paper develops a compiler based approach that automatically
    detects a wide class of idioms supported by libraries or domain
    specific languages for heterogeneous processors. This approach is
    based on a constraint based description language that identifies
    program subsets that adhere to idiom specifications.  Once
    detected, the idioms are mechanically translated into API calls to
    external libraries or code generated by DSL compilers.

    The approach is robust and was evaluated on C/C++ versions of two well
    known benchmark suites: NAS and Parboil. We detected more stencils,
    sparse matrix operations and generalized reductions and histograms than
    existing approaches and generated fast code.

    Future work will extend the constraint formulation to consider other common
    idioms.
    As the number of idioms detected and of implementations available grows, a
    smart profitability analysis will be needed and is the subject of future
    work.

