
    In \autoref{chapter:theory}, the underlying theory for contraint
    programming on SSA compiler intermediate representation was derived.
    Building on that framework, this chapter develops a domain specific
    contraint programming language and presents an implementation of this
    language as an extension to the production quality LLVM compiler
    infrastructure.

    The Compiler Analysis Description Language (CAnDL) automatically generates
    compiler analysis functionality that would otherwise have to be implemented
    manually.
    This fills an important niche in compiler development:
    Optimizing compilers have to use elaborate program transformations to exploit
    increasingly complex hardware.
    Implementing the required analysis functionality for such optimizations to
    be safely applied is a time consuming and error prone activity.
    For example, tens of thousands of lines of code are required in the LLVM
    code base to detect the appropriate places to apply peephole optimizations,
    and bugs in this functionality can result in corrupted user programs.
    This is a barrier to the rapid prototyping and evaluation of new
    optimizations.

    CAnDL enables a workflow that is much more amenable to efficient compiler
    development:
    The compiler implementer only has to write CAnDL programs, which are then
    compiled by the CAnDL compiler fully automatically into C++ LLVM passes.
    These passes automatically spot code sections that adhere the specified
    structure, leaving only the transformation functionality to be implemented
    by hand.
    This provides a uniform manner in which to describe compiler analysis,
    reducing code length and complexity.

    The approach crucially scales to a wide range of compiler analysis tasks,
    ranging from the detection of small scale local optimization opportunities
    over domain specific graphics optimizations to fully capturing polyhedral
    static control flow regions.
    In all cases, these tasks can be  expressed more briefly than with competing
    approaches.

\section{Introduction}

    Compilers are complex pieces of software responsible for the generation of
    efficient code.
    They transform input source code through several compilation stages,
    resulting in a binary program.
    In order to generate fast programs, modern compilers rely on a complex mid
    end.
    In this mid end, user code typically takes the form of a static single
    assignment intermediate representation and it is optimised by successively
    applying a wide range of optimisation passes.

    Most compiler optimisations require two components:
    analysis and transformation.
    First, analysis routines find candidate sections in user programs that
    enable specific transformations.
    They furthermore verify that all conditions are satisfied to legally apply
    them.
    It is crucial that optimisations retain the semantics of the original
    program, as otherwise the resulting binary might be corrupted.
    Transformations are then applied to the analysis results in a second step.
    This often also involves heuristic cost models to gauge the effect on
    runtime, code size and other metrics.

    The complexity of the necessary analysis is an impediment to
    the implementation of new compiler passes, preventing the rapid
    prototyping of new ideas.
    For example, simple peephole optimisations in the LLVM {\tt instcombine}
    pass contain approximately 30000 lines of complex C++ code, despite the
    transformations being simple.
    Furthermore, this is an important source of bugs, and bugs in this stage of
    the compiler are particularly pernicious.
    They tamper with the user programs, but can remain unnoticed and often only
    trigger in corner cases.
    Ideally, we would like a simpler way of implementing such analysis that
    reduces boiler-plate code and opens the way for new compiler optimisation
    innovation.

    This chapter presents the Compiler Analysis Description Language (CAnDL), a
    domain specific language for compiler analysis.
    It is a constraint programming language, operating on the static single
    assignment intermediate representation of the LLVM compiler infrastructure
    (LLVM IR).
    Instead of writing compiler analysis code inside the main codebase of the
    compiler infrastructure, it enables compiler writers to specify optimization
    functionality external to the main C++ code base.
    The CAnDL compiler then generates C++ functions that are linked together
    with the clang compiler binary and that implement LLVM passes.
    The formulation of optimizing transformations in CAnDL is faster, simpler
    and less error prone than writing them in C++.
    It has a strong emphasis on modularity, which enables debugging and the
    formulation of highly readable code.

    CAnDL is based on the constraint programming methodology introduced in
    \autoref{chapter:theory}.
    It uses a solver that is integrated into the LLVM code base.
    CAnDL is developed as a complete programming language, with a full parser
    and code generator.
    The system is evaluated on a range of use cases from
    different domains, including: standard LLVM optimisation passes,
    custom optimisations for graphics shader programs and the detection static
    control flow regions for polyhedral program transformations.

\section{Motivating Example}

\begin{figure}[b]
\centering
\begin{minipage}[t]{0.67\textwidth}
\begin{lstlisting}[language=CAnDL]
Constraint SqrtOfSquare
( opcode{sqrt_call} = call
([$\tt \land$]) {sqrt_call}.args[0] = {sqrt_fn}
([$\tt \land$]) function_name{sqrt_fn} = sqrt
([$\tt \land$]) {sqrt_call}.args[1]  = {square}
([$\tt \land$]) opcode{square} = fmul
([$\tt \land$]) {square}.args[0] = {a}
([$\tt \land$]) {square}.args[1] = {a})
End
\end{lstlisting}
\end{minipage}
\caption{Simple Declarative CAnDL Specificaiton}
\label{fig:candlspec}
\end{figure}

    As an example of the CAnDL workflow, consider \autoref{fig:root}.
    This basic algebraic equation can be interpreted as a recipe for a compiler
    optimisation:
    Assuming an environment without the particularities of floating point
    arithmetic (i.e.\ assuming the \texttt{-ffast-math} flag is active), the
    compiler could use this equality to eliminate some square root invocations
    in user code.
    This is desirable, as the square root has to be approximated with relatively
    expensive numerical methods, whereas computing the absolute value is
    computationally cheap.
    \begin{align}
    \label{fig:root}
    \forall a\in \mathbb{R}\colon\ \sqrt{a*a}=|a|
    \end{align}

    The compiler should use the equation left to right:
    It should analyse the user code in order to find segments that correspond to
    the left side of the equation and then transform all those occurences
    analogous to the right side of the equation.
    The compiler therefore must detect occurrences of $\sqrt{a*a}$ in the
    LLVM IR code and replace them with a call to the \texttt{abs} function.
    The generation of the new function call is trivial, but the detection of
    even this simple pattern requires some care when implementing it manually in
    a complex code base such as LLVM.

    The established approach in the clang compiler is to implement it in the
    \texttt{instcombine} pass, which applies a collection of such
    peephole optimisations and already extends to $\sim30000$ lines of
    C++ code.
    This code makes heavy use of raw pointers and dynamic type casts and has
    been identified as a frequent source of bugs, as documented by
    \citet{Yang:2011:FUB:1993316.1993532} and
    \citet{Menendez:2017:ADP:3062341.3062372}.
    This is impractical and an impediment to compiler development.

    Instead, CAnDL allows a declarative description of the analysis problem.
    It is easier to follow, has no interaction with other optimisations and
    is concise, as visible in \autoref{fig:candlspec}.
    The first line of the program assigns a name to the specification, which is
    then defined by the interaction of seven so-called {\em atomic constraint}s.
    These individual statements have to hold simultaneously on the values of
    \texttt{sqrt\_call}, \texttt{sqrt\_fn}, \texttt{square} and \texttt{a}.
    The lines 2--8 each stipulate one of these constraints, and they are joined
    together with logical conjunctions ``$\land$''.

    The CAnDL compiler translates this declarative program into a C++ analysis
    function that is used in an LLVM optimisation pass.
    This is demonstrated in \autoref{fig:candlexample}, which shows the analysis
    function generated from the CAnDL specification in \autoref{fig:candlspec}
    applied to a user program as follows:
    The input program ({\bf a}) is a simple C function that calls the
    \texttt{sqrt} function twice with squares of floating point values.
    This is translated using the clang compiler into LLVM IR code ({\bf b}),
    applying standard optimisation passes during the process.
    The expressions from the user program are now sequentialised and represented
    as a list of instructions with register assignemnts, with the two occurences
    of the \texttt{SqrtOfSquare} idiom clearly visible:
    The two \texttt{fmul} instructions (lines 3 and 5) compute squares via a
    floating point multiplication and these are then used as arguments to
    \texttt{sqrt} function invocations (lines 4 and 6).

    The optimised LLVM IR representation ({\bf b}) is used as the input to the
    generated analysis function, which detects two opportunities to apply the
    transformation, shown as first solution ({\bf d}) and second solution
    ({\bf e}).
    Each of the two solutions assigns values from within the LLVM IR code to
    each of the variables in the CAnDL program in \autoref{fig:candlspec}, such
    that all the specified constraints are fulfilled.
    The validity of these solutions is demonstrated in the middle row of the
    figure ({\bf e}-{\bf f}).
    Subsituting the variables in the CAnDL program with the concrete
    instances from the solutions, we can verify the individual atomic
    constraints one by one:
    \begin{itemize}
    \item \texttt{\%4} and \texttt{\%6} are function calls and their first
          argument (the function to be called) is \texttt{@sqrt}.
    \item \texttt{@sqrt} is the square root function.
          Note that it is identified by name.
    \item The second arguments of the function call instructions (and hence the
          first function arguments) are \texttt{\%3} and \texttt{\%5}
          respectively.
    \item \texttt{\%3} and \texttt{\%5} are square values, i.e.\ a floating
          point multiplication of a value with itself.
    \end{itemize}

    With the solutions identified by the CAnDL system, performing the
    transformation is now simple ({\bf g}).
    The solutions to the constraint problem are internally provided as C++
    dictionaries of the form \texttt{std::map<std::string,llvm::Value*>},
    containing all the information required to apply appropriate code
    transformations.
    A new function call to \texttt{abs} is generated, with \texttt{a} as the
    only argument (line 6).
    This instruction then replaces the original call instruction that was
    captured in \texttt{sqrt\_call} (line 5).
    The LLVM infrastructure already provides all the necessary functions
    to create and replace instructions in the intermediate representation.
    After post processing with standard dead code elimination, this results in
    the optimized code shown at the bottom of the figure ({\bf h}).

    Although this is only a small example, it illustrates the main steps of the
    CAnDL scheme.
    In practice, the strength of the system is to scale to complex
    specifications, culminating in a full polyhedral analysis, which will be
    demonstrated towards the end of the chapter.
    The next sections give a specification of the CAnDL language and outline how
    it is implemented on top of the constraint programming methodology devised
    in \autoref{chapter:theory}.

\begin{figure*}[p]
    \input{latex/figure_candlexample}
    \label{fig:candlexample}
\end{figure*}

\section{Language Specification}

    The Compiler Analysis Description Language is a domain specific
    programming language for the specification of compiler analysis problems. 
    Individual CAnDL programs define specific computational structures that
    exist in user programs and that can be exploited by applying code
    transformations.
    These structures are specified as constraint programs on single static
    assignment (SSA) form representation of user code in LLVM IR.
    CAnDL could also easily be ported to different SSA representations using the
    underlying theory from \autoref{chapter:theory}.

    The expressed structures can scale from simple instruction patterns that are
    suited for peephole optimizations, over control flow structures such
    as loops, to complex algorithmic concepts such as stencil codes with
    arbitrary kernel functions or code regions suitable for polyhedral code
    transformations.

    Like traditional constraint programs, CAnDL programs have two
    fundamental features: \textbf{variables} and \textbf{constraints}.
    The basic constraint building blocks are well established compiler analysis
    tools, such as constraints on data and control flow, data types and
    instruction opcodes.
    These are composed with logical connectors and several higher level language
    features, such as loops, with finally a system of modularity and
    extensability on top.
    This section will introduce the language features,
    starting from the overall program structure.

\subsection{High Level Structure of CAnDL Programs}

    The following notational conventions are used for the description of CAnDL
    syntax in this section:
    terminal symbols are {\bf bold}, non-terminals are {\it italic},
    $\left<\text{\bf s}\right>$ is an identifier (alphanumeric string) and
    $\left<\text{\bf n}\right>$ is an integer literal.
    CAnDL uses unicode characters such as ``$\land$'', ``$\in$'', ``$\Phi$'' and
    is typically encoded as UTF-8.
    An individual CAnDL program contains constraint formulas that are
    bound to identifiers.
    As previously shown in \autoref{fig:candlspec}, the syntax for this is as
    follows:
\begin{figure}[H]
\centering
\begin{tabular}{|c|}
    \hline
    $specification ::= \text{\bf Constraint}\ \left<\text{\bf s}\right>\ \text{\it formula}\ \text{\bf End}$\\
    \hline
\end{tabular}
\end{figure}

    \noindent
    Furthermore, \autoref{fig:candlspec} already demonstrated that logical
    conjunctions are be used to combine simpler {\it formula}s.
    More generally, a {\it formula} can be any of the following:
\begin{figure}[H]
\centering
\begin{tabular}{|c|}
    \hline
    $formula ::= \text{\it atomic}\mid\text{\it conjunction}\mid\text{\it disjunction}\mid\text{\it foreach}\mid \text{\it forany}\mid\text{\it include}\mid\text{\it collect}$\\
    \hline
\end{tabular}
\end{figure}

    \noindent
    The basis of every CAnDL program are {\it atomic} constraints, which are
    bound together by logical connectives ``$\land$'' and ``$\lor$''
    ({\it conjunction} and {\it disjunction}), as well as other higher level
    constructs.
    These include two kinds of loop structures ({\it foreach}, {\it forany}),
    and a system for modularity (\texttt{include}).
    Lastly, the {\it collect} construct allows for the formulation of more
    complex constraints that require the $\forall$ quantifier.
    The individual classes of atomic constraints will be introduced next,
    followed by the higher level constructs.

\subsection{Atomic Constraints}


\begin{figure}[t]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    syntax & SSA model formulation \\
    \hline
    \hline
    $\text{\bf data\_type}\ \text{\it variable}\ \text{\bf =}\ \left<\text{\bf s}\right>$ &  $(x,s)\in T_\mathcal F$\\
    \hline
    $\text{\bf ir\_type}\ \text{\it variable}\ \text{\bf =}\ \text{\bf literal}$ &  $x\in C_\mathcal F^*$\\
    $\text{\bf ir\_type}\ \text{\it variable}\ \text{\bf =}\ \text{\bf argument}$ & $x\in P_\mathcal F^*$\\
    $\text{\bf ir\_type}\ \text{\it variable}\ \text{\bf =}\ \text{\bf instruction}$ & $x\in I_\mathcal F^*$\\
    \hline
    $\text{\bf opcode}\ \text{\it variable}\ \text{\bf =}\ \left<\text{\bf s}\right>$ & $(x,s)\in I_\mathcal F$\\
    \hline
    $\text{\bf function\_name}\ \text{\it variable}\ \text{\bf =}\ \left<\text{\bf s}\right>$ & $(x,\text{!name=}s)\in T_\mathcal F$\\
    \hline
  \end{tabular}
  \caption{The simplest atomic constraints operate on a single variable and
           check element-of properties for the different sets in the SSA model.
           Function names are accommodated in $T_\mathcal F$.}
  \label{onevaratomics}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    syntax & SSA model formulation \\
    \hline
    \hline
    $\text{\it variable}\text{\bf.args[}\left<\text{\bf n}\right>\text{\bf]}\ \text{\bf =}\ \text{\it variable}$ & $(y,x,n)\in DFG_\mathcal F$\\
    $\text{\it variable}\ \text{\bf $\in$}\ \text{\it variable}\text{\bf .args}$ & $(x,y)\in DFG_\mathcal F^*$\\
    \hline
    $\text{\it variable}\text{\bf.successors[}\left<\text{\bf n}\right>\text{\bf]}\ \text{\bf =}\ \text{\it variable}$ & $(x,y,n)\in CFG_\mathcal F$\\
    $\text{\it variable}\ \text{\bf $\in$}\ \text{\it variable}\text{\bf .successors}$ & $(y,x)\in CFG_\mathcal F^*$\\
    \hline
    $\text{\it variable}\ \text{\bf =}\ \text{\it variable}$ & $x=y$\\
    $\text{\it variable}\ \text{\bf !=}\ \text{\it variable}$ & $x\neq y$\\
    \hline
  \end{tabular}
  \caption{The second class of atomic constraints operate on pairs of variables.
           The constraints check for graph edges in the
           data flow and control low graphs, or simply for shallow equivalence.}
  \label{twovaratomics}
\end{figure}

    Based on the model from \autoref{chapter:theory}, CAnDL provides a wide
    range of atomic constraints.
    The most simple group are those that operate on only a single variable,
    listed in \autoref{onevaratomics}.
    They operate immediately on the underlying mathematical structures, testing
    element-of properties of the sets in order to constrain data types,
    instrution opcodes etc.
    At the bottom of the list is the {\bf function\_name} constraint,
    which is particular.
    Function names are not a fundamental component of the SSA model from
    \autoref{chapter:theory}.
    However, they are easily accommodated within the type information
    $T_\mathcal F$.
    The same data structure can also be used to capture all kinds of LLVM
    specific metadata.

    There are additional atomic constraint that operate on pairs of variables.
    These are listed in \autoref{twovaratomics}.
    These most importantly check for edges in the control flow and data flow
    graphs.
    Furthermore, two constraints are available for shallow comparisons of
    variables.


%\begin{figure}[h]
%  \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    syntax & SSA model formulation \\
%    \hline
%    \hline
%    \multirow{2}{*}{$\text{\it variable}\ \text{\bf ->}\ \text{\it variable}\ \Phi\ \text{\it variable}$}
%        & $(z,phi)\in I_\mathcal F\mathrel\land (y,z)\in m(CFG_\mathcal F^*)\mathrel\land$ \exists n: \\
%        & $(x,z,n)\in DFG_\mathcal F\mathrel\land n=|\{y'\leq y\mid (y',z)\in m(CFG_\mathcal F^*)\}|$\\
%    \hline
%  \end{tabular}
%  \caption{CAnDL exposes Phi nodes with a dedicated atomic constraint.
%           The modified control flow graph $m(CFG_\mathcal F^*)$ duplicates
%           control flow edges into basic blocks with multiple Phi nodes.}
%  \label{phiatomic}
%\end{figure}

    On top of the atomic constraints that operate immediately on the sets of
    the mathematical model, there are a list of constraints that are based on
    based on graph properties.
    These include dominance relationships and the interaction of data flow and
    control flow in $\Phi$ nodes.

%    \begin{align*}
%        \text{\bf control\_origin}&\ \text{\it variable}\\
%        \text{\bf data\_origin}&\ \text{\it variable}
%    \end{align*}
%    The value is an origin of control (function entry) or data (function argument, {\tt load} instruction, inpure function call).

\subsubsection*{Phi Nodes}

    The following syntax is used to express the condition that a value reaches a
    $\Phi$ node via a specific jump instruction:
\begin{figure}[h]
    \centering
    \begin{tabular}{|c|}
        \hline
        $\text{\it variable}\ \text{\bf ->}\ \text{\it variable}\ \Phi\ \text{\it variable}$\\
        \hline
    \end{tabular}
\end{figure}

    \noindent
    More precisely, the expression $\{A\}\text{\bf ->}\{B\}\Phi\{C\}$ means that
    the value of $C$ is $A$ if $B$ was the last branch taken before $C$ was
    reached.
    Using the SSA model, this is equivalent to the following:
    \begin{align*}
        (C,phi)\in I_\mathcal F\mathrel\land{}&(B,h(C))\in CFG_\mathcal F\mathrel\land(A,C,n)\in DFG_\mathcal F,\\
            \text{where }h(C):={}&min\{c\mid (x,phi)\in I_\mathcal F\text{ for all }c\leq x\leq C\}\\
            \text{and }n:={}&\{b\leq B\mid (b,h(C))\in CFG_\mathcal F\}.
    \end{align*}
    Firstly, $C$ is a $\Phi$ node.
    Secondly, $B$ has a control flow edge to the basic block that $C$ is in.
    The first instruction of this basic block is identified as $h(C)$, to
    account for the possibility of more than one $\Phi$ node in the block.
    Thirdly, $A$ is the $n$-th argument of $C$, where $n$ is the index of $B$ in
    the list of jump instructions targeting $h(C)$.

\subsubsection*{Graph Domination}

    In order to express domination in the control flow graph, the following
    syntax is used:
\begin{figure}[h]
    \centering
    \begin{tabular}{|c|}
        \hline
        $\text{\bf domination(}\text{\it variable}\text{\bf,} \text{\it variable}\text{\bf)}$\\
        $\text{\bf strict\_domination(}\text{\it variable}\text{\bf,} \text{\it variable}\text{\bf)}$\\
        \hline
    \end{tabular}
\end{figure}

    \noindent
    For both these constraints, the values are implicitly limited to
    instructions.
    The expression ``{\bf domination}(\{A\},\{B\})'' means that $A$ is a
    dominator of $B$, i.e.\ any path through the control flow graph from the
    entry node to $B$ must go through $A$.
    Strict domination additionally requires that $A$ and $B$ are distinct.

    Another generalised graph domination is given by the following:
\begin{figure}[h]
    \centering
    \begin{tabular}{|c|}
        \hline
        ${\bf all}\ {\bf control}\ {\bf flow}\ {\bf from}\ variable\ {\bf to}\ variable\ {\bf passes}\ {\bf through}\ variable$\\
        \hline
    \end{tabular}
\end{figure}

    \noindent
    This constraint is similar to a standard control flow domination, but
    instead of taking paths from the control flow origin, a third variable is
    used for parameterisation. 
%    \begin{align*}
%        \text{\bf calculated\_from(}\text{\it varlist}\text{\bf,}\text{\it varlist}\text{\bf,}\text{\it variable}\text{\bf)}
%    \end{align*}
%    {\it Varlist} is a set of one or multiple {\it variables}.
%    Any path from one of the entries in the first {\it varlist} to the single
%    {\it variable} argument has to pass through at least one of the entries in
%    the second {\it varlist}.
%    All paths in the union of the data flow and control dependence graph are
%    considered.
%    We will see later how this is useful to specify kernel functions for
%    e.g.\ stencil calculations.

\subsubsection*{Additional Atomic Constraints}

    The set of {\it atomic}s that CAnDL supports is easily extensible.
    This will be further explored in later chapters.
    Possible additions include constraints on function attributes, value
    constraints on literals etc.

\subsection{Range Constraints}

\begin{figure}[t]
\begin{lstlisting}[language=CAnDL]
Constraint ValueChain
 {element[i] ([$\tt \in$]) {element[i+1]}.args foreach i=0..4
End
\end{lstlisting}
\begin{lstlisting}[language=CAnDL]
Constraint ValueChain
( {element[0]} ([$\tt \in$]) {element[1]}.args
([$\tt \land$]) {element[1]} ([$\tt \in$]) {element[2]}.args
([$\tt \land$]) {element[2]} ([$\tt \in$]) {element[3]}.args
([$\tt \land$]) {element[3]} ([$\tt \in$]) {element[4]}.args)
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{Example for the expansion of range constraints in CAnDL:
         Range based expresions can be ``unrolled'' manually, the two
         specifications are identical.}
\label{fig:forall}
\end{figure}


    Building on top of atomics and the fundamental conjunction and disjunction
    constructs, there are range based versions that operate on arrays of
    variables:
\begin{figure}[h]
  \centering
  \begin{tabular}{|c|}
    \hline
    $foreach ::= \text{\it formula}\ \text{\bf foreach}\ \left<\text{\bf s}\right>\ \text{\bf =}\ \text{\it index}\ \text{\bf ..}\ \text{\it index}$\\
    $forany ::= \text{\it formula}\ \text{\bf forany}\ \left<\text{\bf s}\right>\ \text{\bf =}\ \text{\it index}\ \text{\bf ..}\ \text{\it index}$\\
    \hline
  \end{tabular}
\end{figure}

    \noindent
    These constructs allow the replication of a constraint formula over a range
    of indices.
    This is demonstrated in \autoref{fig:forall}, which shows two equivalent
    CAnDL programs, the first one formulated with \texttt{foreach} and the
    second one without.
    In both cases, the program specifies an array of five variables with data
    flow from each element to the next.
    This shows how \texttt{foreach} can be expanded by duplicating the contained
    formula, with logical conjunctions ($\land$) binding the replicas of the
    formula together.
    Otherwise identical, the \texttt{forany} construct invokes logical
    disjunctions ($\lor$) instead.

    Although variable identifiers are black boxes to atomic constraints, their
    syntactic structure is relevant to range constructs, when indices are
    evaluated at CAnDL compile time.

\begin{figure}[h]
  \centering
  \begin{tabular}{|c|}
    \hline
    $variable ::= \left<\text{\bf s}\right>\mid\text{\it variable}\ \text{\bf [}\ \text{\it calculation}\ \text{\bf ]}\mid\text{\it variable}\ \text{\bf.}\ \left<\text{\bf s}\right>$\\
    $calculation ::= \left<\text{\bf s}\right>\mid\left<\text{\bf n}\right>\mid\text{\it calculation}\ \text{\bf+}\ \text{\it calculation}\mid\text{\it calculation}\ \text{\bf-}\ \text{\it calculation}$\\
    \hline
  \end{tabular}
\end{figure}

    \noindent
    The syntax of variable identifiers in CAnDL aligns closely to C/C++
    conventions.
    They can contain simple index calculations to support the range constructs,
    as well as a hierarchical structure.
    This hierarchical structure corresponds to the modularity capabilities of
    CAnDL that ar eintroduced in the next section.

\subsection{Modularity}

    Modularity is central to CAnDL, and it is achieved using the {\it include}
    construct.
    \begin{figure}[h]
        \centering
        \begin{tabular}{|c|}
            \hline
            $\text{\bf include}\ \left<\text{\bf s}\right>
                                [\text{\bf (}\text{\it variable}\ \text{\bf ->}\ \text{\it variable}\ \{\text{\bf ,}\ \text{\it variable}\ \text{\bf ->}\ \text{\it variable}\}\text{\bf )}]
                                [\text{\bf @}\ \text{\it variable}]$\\
            \hline
        \end{tabular}
    \end{figure}

    \noindent
    Note that the syntax in square brackets is optional and the syntax in curly
    brackets can be repeated.
    The basic version of \texttt{\it include}, without the optional structures,
    is simple.
    It copies the {\it formula} that corresponds to the identifier verbatim into
    another {\it formula}.
    If $[\text{\bf @}\ \text{\it variable}]$ is specified, then all the variable
    names of the inserted constraint formula are prefixed with the given
    variable name, separated with a dot.
    The other optional syntax is used to rename individual {\it variable}s in
    the included {\it formula}.

    \autoref{fig:inheritsandrenameandrebase} illustrates this with two
    equivalent programs.
    Both programs specify an addition of four values, first adding pairwise and
    then adding the intermediate results.
    We can see in the first listing that a {\it formula} for the addition of two
    values is bound to the name {\tt Sum}.
    This is then included three times in another {\it formula} names
    {\tt SumOfSums}.
    Using the optional grammatical constructs, the formula operates on a
    different set of {\it variable}s each time such that the third addition
    takes the results of the previous two as input.

\begin{figure}[ht]
\input{latex/figure_candlinheritance}
\label{fig:inheritsandrenameandrebase}
\end{figure}

\subsubsection{Collect}

    The \text{\it collect} construct is used to capture all possible solutions
    of a given formula.
    It is used to implement constraints that require the logical $\forall$
    quantifier.
    For example, it can be used to guarantee that all memory accesses in a loop
    use affine index computations.
    The grammar is simple but the semantics require some elaboration.
\begin{figure}[H]
    \centering
    \begin{tabular}{|c|}
        \hline
        $\text{\bf collect}\ \left<\text{\bf s}\right>\ \text{\it index}\ \text{\it formula}$\\
        \hline
    \end{tabular}
\end{figure}

    \noindent
    In \autoref{fig:simplecollect}, the variables \texttt{arg[0],\dots,arg[N-1]}
    are constraint to contain all of the data dependences of \texttt{ins}.
    The first argument of \text{\it collect} specifies the name of an index
    variable that is used to detect which variables belong to the collected set.
    In this example we want all solutions of \texttt{arg[i]} for a given value
    of \texttt{ins}.
    The second argument gives an upper bound to the amount of collected
    variables, in this case we leave it unspecified by using the symbol
    \texttt{N}.

\begin{figure}[ht]
\begin{lstlisting}[language=CAnDL]
Constraint CollectArguments
( ir_type{ins} = instruction
([$\tt \land$]) collect i N ({arg[i]} ([$\tt \in$]) {ins}.args))
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{Simple collect example in CAnDL}
\label{fig:simplecollect}
\end{figure}

    We can now extend this example to show how {\it collect} can be used to
    implement quantifiers.
    Consider that we want to detect instructions with only floating point data
    dependences.
    This involves the $\forall$ quantifier, as it is equivalent to
    the following equation.
    \begin{align}
        \forall v\colon\ v\in I.\text{args}\implies\text{data\_type}(v)=\text{float}
    \label{fig:implication}
    \end{align}
    We can rewrite this to an equivalent formulation on sets:
    \begin{align*}
        S_1:= \{v\mid v\in I.\text{args}\}\subset S_2:={}\{v\mid\text{data\_type}(v)=\text{float}\}.
    \end{align*}
    Now we can apply the following equivalences:
    \begin{align*}
        S_1\subset S_2\iff S_1 = S_1\cap S_2\iff S=S_1\land S=S_1\cap S_2\text{ for some set }S.
    \end{align*}
    This means that if we constrain a set $S$ to be equal to both $S_1$ and
    $S_1\cap S_2$ at the same time, the constraints are satisfiable if and only
    if the implication in \autoref{fig:implication} holds.

    This condition can be expressed in CAnDL, as is shown in
    \autoref{fig:collectexample}.
    With the first {\it collect} statement in line 3, we constrain the set
    \texttt{arg} to be equal to $S_1$ and with the second one in lines 4-5 we
    constrain it to be $S_1\cap S_2$ as well.
    Note that we were from the onset only interested in the values that qualify
    for \texttt{ins}.
    The set \texttt{arg} was only introduced to further constraint \texttt{ins},
    not because we actually wanted to know the values that it contains.

\begin{figure}[ht]
\begin{lstlisting}[language=CAnDL]
Constraint FloatingPointInstruction
( ir_type{ins} = instruction
([$\tt \land$]) collect i N ( {ins} ([$\tt \in$]) {arg[i]}.args)
([$\tt \land$]) collect i N ( {ins} ([$\tt \in$]) {arg[i]}.args
              ([$\tt \land$]) data_type{arg[i]} = float))
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{Collect Example in CAnDL}
\label{fig:collectexample}
\end{figure}

    The exact same approach can be used to e.g. restrict all array accesses in a
    loop to be affine in the loop iterators:
    First {\it collect} all memory accesses
    (i.e.\ all \texttt{load} and \texttt{store} instrutions) and then use
    another {\it collect} statement to stipulate affine calculations for the
    indices.

\subsection{Expressing Larger Structures}

    The modularity of CAnDL allows the creation of a library of building blocks
    that are shared by multiple CAnDL programs.
    This section will give an overview of how these can be defined.

    Important building blocks include control flow structures such as single
    entry single exit regions and loops.
    These are standard in compiler analysis and the implementation in CAnDL is
    straightforward.
    A for loop involves a comparison of the loop iterator with the end of the
    iteration space.
    In order to be valid, this value has to be determined before the loop is
    entered, it isn't allowed to change from loop iteration to iteration.
    This leaves it to be either a function argument, an actual constant or an
    instruction that strictly dominates the loop entry.
    This is expressed in \autoref{fig:localconstant}.
    Note that this formula is to be included into larger CAnDL programs, as the
    \texttt{begin} variable is under specified otherwise.

\begin{figure}[ht]
\begin{lstlisting}[language=CAnDL]
Constraint LocalConst
( ir_type{value} = literal
([$\tt \lor$]) ir_type{value} = argument
([$\tt \lor$]) strict_domination({value}, {begin}))
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{LocalConst in CAnDL}
\label{fig:localconstant}
\end{figure}

    Another class of important building blocks are different categories of
    memory access.
    These form a hierarchy of restrictiveness and include multidimensional array
    access and array access that is affine in some loop iterators.
    LLVM strictly separates memory access from pointer computations, which means
    that CAnDL only has to concern itself with plain pointer computations here.
    In general it is required that the base pointer is \texttt{LocalConst} in
    order to avoid pointer chases.
    The index computation can then be described using the data flow and
    instruction opcode restrictions.

\begin{figure}[t]
\centering
\includegraphics[width=0.65\textwidth]{figures/compilerFlow2.pdf}
\caption{CAnDL in the LLVM/clang build system}
\label{fig:build2}
\end{figure}

\section{Implementation}

    CAnDL interacts with the LLVM framework, as shown in \autoref{fig:build2}.
    CAnDL programs are read by the CAnDL compiler, which then generates C++
    source code to implement the specified LLVM analysis functionality.
    This code depends on a generic backtracking solver, which is incorporated
    into the main LLVM code base. 
    The evaluation at the end of the chapter will show that this solver adds
    little compile-time overhead in practice.
    The generated code is compiled and linked together with the existing LLVM
    libraries to make LLVM optimization passes available in the clang compiler.

    The generated analysis passes use the solver to search for the specified
    computational structures and output the found instances into report files,
    as well as making them available to ensuing transformation passes.

\subsection{The CAnDL Compiler}

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{figures/candlstages.pdf}
\caption{CAnDL compiler flow}
\label{fig:compilerflow}
\end{figure}

    The CAnDL compiler is responsible for generating C++ code from CAnDL
    programs.
    An overview of its  flow is shown in \autoref{fig:compilerflow}.
    The frontend reads in  CAnDL source code and builds an abstract syntax tree.
    This syntax tree is simplified in two steps to eliminate some of the higher
    order constructs of CAnDL.
    The \texttt{inheritance} clauses are replaced using standard function
    inlining after the contained variables have been transformed accordingly.
    Also, \texttt{foreach} and \texttt{forany} statements are lowered to
    conjunction and disjunction constructs by duplicating the contained
    constraint code and then renaming the contained variable names appropriately
    for each iteration.
    This is equivalent to complete loop unrolling.
    From this point onwards, variable names are treated as flat strings.
    The remaining core language now consists only of atomics, conjunctions,
    disjunctions and collections.

    The CAnDL compiler applies a set of basic optimizations to speed up the
    solving process using the later generated C++ code.
    For example, nested conjunctions and disjunctions are flattened wherever
    possible.

    Finally, the compiler generates the C++ source code.
    This essentially involves constructing the constraint problem as a graph
    structure that is accessible to the solver.

\subsubsection{C++ Code Generation}

\begin{figure}[t]
\centering
\begin{lstlisting}[language=CAnDL]
Constraint SimpleAddition
( opcode{addition} = add
([$\tt \land$]) {addition}.args[0] = {left}
([$\tt \land$]) {addition}.args[1] = {right})
End
\end{lstlisting}
\begin{lstlisting}[language=C++]
auto constr0 = make_shared<AddInstruction>(model);
auto constr1 = make_shared<FirstArgument> (model);
auto constr2 = make_shared<SecondArgument>(model);
auto constr3 = make_shared<Conjunction>(
                   constr0,
                   select<0>(constr1),
                   select<0>(constr2));

vector<pair<string,shared_ptr<BacktrackingPart>>> result(3);
result[0] = make_pair("addition", constr3);
result[1] = make_pair("left",     select<1>(constr1));
result[2] = make_pair("right",    select<1>(constr2));
\end{lstlisting}
\vspace{-0.3cm}
\caption{C++ source code generation}
\label{fig:codegen}
\end{figure}

    The code generation process is demonstrated in \autoref{fig:codegen} with an
    example.
    Each of the atomic constraints results in a line of C++ code that constructs
    an object of a corresponding class:
    In this case, the three involved atomic constraints are implemented by
    \texttt{AddInstruction}, \texttt{FirstArgument} and \texttt{SecondArgument}.
    For constraints that 
    These objects are instantiated as shared pointers.

    The compiler then generates similar objects for the
    \texttt{conjunction}, \texttt{disjunction} and the \texttt{collect} structures.
    In our example, this only affects the variable \texttt{addition}, which is
    part of a \texttt{conjunction} clause.
    This results in an additional object construction that instantiates the
    \texttt{Conjunction} class corresponding to the ``$\land$'' operator
    in CAnDL.

    Those constraint classes that implement constraints on a single variable
    directly implement the \texttt{BacktrackingPart} interface introduced in
    \autoref{cppsolver}.
    In this example, this is true for \texttt{AddInstruction} and
    \texttt{Conjunction}.
    For more complex constraints, the \texttt{select<n>} template is used to
    specify which variable of a constraint is being considered.
    In lines 6--7, this is used to extract the parts of backtracking solutions
    referring to the \texttt{addition} variable, and to then pass them as
    arguments to the conjunction.

    Finally, the generated objects are inserted into a vector together with the
    corresponding variable names.
    This vector corresponds to the backtracking solution of the entire
    constraint problem and is then passed to the solver, which is implemented
    as in \autoref{cppsolver}.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/visual_gui2.png}
\caption{Interactive CAnDL test tool}
\medskip
\small
Left hand panel shows a SCoP in Polybench jacobi-2d, the right hand panel show the corresponding constraint solution
\label{fig:gui}
\end{figure*}

\subsection{Developer Tools}

    CAnDL makes writing compiler analysis passes easier, but reasoning about the
    semantics of compiler IR still remains difficult and the correctness of
    CAnDL programs can only be guaranteed with thorough testing.
    It is important to keep in mind that CAnDL is targeted at expert compiler
    developers.

    In order to make debugging of CAnDL programs more feasible, several
    supporting tools can be used.
    Most importantly, this includes an interactive gui, where developers can
    test out corner cases of CAnDL programs to find false positives and false
    negatives.
    This gui is shown in \autoref{fig:gui}, the example is from one of the use
    cases presented in \autoref{sec:casestudies}.

    In the left column, we can see part of a C program from the PolyBench
    benchmark suite, which implements a two-dimensional Jacobi stencil.
    The gui is configured to look for static control flow regions (SCoPs), as
    described later in one of the use cases.
    The user has clicked the ``analyze'' button, which triggered the analysis to
    run and prints the results in the right column.

    The solver found a SCoP in the IR code (corresponding to lines 459-468 of
    the C program).
    The text on the right shows the hierarchical structure of the solution, with
    IR values assigned to every variable.
    The corresponding C entities can be recovered using the debug
    information that is contained in the generated LLVM IR code.
    By modifying the C code, the developer can now test the detection and
    e.g.\ verify that no SCoP is detected if irregular control flow is
    introduced.

\section{Case Studies}
\label{sec:casestudies}

    The usefulness of CAnDL is evaluated on three real life use cases.
    First, it is used for simple peephole optimizations.
    Secondly, CAnDL is applied to graphics shader code optimisation.
    Finally, the detection of static control flow parts (SCoPs) for polyhedral
    code analysis is demonstrated.
    Where possible, the evaluation compares the number of lines of CAnDL code,
    the achieved program coverage and performance against prior approaches.

\subsection{Simple Optimizations}

    Arithmetic simplifications in LLVM are implemented in the
    \texttt{instcombine} pass.
    One example of this is the standard factorization optimization that uses the
    law of distributivity to simplify integer calculations as shown in
    \autoref{fig:factorization1}.
    It is implemented in 203 lines of code and furthermore uses supporting
    functionality provided by \texttt{instcombine}.
    \begin{align}
        a*b+a*c\rightarrow a*(b+c)
        \label{fig:factorization1}
    \end{align}
    This analysis problem can be formulated in CAnDL as shown in
    \autoref{fig:facopport}.
    Crucually, in lines 5,7,9,11, the specification makes use of
    \texttt{SumChain} and \texttt{MulChain}, which allows the CAnDL program
    to capture a large, generalised class of opportunities for factorization.
    The \texttt{instcombine} pass only has limited support for this, and
    requires to first apply associative and commutative laws to reorder the
    values.
    This is for example needed in the case of \autoref{fig:factorization2} and
    only partially supported by LLVM with the additional \texttt{reassociate}
    pass.
    \begin{align}
        a*b+c+d*a*e->a*(b+d*e)+c
        \label{fig:factorization2}
    \end{align}

\begin{figure}[t]
\begin{lstlisting}[language=CAnDL]
Constraint ComplexFactorization
( opcode{value} = add
([$\tt \land$]) {value}.args[0] = {sum1.value}
([$\tt \land$]) {value}.args[1] = {sum2.value}
([$\tt \land$]) include SumChain at {sum1}
([$\tt \land$]) {mul1.value} = {sum1.last_factor}
([$\tt \land$]) include MulChain at {mul1}
([$\tt \land$]) {mul1.last_factor} = {mul2.last_factor}
([$\tt \land$]) include SumChain at {sum2}
([$\tt \land$]) {mul2.value} = {sum2.last_factor}
([$\tt \land$]) include MulChain at {mul2})
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{ComplexFactorization in CAnDL}
\label{fig:facopport}
\end{figure}

\subsubsection{Setup}

    The program in \autoref{fig:facopport} was evaluated against the default
    factorisation optimisation in LLVM's \texttt{instcombine} on three benchmark
    suites: the sequential C versions of NPB, the C versions of Parboil and the
    OpenMP C/C++ versions of Rodinia.
    The existing LLVM \texttt{instcombine} pass was extended so that it
    automatically reports every time that it successfully applies the
    \texttt{tryFactorization} function.  

    % NPB:     29047 loc
    % Parboil:  7358 loc
    % Rodinia: 58510 loc
    The individual benchmark programs in the three benchmark
    suites consist of 94915 lines of code in total.
    For each benchmark suite, the total number of reported factorisations as
    well as LLVM's total compilation time were measured.

    Then the standard LLVM optimisation was disabled and instead the CAnDL
    generated detection functionality was used.
    The same application programs were compiled, reporting the number of
    factorisations found, and again measuring the total compilation time.
    Note that this compilation time includes all the other passes within LLVM as
    well as the CAnDL generated path.

\subsubsection{Results}

\begin{figure}[ht]
\centering
\begin{tabular}{|l||l|l|}
\hline
         & LLVM  &CAnDL \\
\hline
\hline
Lines of Code & 203 & 12 \\ \hline
Detected in NPB & 1 & 1 + 2 \\
Detected in Parboil & 0 & 0 + 1\\
Detected in  & 24 & 24 + 4\\ \hline
Total Compilation time & 152.2s & 152.2s+7.8s \\ \hline
\hline
\end{tabular}
\vspace{-0.1cm}
\caption{Factorizations LLVM vs CAnDL}
\label{fig:factorization_results}
\end{figure}

    The results are shown in \autoref{fig:factorization_results}.
    In general, the performance impact of peephole optimizations is small
    and in two of the benchmark sets we find only very small numbers.
    LLVM was unable to perform any factorisation in the entire Parboil suite.
    However, the Rodinia suite contains more opportunities, mostly in the
    Particlefilter and Mummergpu programs.

    In all three benchmarks suites, our scheme finds the same factorization
    opportunities as the \texttt{instcombine} pass plus an additional 7 cases.
    With only 12 lines of CAnDL code, we were able to capture more factorization
    opportunities than LLVM did using two hundred lines of code.

    Using CAnDL on large complex benchmark suites only increased total
    compilation time by 5\%, demonstrating its use as a prototyping tool.

\subsection{Graphics Shader Optimizations}

\begin{figure}[t]
\begin{lstlisting}[language=CAnDL]
Constraint FloatingPointAssociativeReorder
( include VectorMulChain and
([$\tt \land$]) collect j N
([$\tt \land$]) ( {hoisted[k]} = {factors[i]} forsome i=0..N
  ([$\tt \land$]) include ScalarHoist({hoisted[j]}->{out},
                       {scalar[j]}->{in})@{hoist[j]})
([$\tt \land$]) collect j N
  ( {nonhoisted[j]}  = {factors[i]} forsome i=0..N
  ([$\tt \land$]) {nonhoisted[j]} != {hoisted[i]} forall  i=0..N))
End
\end{lstlisting}
\vspace{-0.3cm}
\caption{CAnDL code for vectorized multiplication chains}
\label{fig:Lewis}
\end{figure}

    Graphics computations often involve arithmetic on vectors of single
    precision floating point values that represent either vertex positions in
    space or color values.
    Common graphics shader compilers utilize the LLVM framework internally.
    The LunarGLASS project was used in this work as an open source
    implementation of LLVM for graphics shaders.

    There are opportunities for associative reordering that LLVM misses.
    In general purpose code, such reordering can be problematic due to floating
    point computation artifacts.
    However, this is not a problem in the domain of graphics processing.
    Under specific circumstances, associative reordering within computations
    that mixes vector and scalar arithmetic can result in real performance
    improvements.

%    For real shader code, LLVM misses an opportunity for the associative
%    reordering of floating point computations.
%    Although such reordering is problematic in general, it is applicable in the
%    domain of graphics processing.

    There are often products of multiple floating point vectors, where several
    of the factors are actually scalars that were hoisted to vectors.
    By reordering the factors and delaying the hoisting to vectors, some of the
    vector products can be simplified to scalar products, as shown in the
    following equation.
    \begin{align*}
        \vec x={}&\vec a*_v\vec b*_v\text{vec3}(c)*_v\vec d*_v\text{vec3}(e)\\
        ={}&\text{vec3}(c*e)*\vec a*_v\vec b*_v\vec d
    \end{align*}

    The required analysis functionality for this optimization was implemented
    with CAnDL, as shown in \autoref{fig:Lewis}.
    The \mbox{included} \texttt{VectorMulChain} subprogram discovers chains of
    floating point vector multiplications in the IR code and uses the variables
    \texttt{factors} and \texttt{partials} such that the following equations
    hold
    \begin{align*}
        \text{\tt partials}[0] &= \text{\tt factors}[0]\\
        \text{\tt partials}[i+1] &= \text{\tt partials}[i]\times\text{\tt factors}[i+1].
    \end{align*}
    The \texttt{VectorMulChain} program furthermore guarantees that this is a
    chain of maximal length by checking that neither of the first two factors
    are multiplications and the last factor is not used in any multiplication.
    \texttt{ScalarHoist} detects the hoisting of scalars to vectors and this is
    used to collect all hoisted factors into the array \texttt{hoisted}.
    In a last step, all other factors are collected into the array
    \texttt{nonhoisted}.

    The corresponding transformation pass simply generates all the appropriate
    scalar and vector multiplications and replaces the old result with this
    newly generated one.
    We evaluated the performance impact on the CFXBench 4.0 on the  Qualcomm
    Adreno 530 GPU.

\subsubsection{Results}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/qualcomm_plot.pdf}
\caption{Speedup on Qualcomm Adreno 530}
\label{fig:qualcommspeedup}
\end{figure}

    The optimization was relevant to 8 of the fragment shaders in GFXBench 4.0.
    The number of lines of code needed and the resulting performance impact are
    shown in \autoref{fig:candlshader} and \autoref{fig:qualcommspeedup}.
    A total of 19 opportunities for the optimization to be applied were
    detected.
    Although the performance impact was moderate with $1-4\%$ speedup on eight
    of the fragment shaders, it shows how new analysis can be rapidly prototyped
    and evaluated with only a few lines of code.

\begin{figure}[h]
\centering
\begin{tabular}{|l||l|l|}
\hline
         & LLVM  &CAnDL \\
\hline
\hline
Lines of Code & Not implemented& 10 \\ \hline
Detected in GFX 4.0 & - & 19 \\ \hline
\hline
\end{tabular}
\caption{Shader optimization LLVM vs CAnDL}
\label{fig:candlshader}
\end{figure}

\subsection{Polyhedral SCoPs}

    The polyhedral model allows compilers to utilize powerful mathematical
    reasoning to detect parallelism opportunities in sequential code and to
    implement code transformations for well structured nested
    loops.
    It is currently applicable to Static Control Flow Parts (SCoPs) with affine
    data dependencies.
    Detecting SCoPs is fundamental and a necessary first step for any later
    polyhedral optimization.

    Implementations of the polyhedral model may differ in their precise
    definition of SCoPs.
    We implemented SCoP detection functionality in CAnDL and compared against
    the Polly implementation in LLVM.
    We rely on the definition of Semantic SCoPs in \citet{Lengauer2012Polly}.
    The constraints for SCoPs can be broken into several components:

    \paragraph{Structured Control Flow}
    SCoPs require well structured control flow.
    Technically speaking, this means that every conditional jump within the
    corresponding piece of IR is accounted for by for loops and conditionals.
    We enforce this with the \texttt{collect} statement as demonstrated in
    \autoref{fig:collectexample}.
    We use it in CAnDL programs \texttt{ForLoop} and
    \texttt{Conditional} that describe the control flow of for loops and
    conditionals and extract the involved conditional jump instructions.
    We then use another \texttt{collect} to verify that these are indeed all
    conditional jumps within the potential SCoP.

    Once we have established the control flow, we can use the iterators that are
    involved in the loops to define affine integer computations in the loop.
    This is done in a brute force fashion with a recursive constraint program.
    Using this analysis we then check that the iteration domain of all the for
    loops is well behaved, i.e.\ the boundaries are affine in the loop
    iterators.

    \paragraph{Affine Memory Access}
    We want to make sure that all memory access in the SCoP is affine.
    For this to be true we have to verify that for each load and store
    instruction, the base pointer is loop invariant and the index is calculated
    affinely.
    The loop invariant base pointer is easily guaranteed using the
    \texttt{LocalConst} program from \autoref{fig:localconstant}.

    Checking the index calculations is more involved and is again based on the
    \texttt{collect} method that was demonstrated in
    \autoref{fig:collectexample}.
    We use the \texttt{collect} construct to find all of the affine memory
    accesses in all the loop nests.
    We then use collect all \texttt{load} and \texttt{store} instructions and
    verify that both collections are identical.

    \paragraph{}
    We evaluated our detection of SCoPs on the PolyBench suite.
    For both our method as well as for Polly, we counted how many of the
    computational kernels contained in the benchmark suite are captured by the
    analysis.

\subsubsection{Results}

    As is visible in \autoref{fig:candlvspolly}, we capture all the SCoPs that Polly
    was able to detect.
    There is some postprocessing of the generated constraint solutions required
    to achieve this.
    Firstly, our results are not in the jscop format that Polly uses but contain
    the raw constraint solution as shown on the right side of \autoref{fig:gui}.
    Also, our CAnDL implementation does not merge consecutive outer level loops
    into SCoPs of maximum size.
    To compare, we extracted the detected loops from our report
    files, manually grouped them into maximum size SCoPs and verified that they
    fully cover the SCoPs detected by Polly.

    To measure lines of code, we compared our version with the amount of code in
    Polly's \texttt{ScopDetection.cpp} pass.
    We are able to detect the same number of SCoPs with much fewer lines of
    code.
    Note that the LoC count that we give for our SCoP program does not include
    all CAnDL code involved in the detection of polyhedral regions.
    We consider the code that is not specific to this idiom (such as loop
    structures) to be part of the CAnDL standard library.
    In the same way we did not account for e.g\ the ScalarEvolution pass when
    counting the lines for Polly.

    By having a high level representation of SCoPs, we allow polyhedral compiler
    researchers to explore the impact of relaxing or tightening the exact
    definition of SCoPs in a straightforward manner, enabling rapid prototyping.

\begin{figure}[ht]
    \input{latex/figure_candlvspolly}
    \label{fig:candlvspolly}
\end{figure}

\section{Conclusion}

    Optimizing compilers require sophisticated program analysis in order to
    generate performant code.
    The current way of implementing this functionality manually in programming
    languages such as C++ is not satisfactory.

    The domain specific Compiler Analysis Description Language (CAnDL) provides
    a more efficient approach.
    CAnDL programs can be used to automatically generate compiler analysis
    passes.
    They are easier to write and reduce the code size and complexity when
    comparing against manual C++ implementations.

    Although CAnDL is based on a constraint programming paradigm and uses a
    backtracking solver to analyze LLVM IR code, the use of CAnDL results in
    only moderate compile time increases.
    Many compiler analysis tasks are suitable for CAnDL, from
    peephole optimizations to sophisticated program analysis with the polyhedral
    model.

    Future work will investigate how formal verification methods can be
    applied to CAnDL in order to guarantee the correctness of compiler
    optimizations.
    Also, there is some overlap between what can be formulated in CAnDL and what
    is provided by LLVM in the form of the ScalarEvolution pass.
    We are interested as to whether we could use this existing functionality to
    speed up solving times.
