\section{Computational Idioms}

    The concept of computational idioms has been observed in different contexts
    and remains a rather vague concept.
    While terms such as \texttt{reduction}, \texttt{stencil} and
    \texttt{linear algebra} are commonly used, the concrete concepts can be
    surprisingly vague, although previous work has established several formal
    approaches.
    We will not try to create our own formal definitions in this chapter, but
    instead want to give an overview of the literature that sheds different
    perspectives on this topic.

    The basic observation is that software programs don't cover the space of
    possible programs evenly, instead, they tend to be structured among certain
    design principles.
    The same is true algorithmically and particularly for performance intensive
    applications.

\section{Literature Survey}

\subsection{Algorithmic Skeletons}

\subsection{Berkeley Parallel Dwarves}
\subsection{Computational Patterns}

\section{Idiom Specific Optimizations}

\subsection{Important Approaches}
\subsection{Ways of Encapsulating Expertise}


\subsection{Constraint Programming for Program Analysis}

    There is a large body of work that uses constraints for program analysis.
    Constraint systems have long been used in program analysis for classical
    purposes such as dataflow analysis and type inference
    \citep{Aiken:1999:ISC:339853.339897}.
    \citet{Gulwani:2008:PAC:1375581.1375616} showed how constraints can be
    used to for some  existing analysis problems.
    It can also be used to assist generation of programs from specifications
    \citep{Srivastava:2010:PVP:1707801.1706337}.

    There is considerable work on formally verifying existing compiler
    transformations using SMT solvers and theorem provers that operate on IR
    code, among them \cite{Zhao:2012:FLI:2103656.2103709}.
    Recent domain specific language for formally verifying compiler
    optimizations, such as Alive \cite{Lopes:2015:PCP:2737924.2737965} operate
    on single static assignment compiler IR as well.
    However, it has no support for control flow and is limited to simple
    peephole optimizations.

    Further approaches to generating compiler optimizations that focus on formal
    verification instead of programmer productivity include Rhodium
    \citep{Lerner:2005:ASP:1040305.1040335}, PEC
    \citep{Kundu:2009:POC:1543135.1542513} and Gospel
    \citep{Whitfield:1997:AEC:267959.267960}.
    The CompCert tool by \citet{Mullen:2016:VPO:2908080.2908109} verifies peephole
    optimizations directly on x86 assembly code and LifeJacket
    \cite{Notzli:2016:LVP:2931021.2931024} proves the correctness of floating
    point optimizations in LLVM, as does Alive-FP \cite{Menendez2016}, which
    also generates C++ code.
    The authors of \cite{Tate:2010:GCO:1706299.1706345} investigate the
    automatic generation of optimization transformations from examples.
    None of the approaches however, tackle the issue of efficiently writing
    arbitrary compiler analysis passes.

    \citet{Martin1998} present a specification language for program analysis
    functionality called PAG that is based on abstract interpretation.
    Domain specific languages for the conception of optimization passes have
    also been studied before using tree rewrites, among them
    \citet{Olmos:2005:CSD:2136624.2136643}.
    \citet{Lipps1989} propose the domain specific language OPTRAN for matching
    patterns in attributed abstract syntax trees of Pascal programs.
    These patterns can then be automatically replaced by semantically
    equivalent, more efficient implementations. 
    Generic programming
    concepts can also be used to generate optimization passes, as
    demonstrated by \cite{Willcock:2009:RGP:1621607.1621611}.  These
    schemes however are not able to work at the IR level essential for
    compiler implementation and do not scale beyond simple functions.

    As opposed to code transformation techniques based on the LLVM ASTMatcher
    library and LibTooling \citep{be0fa11ddb194bde86a9dab8589b779c}, we work on
    compiler intermediate representation and are independent from the compiler
    frontend.
    This makes us integrate well with the existing optimization infrastructure,
    allows us to be language independent and makes our approach robust to
    shallow syntactic changes.

    Another language for implementing optimizations that emphasizes developer
    productivity is OPTIMIX \citep{Assmann1996,Assmann98optimix}, based on graph
    rewrite rules.
    OPTIMIX programs are compiled into C code that performs the specified
    transformation.
    A domain specific language for the generation of optimization
    transformations was also used in the CoSy compiler \citep{Alt1994}.
    These are simple rewrite engines and have no knowledge of global program
    constraints.

    Other work has investigated the use of constraint solvers for
    detecting structure in LLMV IR.
    \citet{ginsbach2017discovery} use a solver to detect histograms and
    scalar reductions in order to automatically parallelize code.
    A different important approach to detecting structure in intermediate
    code is the polyhedral model.
    Compilers using this model, such as
    \citet{Lengauer2012Polly}, \citet{Baskaran:2010:ACC:2175462.2175482} and
    \citet{Verdoolaege:2013:PPC:2400682.2400713}, capture well behaved
    loops with affine array accesses and are able to perform advanced loop
    optimizations.
    Recent work has investigated performant reduction computations on GPUs with
    the polyhedral model \citep{Reddy2016Reduction}.
    However this approach is not easily extensible to other program structures
    and captures only a very specific class of well behaved programs.

    In \cite{Yuan:2017:TOS:3101282.3101287}, the authors propose the use of the
    Web Ontology Language (WOL) for the description of software architectures.
    This representation enables automatic reasoning and analysis about the
    interoperability of software architectures.