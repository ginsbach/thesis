    Four areas of research are of particular relevance to this thesis:
    {\bf Constraint programming} is central to the introduced methodology, with
    the relevant literature including the research into constraint programming
    in the context of program analysis and to a lesser extent the design of
    constraint programming languages.
    The survey of previous approaches to
    {\bf compiler analysis and auto-parallelisation}
    establishes the baselines for the later evaluation sections.
    Related work on {\bf heterogeneous computing} with its many different
    programming paradigmes to overcome the specific challenges of emerging
    hardware will serve to motivate the proposed approaches.
    Lastly, the diverse research landscape around concepts related to
    {\bf computational idioms} will put our idiomatic approach into context.

\section{Constraint Programming}

    Constraint programming and its use for program analysis has been studied
    extensively, covering several major research areas around security and
    formal verification as well as more compiler centric dataflow analysis and
    type inference.
    It has to be understood in the context of research fields such as SMT
    solving, theorem proving and model checking, but important background for
    this thesis also comes from the programming language perspective,
    particularly around the prolog community.
    These different fields vary greatly in their interests and approaches, but
    many of the underlying challenges can be similar, especially performance and
    scalability issues of backtracking solvers.

    Constraint systems have long been used in program analysis for classical
    purposes such as dataflow analysis and type inference, as in
    \citet{Aiken:1999:ISC:339853.339897}.
    \citet{Gulwani:2008:PAC:1375581.1375616} showed how constraints can be
    used to for some  existing analysis problems.
    It can also be used to assist generation of programs from specifications,
    as in \citet{Srivastava:2010:PVP:1707801.1706337}.

    There is particularly extensive work in the area of security and formal
    verification of software systems.
    This work inxludes investigations into formally verifying existing compiler
    transformations using SMT solvers and theorem provers that operate on IR
    code, among them \citet{Zhao:2012:FLI:2103656.2103709}.

    Recent domain specific languages for formally verifying compiler
    optimizations, such as Alive \citep{Lopes:2015:PCP:2737924.2737965} operate
    on single static assignment compiler IR as well.
    However, it has no support for control flow and is limited to simple
    peephole optimizations.

    Further approaches to generating compiler optimizations that focus on formal
    verification instead of programmer productivity include Rhodium
    \citep{Lerner:2005:ASP:1040305.1040335}, PEC
    \citep{Kundu:2009:POC:1543135.1542513} and Gospel
    \citep{Whitfield:1997:AEC:267959.267960}.
    The CompCert tool by \citet{Mullen:2016:VPO:2908080.2908109} verifies
    peephole optimizations directly on x86 assembly code and LifeJacket
    \cite{Notzli:2016:LVP:2931021.2931024} proves the correctness of floating
    point optimizations in LLVM, as does Alive-FP \cite{Menendez2016}, which
    also generates C++ code.
    The authors of \cite{Tate:2010:GCO:1706299.1706345} investigate the
    automatic generation of optimization transformations from examples.
    None of the approaches however, tackle the issue of efficiently writing
    arbitrary compiler analysis passes.

    \citet{Martin1998} present a specification language for program analysis
    functionality called PAG that is based on abstract interpretation.
    Domain specific languages for the conception of optimization passes have
    also been studied before using tree rewrites, among them
    \citet{Olmos:2005:CSD:2136624.2136643}.
    \citet{Lipps1989} propose the domain specific language OPTRAN for matching
    patterns in attributed abstract syntax trees of Pascal programs.
    These patterns can then be automatically replaced by semantically
    equivalent, more efficient implementations. 
    Generic programming
    concepts can also be used to generate optimization passes, as
    demonstrated by \cite{Willcock:2009:RGP:1621607.1621611}.  These
    schemes however are not able to work at the IR level essential for
    compiler implementation and do not scale beyond simple functions.

    Different to code transformation techniques based on the LLVM ASTMatcher
    library and LibTooling \citep{be0fa11ddb194bde86a9dab8589b779c}, this work
    is on compiler intermediate representation and independent from
    the compiler frontend.
    This makes it possible integrate well with the existing optimization
    infrastructure, allows the approach to be language independent and makes it
    approach robust to shallow syntactic changes.

    Another language for implementing optimizations that emphasizes developer
    productivity is OPTIMIX \citep{Assmann1996,Assmann98optimix}, based on graph
    rewrite rules.
    OPTIMIX programs are compiled into C code that performs the specified
    transformation.
    A domain specific language for the generation of optimization
    transformations was also used in the CoSy compiler \citep{Alt1994}.
    These are simple rewrite engines and have no knowledge of global program
    constraints.

    Other work has investigated the use of constraint solvers for
    detecting structure in LLMV IR.
    \citet{ginsbach2017discovery} use a solver to detect histograms and
    scalar reductions in order to automatically parallelize code.
    A different important approach to detecting structure in intermediate
    code is the polyhedral model.
    Compilers using this model, such as
    \citet{Lengauer2012Polly}, \citet{Baskaran:2010:ACC:2175462.2175482} and
    \citet{Verdoolaege:2013:PPC:2400682.2400713}, capture well behaved
    loops with affine array accesses and are able to perform advanced loop
    optimizations.
    Recent work has investigated performant reduction computations on GPUs with
    the polyhedral model \citep{Reddy2016Reduction}.
    However this approach is not easily extensible to other program structures
    and captures only a very specific class of well behaved programs.

    In \cite{Yuan:2017:TOS:3101282.3101287}, the authors propose the use of the
    Web Ontology Language (WOL) for the description of software architectures.
    This representation enables automatic reasoning and analysis about the
    interoperability of software architectures.

\section{Compiler Analysis and Auto-Parallelisation}

    This work applies constraint programming approaches to compiler analysis
    problems in a novel way.
    In order to evaluate this, traditional compiler analysis approaches are
    used for comparison.

\subsection{Compiler centric linear algebra optimization}

    Compiler management of indirect memory accesses was already studied in the
    1980s using an inspector-executor model for distributed-memory machines in
    \citet{Baxter:1989:RPS:72935.72967}.
    Here the location of read data was discovered at runtime and appropriate
    communication automatically inserted.
    Later work by \citet{pottenger1995idiom,fisher1994parallelizing,
    rauchwerger1999lrpd,suganuma1996detection} was focused on efficient runtime
    dependence analysis and the parallelization of more general programs.
    However, the performance achieved by these approaches is modest due to
    runtime overhead and falls well short of library performance.

\subsection{Polyhedral compiler approaches}

    The polyhedral model is a well established approach for modeling data
    dependencies that has been used by several compilers, with relevant
    publications by \citet{redon1994scheduling, jouvelot1989unified,
    chi1997optimizing, gupta2006simplifying, stock2014framework}.
    Polyhedral optimizers have been implemented in mainstream C/C++ compilers,
    notably are the Polly extensions to LLVM described in
    \cite{Doerfert2015Polly}.
    Recent work by \citet{7429301} has extended the polyhedral model beyond
    affine programs to some forms of sparsity with the PENCIL extensions.
    These can be used to model important features of sparse linear algebra, such
    as counted loops \citep{Zhao:2018:PCF:3178372.3179509}, meaning loops with
    dynamic, memory dependent bounds but statically known strides.
    Such loops are central to sparse linear algebra.
    It uses the PPCG compiler \citep{Verdoolaege:2013:PPC:2400682.2400713} to
    automatically detect relevant code regions, but it relies on well behaved C
    code with all arrays declared in variable-length C99 array syntax.
    Unfortunately such restrictions prevent application to real world programs;
    none of our benchmarks or data sets have this structure.

\subsection{Compiler detection}

    There has been previous work on
    detecting code structures during compilation using constraint programming.
    Early work was based on abstract computation graphs
    \cite{pinter1994program}, but more recent approaches have used real
    compiler intermediate code and made connections to the polyhedral
    model \cite{Ginsbach:2018:CDS:3178372.3179515}.

    In \cite{Ginsbach:2018:AML:3173162.3173182} they implement a method that
    operates on SSA intermediate representation.
    %It is based on the Idiom Description Language (IDL), a domain specific
    %constraint programming language.
    %There are several fundamental differences to our approach:
    %Firstly, IDL
    It uses a  general-purpose low level constraint programming language  aimed at
    compiler engineers. % , not library implementers.
    %It is difficult to use and
    % requires a deep understanding of compiler internals.
    %Secondly,
    The %methodology of the
    paper focuses  on code detection, with library calls inserted by hand.
    %the
    %necessary replacement functionality is then implemented manually.
    It is performed naively without the use of harnesses and the results show
    significant performance degradation due to the redundant memory transfers that
    have to be amended manually.
    Other advanced approaches to extracting higher
    level structures from assembly and well structured FORTRAN code
    involve temporal logic \cite{Mendis2015Helium, Kamil2016Verified}.
    These approaches tend to focus on a more restricted set of
    computations (dense memory access).
    While this allows formal reasoning about correctness, is too restrictive to
    model sparse linear algebra.

\section{Heterogeneous Computing}

    Heterogeneous computing has been a particularly active research field
    recently, especially since the widespread adoption of GPUs for general
    purpose computations in the last decade.
    This includes research from a hardware perspective, studying
    the most meaningful ways of diversification for processors in heterogeneous
    systems as well as many different programming approaches, broadly
    categorised into library approaches, compiler based methods and proposed
    DSLs.

    For the context of this thesis, the most relevant work is on programming
    approaches and for direct comparison also the compiler based methods.

\subsection{Domain Specific Languages}

    Many domain specific languages have been proposed for efficient and
    performant programming of heterogeneous systems.
    They allow implementers to restrict the compiler and runtime away from
    general purpose programming concepts that are difficult to support on
    specific hardware.
    Domain specific languages can be stand alone with an entire tool chain and
    runtime ecosystem of be embedded in existing languages, such as C++ or
    Scala.
    DSLs range in complexity from onlymarginally more flexible than library
    interfaces to full-fledged programming languages such as OpenCL and CUDA.

    There have been multiple domain specific libraries proposed specifically
    for linear algebra computations.
    Many of these contain some degree of autotuning functionality to achieve
    good performance across different platforms.
    The Delite langauge of \citet{Sujeeth:2014:DCA:2601432.2584665} can be used
    as an intermediate representation to construct domain specific languages for
    heterogeneous platforms.
    The approach is tightly integrated with the Scala programming language
    and does not offer a readily available end-to-end solution.
    Halide, as proposed by \citet{Ragan-Kelley:2013:HLC:2499370.2462176}
    was designed for image processing, but is flexible enough to also allow the 
    formulation of matrix multiplications and other computations.
    \citet{Suriana:2017:PAR:3049832.3049863} demonstrates that this can extend
    to reduction computations as well.
    Its core design decision is the scheduling model that allows the separation
    of the computation schedule and the actual computation.
    There has been work on automatically tuning the schedules, e.g.\ 
    \citet{Mullapudi:2016:ASH:2897824.2925952}, but in general the computational
    burden is put on the application programmer.

    The LIFT programming language by \citet{Steuwer:2015:GPP:2858949.2784754}
    provides composable constructs from functional programming to implement
    data-parallel algorithms and operations.
    It is particularly suitable for dense linear algebra applications
    \citep{Steuwer:2016:MMB:2968455.2968521}, but extensions to sparsity exist
    \citep{b5f556d505d746109608f8db3cbce4ac}.
    The MILK programming model \citep{Kiriansky:2016:OIM:2967938.2967948}
    extends C++ with pragmas to annotate indirect memory accesses.
    This allows low level optimizations that are applicable to sparse linear
    algebra.
    The authors report performance gains of up to 3x, but the approach is unable
    to utilize the much greater potential of heterogeneous compute and requires
    detailed programmer intervention.

\subsection{Libraries}

    The most established way of encapsulating fast linear algebra is via
    library implementations, generally based on the BLAS interface
    specification \cite{2002:USB:567806.567807}.
    These are generally very fast on their specific hardware platforms, but
    require application programmer effort and offer little performance portability.
    Implementations of dense linear algebra are available for most suitable
    hardware platforms, such as cuBLAS \cite{cublas} for NVIDIA GPUs, clBLAS
    \cite{clblas} for AMD GPUs and the Intel MKL library \cite{mkl} for Intel
    CPUs and accelerators.

    There are fewer implementations of sparse linear algebra, but they exist for
    the most important platforms, including cuSPARSE \cite{cusparse} for NVIDIA
    GPUs and clSPARSE \cite{clsparse} built on top of OpenCL.
    Several BLAS implementations attempt platform independent acceleration and
    heterogeneous compute, among them \citet{Wang:2016:BHP:2925426.2926256,
    10.1007/978-3-319-64203-1_33, Diego2017Multi}. 

%\paragraph{Graphs as sparse linear algebra}
%Graph processing is an important computational domain, especially in the context
%of large social media graphs \cite{5470687, 6408680}.
%The duality between graph processing and sparse linear algebra is well studied
%\cite{doi:10.1137/1.9780898719918} and has lead to the development of the
%GraphBLAS interface standard \cite{6670338,7761646}.
%GraphBLAS attempts to define a standard set of building blocks for graph
%algorithms that use the language of linear algebra.

\subsection{CPU-GPU data transfer optimizations}

    Data transfers between CPU and GPU have been studied extensively as
    important bottlenecks for parallelization efforts.
    Previous work \cite{Jablin:2011:ACC:1993316.1993516} established a system
    for automatic management of CPU-GPU communication.
    The authors of \cite{Lee:2009:OGC:1594835.1504194} implemented a system to
    move OpenMP code to GPUs, optimizing data transfers using data flow
    analysis.
    However, this approach performs a direct translation of the code, without
    optimizing it for the specific performance characteristics of GPUs.


%% EArly scalar detection sc
%Discovering and exploiting scalar reductions in programs has been
%studied for many years based on dependence analysis and idiom detection
% \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
%Early work focused on well structured Fortran and
%paid little attention to compiler-based detection, a notable exception being. 
%\cite{suganuma1996detection}. 
%In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
% approaches and developed  a dynamic test to speculatively exploit
% reduction parallelism.
%%%Early ploy detection
%Alongside this data dependence based approach, there has also been a
% large body of work exploring mapping of reductions in a
% polyhedral setting \cite{redon1994scheduling, jouvelot1989unified}
%
%%% Expoitation of genearlise reductiosns 
% The treatment of
%more general reduction operations has received  less attention
%Work has focused on exploitation rather than discovery
%\cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000}, examining trade-offs in implementation \cite{yu2006adaptive}
% or exploitation of novel hardware \cite{ravi2010compiler,Huo2011HiPC}
%In \cite{das2010experiences}, they use dynamic profile analysis to
%guide manual analysis and show there is potential for finding
%generalized reductions. In \cite{kim2012dynamic} they explore the use
%of dynamic analysis further, but state that detecting reductions on arrays is
%challenging.
%
%
%
%%% LLVM out there \cite{lattner2004llvm}
%%% What NAS:\cite{seo2011performance}
%
%%% Vaguely related C;
%
%
%%% New polyhedral approaches
%
%More recently, extensions
%to the polyhedral framework have been proposed, allowing  it to capture
%reduction computations \cite{chi1997optimizing, gupta2006simplifying,
%stock2014framework} Such efforts are described
%in \cite{Doerfert2015Polly}.  The authors discuss and implement a
%reduction-enabled scheduling approach as part of Polly and use the
%Polybench benchmark suite to evaluate it, achieving speedups of up to
%2.21x. However as shown in our evaluation section, such schemes are
%fragile in the presence of non static control flow.
%
%%% Language Poly and non-polly
% The difficulty in automatically detecting
%reductions has led to language or annotation based approaches where it
%is the user's responsibility to mark reductions in the
%program \cite{deitz2002high}.
%%% Another approach to incorporate the reduction idiom into the Polyhedral Framework using annotations is described in
%An annotation approach is described in 
%\cite{Reddy2016Reduction}, based on the Platform-Neutral Compute Intermediate Language (\cite{baghdadi2015PENCIL}).
%This used the code generator
%in (\cite{Verdoolaege2013Polyhedral}) to generate CUDA and OpenCL code for multiple compute platforms.
%
%
%
%%% New dynamic approaches
%There has also been recent work following on
%from \cite{rauchwerger1999lrpd} in using more aggressive
%speculation and dynamic analysis \cite{aguilar2015unified} to exploit reduction parallelism.
%The authors of \cite{Han2010Speculative} present an approach for the parallelization of a wide class of scalar reductions.
%They start from the observation that many reductions in real benchmark programs are not detected by current static analysis approaches.
%They propose a hardware assisted speculative parallelization approach for likely runtime reductions, denominated `partial reduction variables'.
%Candidates for speculative parallelization are determined by searching for update-chains in the data flow graph.
%The approach is evaluated on some of the SPEC2000 benchmarks using a simulator.
%They achieve up to $46\%$ speedup by including speculative reductions.
%This approach
%based on update chains in the dependence graph requires hardware
%speculation support to check dependences but is unable to detect
%histogram reductions.
%
%Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a complex
%system featuring compiler support and a runtime to enable speculative
%parallelization.  The core approach is the privatization of memory
%for each thread and an exception mechanism with recovery routines for
%accesses that violate parallelism.  The authors explicitly allow for
%reduction parallelism that involves only a single scalar associative
%and commutative operator.  The implementation approach is to first
%profile the program for hot loops and then to classify all variables
%accessed in those loops into different groups.  A transformation pass
%then identifies corresponding \texttt{malloc} and \texttt{free} calls
%and replaces them by thread local allocations.  In a similar way
%all\texttt{load} and \texttt{store} instructions are replaced by local
%accesses.  At runtime, the system uses manual page table switching and
%memory protection to minimize runtime overhead.  The evaluation is
%done on a limited set of five benchmark programs, yielding a geometric
%mean speedup of 11.4x on a 24 core machine.  The runtime overhead on
%these five programs varies between $<1\%$ and $>50\%$.  Despite this
%complexity they only exploit simple scalar reductions.
%
%In \cite{Andion2015Compilation}, it describes a compiler based
%parallelization approach for heterogeneous computing that is based on
%an idiomatic intermediate representation called KIR.  This
%intermediate representation is based on the concept
%of \mbox{diKiernels}, which constitute algorithmic building blocks and
%are used to automatically generate OpenMP and OpenHMPP code.  The
%authors propose a system that detects diKernels in conventional
%compiler IR and concatenates them to form contiguous sections of KIR.
%Individual examples of diKernels are scalar reductions and irregular
%assignments. It is not clear how such an approach would work on general
%'C' programs.
%



\section{Computational Idioms}

    The concept of computational idioms has been observed in different contexts
    and remains a rather vague concept.
    While terms such as \texttt{reduction}, \texttt{stencil} and
    \texttt{linear algebra} are commonly used, the concrete concepts can be
    surprisingly vague, although previous work has established several formal
    approaches.
    This chapter not try to create new formal definitions, but instead want to
    give an overview of the literature that sheds different perspectives on this
    topic.

    The basic observation is that software programs don't cover the space of
    possible programs evenly, instead, they tend to be structured among certain
    design principles.
    The same is true algorithmically and particularly for performance intensive
    applications.


\section{CGO related}
%% EArly scalar detection sc
Discovering and exploiting scalar reductions in programs has been
studied for many years based on dependence analysis and idiom detection
 \cite{pottenger1995idiom,suganuma1996detection,fisher1994parallelizing}.
Early work focused on well structured Fortran and
paid little attention to compiler-based detection, a notable exception being. 
\cite{suganuma1996detection}. 
In \cite{rauchwerger1999lrpd}, the authors went beyond previous static
 approaches and developed  a dynamic test to speculatively exploit
 reduction parallelism.
%%Early ploy detection
Alongside this data dependence based approach, there has also been a
 large body of work exploring mapping of reductions in a
 polyhedral setting \cite{redon1994scheduling, jouvelot1989unified}

%% Expoitation of genearlise reductiosns 
 The treatment of
more general reduction operations has received  less attention
Work has focused on exploitation rather than discovery
\cite{gutierrez2003optimization,gutierrez2008analytical, Gutierrez:2000}, examining trade-offs in implementation \cite{yu2006adaptive}
 or exploitation of novel hardware \cite{ravi2010compiler,Huo2011HiPC}
In \cite{das2010experiences}, they use dynamic profile analysis to
guide manual analysis and show there is potential for finding
generalized reductions. In \cite{kim2012dynamic} they explore the use
of dynamic analysis further, but state that detecting reductions on arrays is
challenging.



%% LLVM out there \cite{lattner2004llvm}
%% What NAS:\cite{seo2011performance}

%% Vaguely related C;


%% New polyhedral approaches

More recently, extensions
to the polyhedral framework have been proposed, allowing  it to capture
reduction computations \cite{chi1997optimizing, gupta2006simplifying,
stock2014framework} Such efforts are described
in \cite{Doerfert2015Polly}.  The authors discuss and implement a
reduction-enabled scheduling approach as part of Polly and use the
Polybench benchmark suite to evaluate it, achieving speedups of up to
2.21x. However as shown in our evaluation section, such schemes are
fragile in the presence of non static control flow.

%% Language Poly and non-polly
 The difficulty in automatically detecting
reductions has led to language or annotation based approaches where it
is the user's responsibility to mark reductions in the
program \cite{deitz2002high}.
%% Another approach to incorporate the reduction idiom into the Polyhedral Framework using annotations is described in
An annotation approach is described in 
\cite{Reddy2016Reduction}, based on the Platform-Neutral Compute Intermediate Language (\cite{baghdadi2015PENCIL}).
This used the code generator
in (\cite{Verdoolaege:2013:PPC:2400682.2400713}) to generate CUDA and OpenCL code for multiple compute platforms.



%% New dynamic approaches
There has also been recent work following on
from \cite{rauchwerger1999lrpd} in using more aggressive
speculation and dynamic analysis \cite{aguilar2015unified} to exploit reduction parallelism.
The authors of \cite{Han2010Speculative} present an approach for the parallelization of a wide class of scalar reductions.
They start from the observation that many reductions in real benchmark programs are not detected by current static analysis approaches.
They propose a hardware assisted speculative parallelization approach for likely runtime reductions, denominated `partial reduction variables'.
Candidates for speculative parallelization are determined by searching for update-chains in the data flow graph.
The approach is evaluated on some of the SPEC2000 benchmarks using a simulator.
They achieve up to $46\%$ speedup by including speculative reductions.
This approach
based on update chains in the dependence graph requires hardware
speculation support to check dependences but is unable to detect
histogram reductions.

Privateer introduced in \cite{Johnson:2012:SSP:2254064.2254107}, is a complex
system featuring compiler support and a runtime to enable speculative
parallelization.  The core approach is the privatization of memory
for each thread and an exception mechanism with recovery routines for
accesses that violate parallelism.  The authors explicitly allow for
reduction parallelism that involves only a single scalar associative
and commutative operator.  The implementation approach is to first
profile the program for hot loops and then to classify all variables
accessed in those loops into different groups.  A transformation pass
then identifies corresponding \texttt{malloc} and \texttt{free} calls
and replaces them by thread local allocations.  In a similar way
all\texttt{load} and \texttt{store} instructions are replaced by local
accesses.  At runtime, the system uses manual page table switching and
memory protection to minimize runtime overhead.  The evaluation is
done on a limited set of five benchmark programs, yielding a geometric
mean speedup of 11.4x on a 24 core machine.  The runtime overhead on
these five programs varies between $<1\%$ and $>50\%$.  Despite this
complexity they only exploit simple scalar reductions.

In \cite{Andion2015Compilation}, it describes a compiler based
parallelization approach for heterogeneous computing that is based on
an idiomatic intermediate representation called KIR.  This
intermediate representation is based on the concept
of \mbox{diKiernels}, which constitute algorithmic building blocks and
are used to automatically generate OpenMP and OpenHMPP code.  The
authors propose a system that detects diKernels in conventional
compiler IR and concatenates them to form contiguous sections of KIR.
Individual examples of diKernels are scalar reductions and irregular
assignments. It is not clear how such an approach would work on general
'C' programs.

