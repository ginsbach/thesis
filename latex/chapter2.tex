
    The research contributions of the thesis are built on a new method for
    constraint programming on Static Single Assignment (SSA) compiler
    intermediate representation.
    This chapter derives and motivates the underlying methodology, which is
    then used as the basis for
    \Cref{chapter:candl,chapter:reductions,chapter:idioms}.
    The constraint programming approach is developed in three steps.

    First, an overview of program representations during different compilation
    stages is given, and the particular features of SSA representations are
    highlighted.
    These features are used to derive a mathematical characterisation of
    the static structure of SSA programs, the {\it SSA model}.
    This is first derived generically and then demonstrated
    in more detail on \mbox{LLVM IR}, a specific SSA intermediate
    representation.
    Some mathematical notation is used in this section to build a reliable
    foundation for later parts of the thesis.
    The List of Symbols and Notation contains some of the presumed notational
    conventions.

    After the derivation of the SSA model, the concept of SSA constraint
    problems is defined.
    These are formulas that impose restrictions on parts of SSA code,
    formulated as constraints on the SSA model.
    SSA constraint problems can define code structures such as loops.
    Detecting adhering code sections in SSA code is then equivalent to finding
    solutions to the constraints.
    The structure of several classes of constraint formulas is discussed,
    reflecting conventional compiler analysis methods like data flow,
    dominance relationships, and data type restrictions.

    Finally, efficient algorithms for solving SSA constraint problems are
    derived.
    Backtracking is used to find solutions quickly by incrementally extending
    partial solutions.
    The structure of specific classes of SSA constraint problems is analysed,
    and efficient backtracking solutions are derived individually, yielding
    composable building blocks for the solver.
    After deriving the algorithms, implementation considerations are
    discussed.
    Suitable data structures are selected, and runtime complexity is analysed.
    To conclude the chapter, constraint programming on SSA intermediate
    representation is compared with Satisfiability Modulo Theory (SMT) problems.

\section{Background}

    Modern compilers for procedural languages such as
    C/C++, Fortran or JavaScript typically use a succession of different
    representations for the program during compilation.
    They reflect the requirements of the compilation stages in which they are
    used.

    \begin{description}[style=unboxed,leftmargin=0cm,labelindent=\parindent]
    \item[Front end representations] are close to the source program and
    strongly influenced by the specific grammar of the programming language.
    Typically, they are built around an abstract syntax tree with additional
    annotations, such as type information.
    These representations are rich in information about syntactic and stylistic
    choices of the programmer, and they scale in complexity with the source
    language.
    Some advanced language features, such as overloaded operators, are not
    resolved in this class of representations.
    Therefore, the semantics of program parts is highly dependent on context.

    \item[\bf Back end representations] are based on a model of the target
    hardware.
    They typically approach an assembly-style format and expose the instruction
    set architecture of the hardware, making them platform-specific.
    Back end representations also encode decisions for problems that are removed
    from the algorithmic content of the user program, such as instruction
    selection and register allocation.

    \item[\bf Middle end representations] are designed to enable the analysis
    and transformation of code in order to apply optimisations.
    {\bf Static Single Assignment} (SSA) representations have emerged as a
    common choice for the middle end in leading compilers.
    SSA abstracts away the complexities of the source language and the
    target architecture, focusing on a relatively simple description of the
    user program semantics.
    This enables reliable analysis and platform-independent reasoning.
    \end{description}

    Static Single Assignment was first proposed by
    \citet{Rosen:1988:GVN:73560.73562}, and there are now many different
    compiler intermediate representations that implement the concept.
    Instruction sets, type systems and syntax
    (if a textual representation is even specified) of these representations
    vary considerably, depending on the requirements of the source languages
    (static or dynamic) and the operating constraints
    (just-in-time or ahead-of-time).
    Some prominent examples of compilers using SSA are
    {\bf Clang} (LLVM IR), {\bf GCC} (GIMPLE), {\bf v8 Crankshaft} (Hydrogen),
    and {\bf SpiderMonkey} (IonMonkey/MIR).
    Despite many differences, they share the same basic structure that is
    discussed in this chapter.

    SSA form was developed as an improvement over previously used
    compiler intermediate representations.
    Specifically, the eponymous Static Single Assignment property applies to the
    more general Linear Intermediate Representations as an additional
    restriction, as described in \citet{Torczon:2007:EC:1526330}.
    The following \Cref{sec:ssaf} gives an overview of the characteristic shared
    features of SSA representations that are relevant to this work.

\subsection{Static Single Assignment Form}
\label{sec:ssaf}

    SSA representations are Linear Intermediate Representations, meaning that
    they represent each function as a single linear sequence of instructions.
    These instructions operate on an unlimited number of registers, using a well
    defined -- but representation specific -- instruction set.
    Branch instructions are used to redirect the execution conditionally or
    unconditionally.
    Consecutive instructions with no branching between them are grouped into
    basic blocks.
    Within these, instructions are executed in order of appearance.
    Basic blocks may have labels or be identified simply by enumerating them.
    Instruction arguments can be registers, constants, globals or function
    parameters.
    Additionally, branch instructions take basic block arguments as branch
    targets.
    Instructions may write their result into a single output register.

    The SSA property stipulates that within a function, no register can be
    written at more than one static location.
    This means that registers can be identified directly with the instructions
    that define them.
    The registers can, therefore, be made implicit, with only the data flow
    between instructions required to recover them.
    In the presence of dynamic control flow, \mbox{$\Phi$-instructions} are
    required to uphold the SSA property.
    These \mbox{$\Phi$-instructions} are placed at the beginning of a basic
    block, and select one of several values dynamically, depending on the origin
    of incoming branches, i.e.\ depending on the previously executed
    basic block.

    \Cref{fig:ssaoverview} shows how SSA programs can be represented as a
    hierarchy of lists.
    Programs are lists of functions, which are lists of basic blocks, which are
    lists of instructions, which take lists of arguments.
    The ability to enumerate all these entities in linear sequences enables the
    identification of SSA values with integer indices in the later sections of
    this chapter.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/ssaoverview}
\caption{Structural overview of SSA: Programs are represented as hierarchies of
    lists. The SSA property makes registers implicit; values can be statically
    matched to defining instructions.}
\label{fig:ssaoverview}
\end{figure}

\subsection{SSA Emerges During Compilation}

    \Cref{ssaexample} shows how critical features of SSA form emerge from
    simplification steps that are applied to source code.
    This is demonstrated on the ``{\tt sqrt}'' example function that
    approximates the square root of a double-precision floating-point value
    in C using the Babylonian method in \Cref{babylonian_equation}.
    Beginning with an initial guess $x_0$, the approximation is improved
    iteratively.

\begin{equation}
    x_0\approx\sqrt{S},\text{\hspace{1cm}}
    x_{n+1}=\frac{x_n+\frac{S}{x_n}}{2}\text{\hspace{1cm}}
    \implies\text{\hspace{1cm}}
    \lim_{n\rightarrow\infty}x_n=\sqrt{S}
    \label{babylonian_equation}
\end{equation}

    Starting from the source code {\bf(a)} at the top left of the figure, the
    function is first modified by breaking down complex expressions.
    The expressions are turned into sequences of basic operations, shown at the
    top right {\bf(b)}.
    Explicit variables appear for the previously implicit temporary values.
    This simplifies the program, with many of the operations directly mapping to
    individual processor instructions.
    Furthermore, different input programs get mapped to the same,
    predictable output by this transformation, making it a normalisation.

    In a second step, the structured control flow of the program is replaced
    with goto statements that coordinate the control flow between basic blocks.
    This is shown at the bottom right {\bf(c)}.
    Although this does not ease the intuitive understanding of the program, it
    unifies several distinct control flow structures provided in the source
    language into a single mechanism.
    This simplifies program analysis.
    Importantly, no relevant information is lost by discarding the control flow
    structures.
    They can be reconstructed algorithmically.

    Finally, the Static Single Assignment property is introduced at the bottom
    left {\bf(d)}.
    Each variable that is assigned at more than one static location in the
    program is instead duplicated into multiple variables.
    Where necessary, these now distinct variables are bound together with
    $\Phi$-instructions.
    This cannot be expressed in the C programming language syntax.
    Instead, the behaviour is documented by comments at lines 8--13.

    The impact of the SSA property seems minor at first, but convenient
    implications can already be identified within the C language.
    As all local variables are written at exactly one static location, each
    variable can be declared and defined in the same place.
    This means that it is always known statically, which expression yielded the
    value of each variable.
    This immediately guarantees that the variable ``\texttt{i}'' in the example
    always has the value \texttt{0}, and that the variable~``\texttt{x}'' always
    has the value \texttt{1.0}.

    In summary, this section developed an understanding of how SSA originated
    historically and how it emerges during the compilation process.
    The following section uses the observations from
    \Cref{fig:ssaoverview} -- that most parts of SSA code can be enumerated
    and represented as elements in lists -- to derive a mathematical
    characterisation of the static structure of SSA programs.
    This characterisation then serves as the foundation of later sections, which
    define compiler analysis problems on it via constraints.

\begin{figure}[p]
    \input{latex/figure_ssaemergence}
    \caption{Static Single Assignment emerges from successive simplification
             and normalisation of source language features: 
             Demonstration on an example C function that approximates the
             square root of an input value with the Babylonian method.
             The transformation results {\bf (b-d)} are rendered in C.
             Real compilers typically operate on
             dedicated internal representations instead.}
    \label{ssaexample}
\end{figure}

\section{Deriving the SSA Model}

\begin{figure}[p]
\begin{definition}{Features of Static Single Assignment Functions}{ssa}
    For the remainder of this section, some function $\mathcal F$ in SSA form is
    assumed fixed. 
    The following identifiers are then used to describe the features of this
    function:

    \begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt,parsep=0pt]
    \item $|par|$ is the number of function parameters
          $par_1,\dots par_{|par|}$.
    \item $|ins|$ is the number of instructions $ins_1,\dots ins_{|ins|}$ that
          make up the function, in depth-first order, starting from the
          execution entry.
    \item $|glb|$ is the number of unique globals $glb_1,\dots glb_{|glb|}$ that
          are used as operands of any of the instructions.
    \item $|cst|$ is the number of unique constants $cst_1,\dots cst_{|cst|}$
          that are used as operands of any of the instructions.
    \end{itemize}
\end{definition}
\begin{definition}{List of Used Values}{usedvalues}
    The {\em list of used values} of the SSA function $\mathcal F$ is the tuple
    \begin{align*}
       val = (par_1,\dots,par_{|par|},
              ins_1,\dots,ins_{|ins|},
              glb_1,\dots,glb_{|glb|},
              cst_1,\dots,cst_{|cst|}).
    \end{align*}
    Furthermore, the following values are used to identify specific ranges in
    $val$:
    \begin{align*}
        L_{par}={}&1         & R_{par}={}&|par| &
        L_{ins}={}&R_{par}+1 & R_{ins}={}&R_{par}+|ins|\\[-1.5mm]
        L_{glb}={}&R_{ins}+1 & R_{glb}={}&R_{ins}+|glb| &
        L_{cst}={}&R_{glb}+1 & R_{cst}={}&R_{glb}+|cst|\\[-1.5mm]
        L_{val}={}&L_{par}   & R_{val}={}&R_{cst} &&&&
    \end{align*}
\end{definition}
\begin{definition}{Data Flow Graph}{dfg}
    The {\em data flow graph} of the SSA function $\mathcal F$ is the set
    $DFG_{\mathcal F}\subset \mathbb N^3$ such that
    \begin{align*}
        (n,a,b)\in DFG_{\mathcal F}\iff{}&\left(L_{val}\leq a\leq R_{val}\right)
            \mathrel{\land}\left(L_{ins}\leq b\leq R_{ins}\right)\\
            \mathrel{\land}{}&(val_a\text{ is the $n$th argument of }ins_{b-L_{ins}+1}).
    \end{align*}
    For $\Phi$-instructions, the incoming basic blocks are ordered according to
    the order on $ins$.
    The $n$th argument is the incoming value attached to the $n$th incoming
    basic blocks.
\end{definition}
\begin{definition}{Control Flow Graph}{cfg}
    The {\em control flow graph} of the SSA function $\mathcal F$ is the set
    $CFG_{\mathcal F}\subset \mathbb N^3$ such that
    \begin{align*}
        (n,a,&{}b)\in CFG_{\mathcal F}\iff\left(L_{ins}\leq a\leq R_{ins}\right)\mathrel\land\left(L_{ins}\leq b\leq R_{ins}\right)\\
               \mathrel{\land}&\left(\begin{aligned}[c]
                                    (\neg (ins_{a-L_{ins}+1}\text{ terminates basic block})\mathrel\land{}&(b=a+1)\mathrel\land(n=1))\\
                      \mathrel{\lor}(\phantom{\neg}(ins_{a-L_{ins}+1}\text{ terminates basic block})\mathrel\land{}&(ins_{b-L_{ins}+1}\text{ first instruction in}\text{ $n$th}\\[-0.5em]
                                                   &\text{target basic block of }ins_{a-L_{ins}+1}))\hspace{4mm}.\hspace{-4mm}
        \end{aligned}\right)
    \end{align*}
\end{definition}
\end{figure}

    This section develops a mathematical characterisation of SSA programs,
    denoted {\em SSA model}.
    The focus is on precise notation to express the static structure of existing
    SSA intermediate representations.
    The SSA model is unrelated to the operational semantics of programs and not
    a method for studying their behaviour at runtime.
    However, it contains all the information required to reconstruct a program
    that is semantically equivalent to the one from which it was extracted.
    The remainder of this section adheres to the naming conventions in
    \Cref{def:ssa}.

    This notation already implies several decisions about the SSA model that is
    derived in this section.
    Firstly, the model captures only a single function at once.
    Secondly, basic blocks are not explicitly encoded.
    Instead, all instructions of the function are enumerated sequentially in
    depth-first order, starting from the function entry.
    Basic blocks can, however, be reconstructed from the control flow, as
    discussed in \Cref{sec:remainingstructure}.
    Thirdly, the argument structure of the instructions is modelled separately.

\subsection{Data Flow and Control Flow}
\label{sec:derivingamodel}

    The data flow between instructions, as well as the control flow, is captured
    in graph structures.
    The Static Single Assignment property makes registers implicit, and the
    direct interaction between instructions becomes the natural model for data
    flow.
    Instruction arguments fall into four categories: function parameters ($par$),
    other instructions ($ins$), globals ($glb$), and constants ($cst$).
    Branching instructions also take basic block labels as branch targets,
    but those are treated separately in the control flow graph.
    All instruction arguments are therefore taken from the named lists
    introduced in \Cref{def:ssa}.

    For each function, the sequences $par$, $ins$, $glb$, $cst$ can be
    statically determined.
    Individual instruction arguments can, therefore, be encoded by one integer
    each, indexing into the union of those four sequences as in
    \Cref{def:usedvalues}.
    The entire argument structure of the instructions in an SSA
    function can then be turned into a labelled multigraph, with edge labels
    accounting for the positional order of the arguments.
    This is the {\it data flow graph} in \Cref{def:dfg}.
    The source of a data flow edge can be any value, but the target is
    always an instruction ($L_{ins}\leq b\leq R_{ins}$).

    Complementing the data flow graph is the {\em control flow graph} of the
    function, as introduced in \Cref{def:cfg}.
    The control flow graph is often defined with edges between basic blocks.
    However, in this definition, the edges are directly between instructions.
    This is convenient later, in
    \Cref{sec:constraintprogramming,sec:constraintsolving}, where both graphs
    can be treated identically.

    The defining equation can be separated into several parts.
    The first line expresses that edges in the control flow graph are always
    between two instructions.
    The remainder of the equation gives two options, corresponding to different
    types of edges in the control flow graph.
    Firstly, there are trivial edges within basic blocks.
    Secondly, there are edges between basic blocks.


\subsection{Identifying Remaining Structure}
\label{sec:remainingstructure}

    \Cref{sec:derivingamodel} introduced structures to model the control flow
    and data flow of SSA programs.
    This section identifies the remaining information that the SSA model needs
    in order to capture the program semantics fully.
    The benchmark for completion of the SSA model is the ability to recreate a
    semantically equivalent program in the SSA intermediate representation from
    it.
    Most of the program structure can already be recovered from $DFG_\mathcal F$
    and $CFG_\mathcal F$:

\begin{enumerate}
    \item The basic block boundaries are reconstructed by identifying all
          consecutive instructions $A$, $B$, where at least one of the following
          conditions is violated:
    \begin{itemize}
        \item $\{(n,a,b)\in DFG_\mathcal F^*\mid a=A\}=\{(1,A,B)\}$
        \item $\{(n,a,b)\in DFG_\mathcal F^*\mid b=B\}=\{(1,A,B)\}$.
    \end{itemize}
    \item Basic block labels and register names can be chosen freely
          without changing semantics.
    \item The arguments of all instructions can be immediately filled in from 
          $DFG_\mathcal F$.
          Similarly, $CFG_\mathcal F$ directly provides the target instructions
          for all goto statements.
    \item The positional arguments of $\Phi$-instructions in $DFG_\mathcal F$
          are attached as incoming values to the incoming basic blocks
          after ordering those according to the order on $ins$.
          The original positional order of the incoming pairs is
          not recovered, but it is semantically equivalent.
\end{enumerate}

    The only part of the SSA representation that still needs modelling is
    per-value information.
    This includes the opcodes of instructions, the values of constants,
    and type information.
    This is demonstrated in \Cref{fig:separation}.
    At the top of the figure is a simple function in an abstract SSA
    representation, which calculates an approximation of the square root of a
    number using the Babylonian method.
    It has 11 instructions separated into four basic blocks, with the majority
    of the instructions in a loop that iteratively improves the result.
    The entire semantic information that is encoded in this SSA representation
    can be recovered from the structures at the bottom of the figure:
    per-instruction opcode information, lists of the parameters, globals, and
    constants used, the data flow graph as in \Cref{def:dfg} and the control
    flow graph as in \Cref{def:cfg}.

    Instruction sets and type systems differ between SSA representations,
    although they overlap significantly.
    This chapter aims to capture commonalities of SSA representations, the study
    of instruction sets and type systems is orthogonal to this.
    These structures are, therefore, modelled as opaque sets as in
    \Cref{def:isatypes}.

\begin{figure}[h]
\begin{definition}{Representation Specific Sets}{isatypes}
    $\textit{Opcodes}_L$ is the set of all opcodes available in the SSA language $L$,
    $\textit{Types}_L$ is the set of all types in $L$,
    and $\textit{GlobalNames}_L$ is the set of all available names for global values.
\end{definition}
\end{figure}

\begin{figure}[p]
\input{latex/figure_ssadecomposition}
\caption{SSA representation is decomposed into individual instructions, data
         flow and control flow.
         This is an equivalent representation of the function; no semantic
         information is lost.
         The example is a rendering of the Babylonian method in
         \Cref{ssaexample}, abstracting away the C syntax.}
\label{fig:separation}
\end{figure}

\subsection{Putting the SSA Model Together}

\begin{figure}[p]
    \input{latex/figure_model}
\end{figure}

    With separate mathematical structures in place to capture all the relevant
    information contained in SSA programs, the SSA model can now be assembled.
    \Cref{def:ssamodel} shows the completed SSA model.
    The data flow graph $DFG_\mathcal F$ and the control flow graph
    $CFG_\mathcal F$ were discussed in detail previously, but some
    clarifications are provided for the remaining five structures.

    The {\it type model}, {\it instruction model}, and {\it constant model}
    assign additional information from different domains to the values used in
    the function.
    Instead of attaching a single type to each value, the type model
    allows values to be linked with several elements in the set {\it Types}.
    This is convenient to model subtyping hierarchies where, for example, an
    ``integer pointer'' value can also be a ``pointer''.
    The same is true for the instruction model, which enables it to express
    opcode categories, e.g.\ an ``arithmetic'' operation might be a
    ``subtraction'' in particular.
    The constant model is defined likewise as a subset of
    $\mathbb R\times\mathbb N$ but makes no use of the ability to assign
    multiple numeric values to the same constant.
    
    Finally, the {\it parameter model} and {\it global model} encode which
    elements in the list of used values of $\mathcal F$ are parameters and
    globals, respectively.
    Parameter names are not significant, because their position already
    identifies parameters uniquely within the function signature.
    Therefore, the parameter model identifies all parameters but attaches
    no additional data.
    For global values, on the other hand, the names are attached by the
    global model as elements from the set $GlobalNames$.

\subsection{Additional Notation}

    The seven basic components of the SSA model are each expressed as a set of
    tuples.
    In order to conveniently manipulate these structures in later sections, 
    \Cref{def:convenience} introduces several functions and shorthand notation.

    For a set of tuples, the function ``$heads$'' returns the set of all the
    first elements of the tuples.
    For example, $heads(I_\mathcal F)$ yields all the opcodes that are used in
    the function $\mathcal F$.
    In contrast, the function ``$tails$'' removes the first element of all
    tuples within a set.
    For example, $tails(I_\mathcal F)$ identifies the indices of all the
    instructions within the list of used values of $\mathcal F$ but removes the
    information about their opcodes.
    Finally, the ``$select$'' function is used to filter a set for only those
    tuples with a specific first element, and then returns the tails of all
    these tuples.
    For example, $select(add,I_\mathcal F)$ gives the indices of all additions
    within the list of used values of the function $\mathcal F$.

    The representation of data flow and control flow as labelled multigraphs
    contains more information than is required for many tasks.
    The simplified versions $DFG_\mathcal F^*$ and $CFG_\mathcal F^*$ are
    constructed with the ``$tails$'' function, effectively removing the labels
    and resulting in ordinary graph structures.
    Similarly, the sets $I_\mathcal F^*$ and $C_\mathcal F^*$ are used for
    shorter notation.

\subsection{The LLVM Compiler Framework}

    The previously introduced SSA model is generic and applies to all SSA
    compiler intermediate representations.
    However, it needs to be specialised to a specific representation in
    order to use it on real compiler problems.
    LLVM intermediate representation (LLVM IR) is one of the most common
    languages in that class, because of its use in the popular LLVM
    framework started by \citet{lattner2004llvm}.
    It will be used throughout the thesis for demonstration and evaluation.

    LLVM is a comprehensive compiler infrastructure project, using LLVM IR as
    its central abstraction.
    The LLVM project was formerly named ``Low Level Virtual Machine'', as the
    intermediate representation was conceptualised as a typed assembly-style
    language for a virtual machine.
    The instruction set of LLVM IR is roughly aligned to the semantics of
    C-style programming languages, but the project has matured beyond this
    background.
    It is now a widely influential framework with compiler front ends
    for a diverse set of languages, including C, C++, Haskell, Julia,
    Objective-C, Rust, Scala, and CUDA.

    Other mainstream compilers, such as the GCC project, use very similar
    representations internally.
    However, LLVM is particular in understanding the intermediate
    representation as an advertised and documented interface within the
    toolchain, as opposed to an obscure internal abstraction.
    This makes it very suitable for research implementations.

\subsection{LLVM IR Example}

    Some crucial features of LLVM IR in the context of this thesis are
    demonstrated on an example in \Cref{llvmirexample}.
    At the top {\bf (a)} of the figure, a dot product is implemented as a
    function in C.
    This function takes as arguments two pointers ``{\tt a}'', ``{\tt b}'' to
    arrays of double-precision floating-point values representing the input
    vectors, and an unsigned integer ``{\tt n}'' giving the size of the arrays.
    Within a single loop at lines 4--5, the dot product is accumulated in
    the variable ``{\tt d}'', which is eventually returned as the result of the
    function.

    At the bottom {\bf (b)} of the figure is the corresponding LLVM IR code,
    captured from the middle end of the LLVM-based Clang compiler after
    optimisations.
    Additional comments were inserted manually.
    Lines 2--4 are the result of an optimisation called ``loop inversion''.
    If the arrays are empty, the loop is skipped entirely, and the program
    returns zero via lines 6--8.
    Otherwise, the loop at lines 13--24 is entered via the basic block at lines
    10--11, which exists for normalisation purposes as the dedicated loop entry
    block.

    Memory access in LLVM is expressed separately from index calculations.
    This is visible at lines 16--19.
    The ``{\tt getelementptr}'' instruction is used to calculate the memory
    addresses of the array elements {\tt a[i]} and {\tt b[i]}.
    The ``{\tt load}'' instruction then reads the memory at the calculated
    addresses.
    This scheme simplifies the effectful memory access instructions, pushing the
    complexity of index calculations into the ``{\tt getelementptr}''
    instruction.

\begin{figure}[p]
\vspace{-0.09cm}
\begin{lstlisting}[language=MyCpp,captionpos=t,title=
   {{\bf(a)} {} C source code of a dot product function implementation:
    \leftskip=0pt}]
double dot(double* a, double *b, size_t n)
{
  double d = 0.0;
  for(int i = 0; i < n; i++)
    d += a[i]*b[i];
  return d;
}
\end{lstlisting}
\vspace{-0.09cm}
\begin{lstlisting}[language=LLVM,breaklines=true,captionpos=t,title=
   {{\bf(b)} {} LLVM IR of the same dot product function:
    \leftskip=0pt}]
define double @dot(double* %0, double* %1, i64 %2) {
; <label>:3:
  %4 = icmp eq i64 %2, 0  ; integer (i) comparison (cmp): check if register %2 is equal (eq) to constant zero
  br i1 %4, label %5, label %7 ; jump to line 6 if the comparison held, otherwise jump to line 10 instead

; <label>:5:
  %6 = phi double [ 0.0, %3 ], [ %16, %8 ] ; result is 0 if the phi node was reached from line 4, otherwise it was reached from line 24 and the result is taken from %16
  ret double %6

; <label>:7:
  br label %8

; <label>:8:
  %9 = phi i64 [ %17, %8 ], [ 0, %7 ]
  %10 = phi double [ %16, %8 ], [ 0.0, %7 ]
  %11 = getelementptr double, double* %0, i64 %9 ; getelementptr calculates memory addresses: here it computes the address of the %9-th value in the array %0
  %12 = load double, double* %11 ; loads a double precision floating point value from the calculated address
  %13 = getelementptr double, double* %1, i64 %9
  %14 = load double, double* %13
  %15 = fmul double %12, %14
  %16 = fadd double %10, %15
  %17 = add i64 %9, 1
  %18 = icmp eq i64 %17, %2
  br i1 %18, label %5, label %8
}
\end{lstlisting}
\caption{The correspondence between C and LLVM IR on an example function:
         The function computes the dot product of two vectors in a simple
         reduction loop.
         In the LLVM IR -- generated by Clang -- pointer calculations and memory
         accesses are visible within the SSA representation.}
\label{llvmirexample}
\end{figure}

    \Cref{fig:derivemaths} shows in detail how the SSA model is constructed from
    the LLVM intermediate representation of the program in \Cref{llvmirexample}.
    At the top left, implementations of the dot product are shown
    in three programming languages that LLVM supports: Fortran, C and C++.
    At the top right is the corresponding LLVM IR code.
    While this is not precisely identical for different implementations,
    its basic structure is independent of the source language.

    In the middle row, the structure of the LLVM IR code is separated into three
    components: labelled multigraphs for the data flow and control flow, as well
    as the per-instruction properties represented as a list.
    Together, they capture all the semantically significant information of the
    function, as previously demonstrated in \Cref{sec:remainingstructure}.
    In the bottom row, the SSA model is shown, adhering to \Cref{def:ssamodel}.
    The labelled multigraphs for the control flow and data flow graphs are
    represented as sets of $3$-tuples of integers.

\begin{figure}[p]
\input{latex/figure_derivemaths}
\caption{Compiler-generated LLVM IR code is decomposed into data flow, control
         flow and per-value attributes.
         Mathematical notations of the three components are shown at the
         bottom.}
\label{fig:derivemaths}
\end{figure}

\section{Constraint Programming on the SSA Model}
\label{sec:constraintprogramming}

    Properties of SSA programs can now be formulated as constraint problems on
    the SSA model.
    For this purpose, the {\it set of SSA models} is introduced in
    \Cref{def:modelrepresentations}, which \Cref{def:cprob} then uses to
    formulate {\em SSA constraint problem}s.

\begin{figure}[H]
\begin{definition}{Set of SSA Models}{modelrepresentations}
    Given a specific SSA representation (LLVM, Hydrogen, MIR, $\dots$),
    denote $F$ the set of all valid functions that can be expressed in it.

    The {\em set of SSA models} $\mathcal M$ is defined as
    \begin{align*}
        \mathcal M=\{M\mid M \text{ is the SSA model of some }\mathcal F\in F\}.
    \end{align*}
\end{definition}

\begin{definition}{SSA constraint problem}{cprob}
    An {\it SSA constraint problem} $(V,C)$ is a pair of a finite set of
    variables $V$ and a boolean predicate
    $C\colon\mathcal M\times\mathbb N^V\mapsto\{1,0\}$.
    The set of {\em constraint solutions} for an SSA constraint problem in the
    context of a specific SSA model $M\in\mathcal M$ is given as
    \begin{align*}
        S_M(V,C) = \{s\in\mathbb N^V\mid C(M,s)=1\}.
    \end{align*}
\end{definition}
\end{figure}

    The following intuition applies:
    The first argument of $C$ is the SSA model of a function.
    The second argument of $C$ is a solution candidate.
    $\mathbb N^V$ can be understood as abstractly rendering
    \lstinline[language=MyCpp]{map<string,unsigned>}, with elements assigning
    an integer to each constraint variable.
    These integers represent values in the SSA function by indexing into the
    list of used values.
    The predicate function determines whether these values have a specific
    relationship to each other.
    Finally, the set of constraint solutions lists all those tuples for which
    the predicate holds.

\subsection{SSA Constraint Problem Example}

\begin{figure}[p]
    \input{latex/figure_solutionexample}
\caption{Detection of simple loop iterators is formulated as a constraint
         problem and applied to the SSA model from \Cref{fig:derivemaths}.
         A single solution corresponding to the C variable ``{\tt i}'' is found.}
\label{fig:constraintsolution}
\end{figure}

    Consider the task of detecting all simple loop iterators in a program.
    These are variables within a loop that are incremented by a constant value
    of one in each iteration.
    \Cref{fig:constraintsolution} shows how this can be formulated as an
    SSA constraint problem and then demonstrates its application on the
    SSA model that was derived in \Cref{fig:derivemaths}.
    The top {\bf(a)} of the figure first gives an intuition about how such a
    compiler analysis task can be interpreted as a constraint problem to be
    solved in the context of an SSA model.

    Simple loop iterators show up in LLVM IR as data flow cycles between a
    $\Phi$-instruction and an addition.
    This is expressed as an SSA constraint problem at the top of the central
    part {\bf(b)} of the figure.
    The formulation introduces the variables
    ``phi'', ``update'', ``step'' and a predicate $C$ to describe the
    required conditions on the variables.
    This predicate is composed by logical conjunctions (``$\land$'') of
    several {\em element-of} relationships that must hold simultaneously on
    the structures $C_\mathcal F$, $I_\mathcal F$, and $DFG_\mathcal F$ of
    the SSA model:
    \begin{itemize}
        \item The iterator is incremented in steps of one
              [$(1,x_\text{step})\in C_\mathcal{F}$].
        \item The updated iterator value is computed as an addition
              [$(add,x_\text{update})\in I_\mathcal{F}$] of the previous
              iterator value
              [$(x_\text{phi},x_\text{update})\in DFG_\mathcal{F}^*$] and the
              step size
              [$(x_\text{step},x_\text{update})\in DFG_\mathcal{F}^*$].
        \item The updated iterator is an incoming value
              [$(x_\text{update},x_\text{phi})\in DFG_\mathcal{F}^*$] to the
              $\Phi$-instruction that holds the current iterator value
              [$(phi,x_\text{phi})\in I_\mathcal{F}$].
    \end{itemize}
    Explicit control flow constraints for establishing the loop structure around
    the iterator are not required, because the data flow cycle
    $[(x_\text{phi},x_\text{update})\in DFG_\mathcal{F}^*\mathrel\land(x_\text{update},x_\text{phi})\in DFG_\mathcal{F}^*]$
    already implies the presence of a loop.

    The lower section of \Cref{fig:constraintsolution} {\bf(b)} replicates the
    SSA model from \Cref{fig:derivemaths}.
    Finally, the bottom {\bf(c)} of the figure shows the constraint solutions
    $S_{M_{dot}}(V,C)$.
    This set contains only one tuple:
    $\{\text{phi}\mapsto9,\text{update}\mapsto17,\text{step}\mapsto22\}$.
    The underlined items in the SSA model evidence the validity of this
    solution:
    $(1,22)\in C_\mathcal F$,
    $(\textit{add},17)\in I_\mathcal F$,
    $(1,9,17)\in DFG_\mathcal F$,
    $(2,22,17)\in DFG_\mathcal F$,
    $(2,17,9)\in DFG_\mathcal F$, and
    $(\textit{phi},9)\in I_\mathcal F$.
    Therefore, all six {\it element-of} conditions that make up the SSA
    constraint problem hold.

    \Cref{fig:derivemaths} identifies the integer values $9$ and $17$ with the
    instructions at lines 14 and 22 of the LLVM IR code from
    \Cref{llvmirexample} {\bf(b)}.
    These two instructions correspond to the simple loop iterator ``\texttt{i}''
    of the for-loop at lines 4--5 of the C source code in
    \Cref{llvmirexample} {\bf(a)}, correctly identifying the only simple loop
    iterator in the program.

    After the detailed derivation of the SSA model and some intuition about the
    nature of SSA constraint problems, only the solver $S$ in
    \Cref{fig:constraintsolution} remains unexplained.
    The next sections derive how backtracking can be used to
    compute the set of constraint solutions efficiently.

\section{Solving SSA Constraint Problems}
\label{sec:constraintsolving}

    The solver for SSA constraint problems should efficiently compute $S_M(V,C)$
    for some concrete SSA constraint problem $(V,C)$ and SSA model $M$.
    This is a search problem:
    all values in $\mathbb N^V$ that satisfy $C$ in the context $M$ need to be
    identified.

    The search space $\mathbb N^V$ is infinitely large, but it can immediately
    be trimmed to only tuples that have all values $\leq|val|$.
    The case \mbox{$|V|>50$} is common in
    \Cref{chapter:candl,chapter:reductions,chapter:idioms}, and interesting
    functions often have \mbox{$|val|>100$}.
    The remaining search space therefore has $>100^{50}=10^{100}$ elements.
    Brute-force search is unfeasible on such a large search space, even when
    assuming that the direct evaluation of the predicate for any potential
    solution can be performed efficiently.
    However, backtracking can be used to find partial solutions that are
    incrementally extended.

\begin{figure}[h]
    \begin{definition}{Backtracking Solution of Constraint Problems}{backtracking}
        Given an SSA constraint problem $(V,C)$ and an enumeration of
        $V=\{v_1,\dots,v_{|V|}\}$, any collection $(B_k)_{k=1\dots |V|}$
        of functions
        $B_k:\mathcal M\times \mathbb N^{k-1}\rightarrow\mathcal P(\mathbb N)$
        is denoted a {\em backtracking solution} of $(V,C)$ if and only if the
        following is satisfied for all $M\in\mathcal M,x\in\mathbb N^V$:
        \begin{align}
            C(M,x)=1\iff\left[x_{v_k}\in B_k(M,p_{k-1}(x))\text{ for all }1\leq k\leq |V|\right],
        \end{align}
        where the projections $p_k\colon\mathbb N^V\rightarrow\mathbb N^k$ are
        defined by $x\mapsto(x_{v_1},\dots x_{v_k})$ for all $0\leq k\leq |V|$.
    \end{definition}
\end{figure}

    \Cref{def:backtracking} defines the concept of a backtracking solution, but
    it does not show how such a backtracking solution could be constructed.
    That is explained in the next section.
    The concept is introduced in order to allow the definition of the
    backtracking search \Cref{backtrackalg}, which works as follows:
    The variable $x$ iterates over $\mathbb N^n$
    ($n=|V|$, identified with $\mathbb N^V$ at line 7).
    The variable $k$ tracks the number of currently considered dimensions of
    this partial solution.
    After the initialisation assignments at line 2, a single loop spans the
    remainder of the algorithm.
    In each iteration, the algorithm tries to assign a valid value to the latest
    considered element in the partial solution $x$ (line 4).
    By convention, the minimum of the empty set is the
    \mbox{symbol ``$\infty$''}.
    If this case occurs, the algorithm backtracks at lines 12--17.
    Otherwise, the solution is either complete (lines 6--8), or the dimension
    $k$ of the partial solution is increased at line 10, and the
    algorithm continues by searching for the next element of $x$ in the
    following iteration.

\begin{algorithm}[p]
    \caption{Backtracking algorithm}
    \begin{algorithmic}[1]
        \Procedure{Detect}{$M,(B_k)_{k=1\dots n},(v_k)_{k=1\dots n}$}\vspace{-0.45em}
            \State $k\gets1$,\quad$x\gets(1,\dots,1)\in\mathbb N^n$\vspace{-0.45em}
            \While{$true$}\vspace{-0.45em}
                \State $x_k\gets min\{y\in B_k(M,p_{k-1}(x))\mid y\geq x_k\}$\vspace{-0.45em}
                \If{$x_k<\infty$}\vspace{-0.45em}
                    \If{$k=n$}\vspace{-0.45em}
                        \State {\bf yield} $\{v_k\mapsto x_k\}_{k=1\dots n}\in\mathbb N^V$\vspace{-0.45em}
                        \State $x_k\gets x_k+1$\vspace{-0.45em}
                    \Else\vspace{-0.45em}
                        \State $k\gets k+1$\vspace{-0.45em}
                        \State $x_k\gets1$\vspace{-0.45em}
                    \EndIf
                \Else\vspace{-0.45em}
                    \State $k\gets k-1$\vspace{-0.45em}
                    \If{$k\geq1$}\vspace{-0.45em}
                        \State$x_k\gets x_k+1$\vspace{-0.45em}
                    \Else\vspace{-0.45em}
                        \State {\bf exit}
                    \EndIf
                \EndIf
            \EndWhile
        \EndProcedure
    \end{algorithmic}
    \label{backtrackalg}
\end{algorithm}

\subsection{The Structure of SSA Constraint Problems}

    Basic construction rules for SSA constraint problems are the {\it element-of
    constraint problem} in \Cref{def:elementofconstr},
    and the {\it conjunction and disjunction constraint problems} in
    \Cref{def:conjconstr}.
    \Cref{def:constrext} enables the composition of constraint problems that are
    not defined on the same set of variables.
    The example in \Cref{fig:constraintsolution} can be constructed with only
    these rules.

\begin{figure}[p]
    \begin{definition}{Element-of Constraint Problem}{elementofconstr}
        Given a set of tuples $S(M)\subset\mathbb N^V$ that may depend on
        $M\in\mathcal M$ (e.g.\ $S(M)\approx DFG_\mathcal F^*$),
        the {\it element-of constraint problem} $(V,E_S)$ is defined by
        \begin{align*}
            E_S(M,x)=\left\{\begin{array}{l}
                                1\text{\quad if }x \in S(M)\\
                                0\text{\quad otherwise}.
                            \end{array}\right.
        \end{align*}
    \end{definition}

    \begin{definition}{Conjunction and Disjunction Constraint Problems}{conjconstr}
        Given SSA constraint problems $(V,C)$ and $(V,C')$, the
        {\it conjunction constraint problem}
        $(V,C\hspace{-0.5mm}\land\hspace{-0.5mm}C')$ and the
        {\it disjunction constraint problem}
        $(V,C\hspace{-0.5mm}\lor\hspace{-0.5mm}C')$ are defined by
        \begin{align*}
            &C\hspace{-0.5mm}\land\hspace{-0.5mm}C'(M,x)=\left\{
                \begin{array}{l}
                    1\text{\quad if }C(M,x)=1\mathrel\land C'(M,x)=1\\
                    0\text{\quad otherwise}
                \end{array}\right.\\
            &C\hspace{-0.5mm}\lor\hspace{-0.5mm}C'(M,x)=\left\{
                \begin{array}{l}
                    1\text{\quad if }C(M,x)=1\mathrel\lor C'(M,x)=1\\
                    0\text{\quad otherwise}.
                \end{array}\right.
        \end{align*}
    \end{definition}

    \begin{definition}{Extension of a Constraint Problem}{constrext}
        Given an SSA constraint problem $(V,C)$ and an injection
        $i:V\hookrightarrow W$, the
        {\it extension of the constraint problem} $(W,C^W)$ is defined by
        \begin{align*}
            C^W(M,x)=C\left(M,\left(x_{i(v)}\right)_{v\in V}\right).
        \end{align*}
    \end{definition}
\end{figure}

\begin{figure}[p]
    \input{latex/figure_theorems1}
\end{figure}

\begin{figure}[p]
    \input{latex/figure_theorems2}
\end{figure}

    \Cref{theo:theo1} introduces backtracking solutions for element-of
    constraint problems.
    They are constructed such that
    $B_k[E_S](M,x)=\{y_{v_k}\mid y\in S(M), y_{v_1}=x_1,\dots,y_{v_{k-1}}=x_{k-1}\}$.
    This is optimal in the sense that the backtracking algorithm will only ever
    backtrack immediately after yielding a result.
    In the theorem, the backtracking solution is not defined directly.
    Instead, it uses a helper construct $R_k$.
    \Cref{subsec:impl} derives an efficient implementation of $R_k$.

    \Cref{theo:theo2} introduces backtracking solutions for conjunction
    constraint problems.
    These are constructed in an obvious way, by taking the intersection set of
    the underlying backtracking solutions at each $k$.
    This is not possible for disjunction constraint problems, which are
    discussed in \Cref{theo:theo3}.
    The additional structure $R_k$ is used to keep track of whether the current
    partial solution satisfies $C$ or $C'$ so far
    (this was a given for conjunctions).
    The backtracking solution at $k$ is then the union only of those underlying
    backtracking solutions at $k$ where the corresponding element in $R_k$
    signals validity.
    Finally, \Cref{theo:theo3} shows that backtracking solutions for extensions
    of constraint problems can be constructed by skipping the
    additional variables.

\subsection{Backtracking Example}

\begin{figure}[p]
    \input{latex/figure_backtrackexample}
    \caption{Backtracking is used to find the single solution of the
             SSA constraint problem for simple loop iterators from
             \Cref{fig:constraintsolution}.
             The backtracking solution $(B_k[C])_{k=1\dots3}$ is constructed
             with \Cref{theo:theo1,theo:theo2,theo:theo4}.
             The partial solution ``$x$''  is extended in three steps, from top
             to bottom.}
    \label{fig:backtracsol}
\end{figure}

    \Cref{fig:backtracsol} demonstrates how backtracking can algorithmically
    determine the solution of the SSA constraint problem that was introduced in
    \Cref{fig:constraintsolution} for recognising simple loop iterators.
    This SSA constraint problem is replicated at the top of the figure.
    Its construction follows the rules in
    \Cref{def:elementofconstr,def:conjconstr,def:constrext}, using conjunctions
    of element-of constraint problems.

    The bottom left part of the figure shows the backtracking solution
    of the SSA constraint problem that is obtained by applying
    \Cref{theo:theo1,theo:theo2,theo:theo4}.
    This construction, according to the theorems, guarantees that the defining
    condition in \Cref{def:backtracking} holds for $(B_k[C])_{k=1\dots3}$ and
    that \Cref{backtrackalg} can be applied.
    The construction of one element in the backtracking solution is
    explained in detail as follows.

    The condition $(x_\text{update},x_\text{phi})\in DFG_\mathcal F^*$
    corresponds to the \mbox{element-of} constraint problem $E_S$ with
    $S(M)=\{x\in\mathbb N^{\{\text{update},\text{phi}\}}\mid (x_\text{update},x_\text{phi})\in DFG_\mathcal F^*\}$.
    Following \Cref{theo:theo1}, this gives
    \begin{align*}
    v_1={}&\text{phi}&v_2={}&\text{update}\\
    R_1(M)={}&rev(DFG_\mathcal F^*)&
    R_2(M,x_1)={}&select(x_1,rev(DFG_\mathcal F^*)).
    \end{align*}
    Therefore, the backtracking solution is given by
    \begin{align*}
        B_1[E_S](M)={}&heads(rev(DFG_\mathcal F^*))\\
        B_2[E_S](M,x_1)={}&heads(select(x_1,heads(rev(DFG_\mathcal F^*)))).
    \end{align*}
    This backtracking solution for $E_S$ is extended to the entire set $V$ using
    \Cref{theo:theo4}, yielding the additional function
    $B_3[E_S^V](M,x)=\mathbb N$.
    Finally, \Cref{theo:theo2} embeds $B_1[S_S^V]$ in $B_1[C]$ and
    $B_2[S_S^V]$ in $B_2[C]$ using set intersections with the other terms.
    The third term $B_3[E_S^V]=\mathbb N$ disappears in the assembled
    backtracking solution, as the intersection with $\mathbb N$ is redundant.

    The bottom right of the figure shows the backtracking algorithm applied to
    the SSA model from \Cref{fig:derivemaths}.
    Starting from the top with an empty partial solution $x=()$, candidates for
    $x_1$ are determined, corresponding to the variable ``phi''.
    There are three $\Phi$-instructions in the SSA model, all of which also
    satisfy the other conditions of being the source and destination of some
    edge in the data flow graph.
    Therefore, the algorithm continues with the partial solutions $x=(6)$,
    $x=(9)$ and $x=(10)$ one level further down in the figure.

    For $k=2$, which corresponds to the variable ``update'', the backtracking
    solution requires $x_2$ to form a data flow cycle with $x_1$.
    This is expressed  in the second and in the last line of $B_2[C](M,x)$.
    Furthermore, the value $x_2$ has to be an ``$add$'' instruction.
    The partial solution~$(6)$ corresponds to a $\Phi$-instruction that is not
    part of any data flow cycle.
    Therefore, the algorithm backtracks.
    For the partial solution $(10)$, the $\Phi$-instruction is part of a data
    flow loop cycle with the value $16$, but that value is an ``$fadd$''
    instruction.
    The partial solution $(9)$, however, can be extended according to the
    backtracking solution with the value $17$.
    Therefore, the algorithm only continues at the bottom of the figure with
    this one partial solution $(9,17)$.

    To complete this partial solution, $B_3[C]$ requires the value for the
    ``step'' variable to be a constant of value $1$.
    Furthermore, it needs to be used as an argument to $x_2$, i.e.\ the
    ``update'' variable.
    Both of these conditions hold for the value $22$, yielding a complete
    solution.

\subsection{Implementation, Data Structures and Complexity}
\label{subsec:impl}

\begin{figure}[p]
    \input{latex/figure_backtrackingcpp}
\end{figure}

    With the explicit construction rules for backtracking solutions in
    \Cref{theo:theo1,theo:theo2,theo:theo3,theo:theo4}, suitable data structures
    can be identified in order to efficiently implement the
    backtracking algorithm.
    This requires a slight extension of \Cref{backtrackalg} to provide suitable
    interfaces for computing the supporting structures $R_k$ from
    \Cref{theo:theo1,theo:theo3}.
    \Cref{cppsolver} shows how the backtracking algorithm with those hooks is
    implemented as a C++ function.
    Most of the ``\texttt{solver}'' function at lines 12--43 maps directly onto
    \Cref{backtrackalg} and needs no further explanation.
    However, some crucial details need elaboration, mostly relating to the
    iteration over the backtracking solution at line 4 of \Cref{backtrackalg}.

    The ``\texttt{BacktrackingPart}'' class provides a C++ interface
    for the individual functions $B_k$ in backtracking solutions as in
    \Cref{def:backtracking}.
    Four member functions must be implemented.
    The most important of them, ``\texttt{skip\_invalid}'',
    gives $x_k\gets min\{y\in B_k(M,p_{k-1}(x))\mid y\geq x_k\}$
    from line 4 of \Cref{backtrackalg} via
    \texttt{B[k]->skip\_invalid(x[k])}.

    The other three member functions of
    ``\texttt{BacktrackingPart}'' manipulate shared state
    of ``\texttt{BacktrackingPart}'' objects that together
    comprise a backtracking solution.
    This allows computations that would otherwise have to be performed
    repeatedly by ``\texttt{skip\_invalid}'', to be done ahead of time.
    More specifically, this corresponds to $R_k$ in
    \Cref{theo:theo1,theo:theo4}.
    Crucially, this distribution of the computations allows all previously
    discussed types of SSA constraint problems to be implemented efficiently
    with the ``\texttt{BacktrackingPart}'' interface.

\subsubsection{Data Structures for Constraint Classes}

    \paragraph*{Element-of constraint problems} require the structures $R_k$.
    These can be implemented with a tree, built around a sorted array of pairs,
    as shown in the first line of the top section of \Cref{partimplementations}.
    Each subtree in the array corresponds to a potential $R_{k+1}$.
    The {\tt BacktrackingPart} instances for this constraint keep indices
    (line~25) into
    the array (line~3), which {\tt begin} initialises (line~18).
    Each time {\tt skip\_invalid} is called, the index is incremented until
    it either points to an entry $\geq$ the lower bound {\tt c} or hits the
    end of the array (lines 9--10).
    Finally, {\tt resume} does nothing (line 22), and {\tt fixate} simply
    accesses the array at the current index for the subtree $R_{k+1}$
    (lines 20--21).

    \paragraph*{Conjunction constraint problems} are simpler to implement, as
    shown in the lower section of \Cref{partimplementations}.
    The methods {\tt begin}, {\tt fixate}, and {\tt resume} call the
    corresponding member functions of the underlying {\tt BacktrackingPart}
    objects (line 20), and \texttt{skip\_invalid} checks both of them with
    short-circuit evaluation in case of a {\tt FAIL} result.

    \paragraph*{Disjunction constraints} again require the structures $R_k$,
    consisting of an array of booleans, with each element corresponding to one
    underlying constraint option.
    Calls to \texttt{skip\_invalid} are passed on to those underlying
    \texttt{BacktrackingPart} objects that are not disabled via $R_k$.
    If any succeeded, then the smallest resulting value is chosen.
    The {\tt fixate} function determines $R_{k+1}$ with \texttt{skip\_invalid},
    disabling all of the underlying constraint options that do not return
    {\tt SkipResult::PASS}.
    The \texttt{begin}, \texttt{fixate} and \texttt{resume} member calls are
    also passed on to {\tt BacktrackingPart} instances that are enabled in
    $R_k$ for appropriate bookkeeping.

\begin{figure}[p]
\lstset {
 basicstyle=\linespread{0.944}\ttfamily
}
\begin{lstlisting}[language=MyCpp]
struct Tree { vector<pair<int,Tree>> t; };
template<int n>
struct ElementOfShared { Tree S; array<Tree*,n> R; };

template<int n, int k>
class ElementOfPart : public BacktrackingPart {
public:
    SkipResult skip_invalid(unsigned& c) override {
        while(index < shared->R[k]->t.size()
           && c > shared->R[k]->t[index].first) index++;
        if(index == shared->R[k]->t.size())
            return SkipResult::FAIL;
        if(shared->R[k]->t[index].first == c)
            return SkipResult::PASS;
        c = shared->R[k]->t[index].first;
        return SkipResult::SUCCESS;
    }
    void begin() override { index = 0;
        if(k == 0) shared->R[k] = &shared->S; }
    void fixate(unsigned) override {
        if(k < n) shared->R[k+1] = &shared->R[k][index]; }
    void resume() override {}
private:
    shared_ptr<ElementOfShared<n>> shared;
    size_t                         index;
};
\end{lstlisting}
\begin{lstlisting}[language=MyCpp, label={partimplementations}, caption=
   {{\tt BacktrackingPart} is implemented for element-of constraints and
    conjunction constraints.
    The struct {\tt ElementOfShared} corresponds to
    $\protect{(R_k[E_S](M,x))_{k=1\dots n}}$ in \Cref{theo:theo1}.}]
class ConjunctionPart : public BacktrackingPart {
public:
    SkipResult skip_invalid(unsigned& c) {
        SkipResult r1 = parts[0]->skip_invalid(c);
        if(r1 == SkipResult::FAIL)   return r1;
        if(r1 == SkipResult::CHANGE) return r1;
        SkipResult r2 = parts[1]->skip_invalid(c);
        if(r2 == SkipResult::FAIL)   return r2;
        if(r2 != SkipResult::PASS)
            return SkipResult::CHANGE;
        return r1;
    }
    void begin() override {
        for(auto part: parts) part->begin(); }
    void fixate(unsigned c) override {
        for(auto part: parts) part->fixate(c); }
    void resume() override {
        for(auto part: parts) part->resume(); }
private:
    array<shared_ptr<BacktrackingPart>,2> parts;
};
\end{lstlisting}
\end{figure}

\subsubsection{Computational Complexity}

    The chosen data structures allow for efficient implementations
    of the individual functions that are used to search for solutions.
    However, SSA constraint problems are a generalisation of the subgraph
    isomorphism problem.
    Any subgraph can be specified using conjunctions of element-of constraints,
    and the solver will search for these within any provided SSA model.
    \citet{Cook:1971:CTP:800157.805047} showed that this problem is NP-hard
    \citep{Leeuwen:1990:HTC:574797}.
    Therefore, the computational complexity has an exponential worst case
    (in the number of variables) when evaluated in general.

    However, this is no impediment due to the particular use case of
    SSA constraint problems.
    In practice, the nature of SSA constraint problems means that a solver is
    going to solve the same limited set formulas repeatedly -- only in the
    context of different SSA models.
    This means that is it viable to craft the formulas in such a way to be
    efficiently solvable, in particular by choosing a good order in which to
    iterate over the variables.
    For specific SSA constraint problems, it can then be possible to make
    complexity assumptions.

\subsection{Additional SSA Constraint Problems}

    Beyond \Cref{def:elementofconstr,def:conjconstr,def:constrext}, there are a
    number of supplementary construction rules for SSA constraint problems that
    are used in this thesis.
    Firstly, there are a number of very simple SSA constraint problems that
    operate directly on the integer values, without interpreting them as indices
    into the value list of the SSA model.
    These are listed in \Cref{def:auxconstr}.
    The first two, $(\{a,b\}, C^=)$ and $(\{a,b\}, C^{\neq})$, enforce
    equality and inequality of the integer values respectively.
    The third, $(\{a\}, C^{unused})$, is used to signal that no value from the
    SSA model was assigned.
    This is convenient within disjunctions for SSA constructions that may be
    omitted, i.e.\ an offset that is optionally added when accessing an element
    in a structure with a pointer to the object base.
    The value $|val|+1$ is the smallest integer that can no longer be
    interpreted as a value in the SSA model.
    Therefore, the constraint enforces equality to this number.

    Generalised graph domination constraint problems as in \Cref{def:domconstr}
    are versatile tools for analysing SSA models.
    The special case of $|Orig|=|Dom|=|Dest|=1$, with $x_v$ restricted to the
    control origin of the function for $v\in Orig$, turns this definition
    into the established control flow dominance concept.
    However, the generalisation encompasses a much larger set of interesting
    properties.
    Importantly, this can be applied also to data flow, which
    \Cref{chapter:reductions} makes use of for defining kernel functions with
    restricted interfaces.
    In this interpretation, the set $Dom$ represents an interface to the
    computation of the values in $Dest$, when data flow can originate from all
    the values in $Orig$.

    The backtracking solution of generalised graph domination constraint
    problems is brute force, meaning that $B_n[D_g]$ checks the condition
    as specified, and $B_k[D_G]\equiv 1$ for all $1\leq k<n$.
    The checking of the condition in $B_n[D_G]$ is typically preformed in
    $O(|val|)$ time, so this is a computationally expensive constraint.
    The solver relies on this construction occuring only in conjunctions with
    other, more restrictive constraints.
    However, special cases can also be implemented as element-of constraint
    problems using precomputated dominator trees.

    Finally, collect-all constraint problems are introduced in
    \Cref{def:collectconstr}.
    Given a constraint problem $(V,C)$, the variables $V$ are divided into
    two subsets.
    The collect-all constraint problem identifies all possible solutions over
    one of the variable subsets, given that the other variable subset is fixed.
    The parameter $N$ gives an upper limit to the number of these solutions.
    Collect-all constraint problems are used to approximate logical quantifiers.
    This is discussed in more detail in \Cref{chapter:candl}.

    In order to calculate a backtracking solution of collect-all constraint
    problems, the order on $W$ needs to satisfy the property
    $w_a\in U,w_b\notin U\implies a<b$.
    For values in $U$, the backtracking solution simply passes through to the
    underlying constraint.
    Whenever a partial solution is determined over all values in $U$, a nested
    call to the solver is performed in order to identify all possible partial
    solutions over $V\setminus U$.
    The remaining steps of the backtracking solution then simply enforce this
    precomputed list of solutions.


\begin{figure}[p]
    \begin{definition}{Simple Integer Constraint Problems}{auxconstr}
        The SSA constraint problems $(\{a,b\}, C^=)$, $(\{a,b\}, C^{\neq})$
        and $(\{a\}, C^{unused})$ are given by
        \begin{align*}
        C^=(M,x)=1\iff{}&x_a=x_b\\
        C^{\neq}(M,x)=1\iff{}&x_a\neq x_b\\
        C^{unused}(M,x)=1\iff{}&x_a=|val|+1.
        \end{align*}
    \end{definition}
    \begin{definition}{Generalised Graph Domination Constraint Problems}{domconstr}
        For a graph $G(M)\subset\mathbb N^2$, a set of variables $V$,
        a {\it set of origins} $Orig\subset V$,
        a {\it set of generalised dominators} $Dom\subset V$, and
        a {\it set of destinations} $Dest\subset V$, the
        {\em generalised graph domination constraint problem}
        $(V,D_G)$ is defined by
        \begin{align*}
            D_G(M,x)=1\iff(x_v\notin R_{\infty}(M,x)\text{ for all }v\in Dest\text{ with }x_v\leq |val|),
        \end{align*}
        where $R_k(A,G)\subset\mathbb N$, the {\it set of vertices reachable in $k$ steps},
        is defined as
        \begin{align*}
            R_0(M,x)={}&\{x_v\mid v\in Orig\}\setminus\{x_v\mid v\in Dom\}\\
            R_{k+1}(M,x)={}&\{b\mid (a,b)\in G(M),a\in R_k(M,x)\}\setminus\{x_v\mid v\in Dom\}\\
            R_\infty(M,x)={}&\bigcup_{k=0}^\infty R_k(A,G).
        \end{align*}
    \end{definition}
    \begin{definition}{Collect-all Constraint Problem}{collectconstr}
        For a constraint problem $(V,C)$, a subset $U\subset V$ with
        $U=\{u_1,\dots,u_m\}$, and some $N>1$, the
        {\em collect-all constraint problem} $(W, C^{N[U]})$, where
        $W:=V\setminus U\cup\{1,\dots,N\}\times U$, is uniquely defined
        as satisfying for all $s\in S_M(W,C^{N[U]})$ the following conditions
        \begin{enumerate}
        \item $((s_{1,u_1},\dots s_{1,u_m}),\dots,(s_{N,u_1},\dots s_{N,u_m}))$ is an ordered tuple of tuples in $\mathbb N^U$.
              If there are any duplicates, they have to be of the form $(|val|+1,\dots,|val|+1)\in\mathbb N^U$.
        \item The sets $A$ and $B$ satisfy either $A=B$ or $((A=\emptyset)\mathrel\land(|B|>N))$, where\\
              $A:=\{(s_{k,u})_{u\in U}\mid 1\leq k\leq N, s_{k,u}\neq|val|+1\text{ for some }u\in U\}\subset N^U$ and\\
              $B:=\{(x_u)_{u\in U}\mid x\in S_M(V,C), x_v=z_v\text{ for all }v\in X\}\subset N^U$.
        \end{enumerate}
    \end{definition}
\end{figure}

\subsection{Satifiability Modulo Theory}
\label{sec:SATcomp}

    There are some parallels between SSA constraint problems and other
    satisfiability problems, most importantly Satisfiability Modulo Theory
    (SMT).
    Both methods construct formulas with constraints on variables that are
    combined with logical connectors, and backtracking is in both cases used as
    the basis for constructing efficient solvers.
    This suggests that recasting SSA constraint problems in the language of SMT
    was possible:
    Where typical SMT problems might on the {\em theory of linear arithmetic} or
    the {\em theory of bit-vectors}, SSA constraint problems operate on the
    {\em theory of SSA intermediate representation}.
    Powerful solvers have been developed for SMT problems over the last decade,
    making this appear worthwhile.

    However, the structure and usage scenario of SSA constraint problems make
    them unfitting for this approach.
    The contextual nature of SSA constraint problems means that the constraint
    formulation in SMT would have to encode also the graph structure of the
    SSA function as context.
    This encoding is non-trivial and would introduce significant overhead.

    Furthermore, one SSA constraint problem is typically evaluated with many
    different SSA function contexts, but
    an off-the-shelf SMT solver has no way of leveraging this knowledge.
    SSA constraint problems are crafted by hand, and some manual tuning is
    acceptable to enable quick solving times.
    In particular, the backtracking order for variables can be adjusted.
    Thus, it is a requirement for the solver to have consistently good
    performance across all SSA function contexts, but not necessarily across
    every possible constraint formula.
    Instead, it can dictate specific programming styles.
    The solver depending on such pre-processed formulas makes redundant many
    sophisticated optimisations that are crucial for SMT.

\section{Summary}

    The chapter introduced an approach for applying constraint programming to
    SSA intermediate representation code.
    Based on a model of the static structure of SSA programs,
    {\it SSA constraint problems} were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    The chapter derived efficient algorithms for solving SSA constraint problems
    and discussed several significant types of constraint formulas, reflecting
    compiler analysis methods like data flow and dominance relationships.

    \paragraph*{Outlook}
    Constraint programming on SSA code forms the principal methodological basis
    of the thesis.
    \Cref{chapter:candl,chapter:reductions} refer back to the definitions in
    this chapter for deriving the languages CAnDL and IDL, which integrate
    constraint programming on SSA code into LLVM.
    Preceding that, \Cref{chapter:literature} provides a review of related
    literature, putting the work into a broader context. 


    