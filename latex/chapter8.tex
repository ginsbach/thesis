
    The thesis developed a constraint programming methodology that operates on
    SSA compiler intermediate representation.
    The Compiler Analysis Description Language (CAnDL) and the extended Idiom
    Description Language (IDL) were designed and implemented in the LLVM
    framework, making the constraint programming methodology available for
    program analysis.

    Several computational idioms were implemented in these languages, which
    enabled the automatic recognition of adhering user code sections during
    compilation.
    Among these idioms, stencils and different forms of both sparse and
    dense linear algebra are well-understood kernels.
    Complementing these established computational idioms, Complex Reduction and
    Histogram Computations (CReHCs) were introduced as a new grouping of
    computations.
    Evaluation on established benchmark suites NPB and Parboil demonstrated that
    each of these idioms covers significant performance bottlenecks.

    Recognising these computational idioms enabled generic compilers to apply
    idiom-specific optimising transformations, which are traditionally available
    only within domain-specific tools.
    Such transformations included the automatic parallelisation of programs that
    were inaccessible for previous analysis approaches.

    In the final chapter, the idiom detection functionality was applied to
    achieve the automatic heterogeneous parallelisation of sequential C/C++
    code.
    Idiomatic loops in user code were automatically redirected to
    domain-specific code generators, leveraging the domain knowledge that is
    available for code that is formulated in restrictive idioms.
    This approach was effective on 10 of the 21 benchmark programs from NPB and
    Parboil, resulting in significant speedups from 1.26$\times$  to 275$\times$
    against the sequential baseline versions.

    In summary, computational idioms were identified and confirmed as an
    effective interface to heterogeneous acceleration, and the methodology
    for automatically recognising such idioms via constraint programming was
    derived.
    Main contributions are the methodology of constraint programming on SSA
    intermediate representation; the design and implementation of CAnDL and IDL;
    and the identification of CReHCs as a significant class of benchmark
    bottlenecks.

\section{Contributions}

\subsection*{Constraint Programming on SSA Code}

    \Cref{chapter:theory} introduced an approach for applying constraint
    programming to SSA intermediate representation code.
    Based on a model of the static structure of SSA programs, SSA constraint
    problems were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    The chapter derived efficient algorithms for solving SSA constraint problems
    and discussed several significant types of constraint formulas, reflecting
    compiler analysis methods like data flow and dominance relationships.
    The methodology was derived from algebraic formulation to implementation
    considerations, applies to any SSA intermediate representation,
    and is suitable for a wide range of compiler analysis problems.

\subsection*{CAnDL and IDL}

    \Cref{chapter:candl,chapter:reductions} designed and implemented two
    custom declarative programming languages: The Compiler Analysis
    Description Language (CAnDL) and the extended Idiom Description Langauge
    (IDL).
    These languages make the constraint programming methodology directly
    available in the LLVM framework.
    This enables the previously derived approach to operate or real user code,
    directly from within the state-of-the-art Clang C/C++ compiler.

\subsection*{Complex Reduction and Histogram Computations}

    Complex Reduction and Histogram Computations (CReHCs) were introduced in
    \Cref{chapter:reductions} as a computational idiom.
    CReHCs are a generalisation of well-understood scalar reductions that
    encompasses indirect array accesses as typical in calculations of
    histograms.
    This class of loops was not previously studied as one unit, but the
    chapter showed that shared parallelisation opportunities exist.
    Furthermore, an evaluation of established benchmark suites demonstrated
    that significant performance bottlenecks in each of NPB, Parboil, and
    Rodinia are CReHCs.

\subsection*{Formulation of Computational Idioms}

    \Cref{chapter:candl,chapter:reductions,chapter:idioms} formulated a range of
    different computational idioms in CAnDL and IDL.
    These idioms included stencils, different forms of sparse and dense linear
    algebra, polyhedral Static Control Parts (SCoPs), and CReHCs.
    The forulations enabled the automatic recognition of high-level algorithmic
    structure during compilation, making it possible for generic compilers to
    apply domain-specific reasoning.

\subsection*{Heterogeneous Acceleration Pipeline}

    \Cref{chapter:idioms} implemented a heterogeneous acceleration pipeline that
    is based on the formulation of computational idioms in IDL.
    The detection results during compilation were used to redirect relevant
    program parts to domain-specific code generators for heterogeneous
    acceleration.
    This pipeline resulted in significant parallelisation speedups on benchmark
    programs that oreviously were mostly inaccessible to automatic compiler
    reasoning.

\section{Critical Analysis}

    Several iddues remain with the work.
    Only a finite number of variables.
    Programs difficult to debug.
    SSA model needs to be reconstructed whenever acting on the results.

\subsection*{Generality of Computational Idioms}  

    Computational idioms were defined in
    \Cref{chapter:reductions,chapter:idioms} and
    evaluated succesfully on a range of established benchmark collections.
    However, the NPB and Parboil collections both are from areas of scientific
    computing.
    It remains unclear how generally applicable these idioms are on more
    diverse code bases from other domains.
    Similar concerns are faced by competing approaches, particularly the
    polyhedral model.

    More generally, it is unclear how much code in complex applications could
    be captured as ``idiomatic'' with even a very range and diverse set of
    idioms.
    Some benchmark programs even within NPB and Parboil, e.g.\ the ``sad''
    program, contain idiosyncratic computations that are unlikely to reoccur
    frequently in other contexts.

\subsection*{Finite Varaiable Numbers}

    The most visually apparent imperfection of both CAnDL and IDL is the
    limitation to a finite amount of variables in the underlying constraint
    problem.
    This results in upper bounds for the number of features in almost all idioms
    that have no fundamental justification:
    CReHCs can only contain a maximum of two histograms, stencil codes can only
    use a neighbourhood of up to 32 items, and so on.
    In practice, this involves a tradeoff between performance and the genreality
    of the idiom specification, as each additional variable introduces a slight
    overhead for the solver.

    Similar problems are well-known in other research disciplines that involve
    an underlying solver.
    Most prominently, bounded model checking techniques are built on SMT solvers
    with the same limitation.
    The same solution could be applied to IDL: Just repeatedly run the solver
    with more and more variables, up to a certain boundary.
    That way easy solutions are quickly found, but difficult solutions are not
    discarded entirely.

\subsection*{Compile Time Cost}

    Compiler times costs were 

\subsection*{Acting on Solutions Invalidates SSA Model}

\section{Future Work}

    Future work will focus on extending the approach to pointer-intensive
    idioms, improving the usability of specification languages, and
    complementing the static approaches with dynamic techniques.

\subsection*{Pointer-chasing Idioms}

    The ability to express sparse linear algebra sets the detection ability of
    constraint programming on SSA immediate representation apart from previous
    approaches using the polyhedral model or data-flow analysis.
    Going beyond simple data access indirection, IDL will be used on data access
    patters that involve pointer chases.
    This will enable it to detect graph operations, such as depth-first search
    and the PageRank algorithm.
    Furthermore, this approach could detect list traversal, insertion, and
    removal of elements.
    Such operations are not typically performance critical, but will be a first
    step to a deeper semantic understanding of programs in compilers.

\subsection*{Simpler Specification Languages}

    Compared to previous approaches, the specification languages CAnDL and IDL
    simplify the implementation of compiler analysis functionality, and enable
    the detection of more complex idiomatic structures.
    However, profound knowledge of compiler internals is still required to
    sucessfully write correct specifications.
    Furthermore, while the idiom specifications are mostly independent of
    LLVM IR as the specific underlying intermediate representation, the precise
    extent of this independence has not been explored thoroughly.

    Future work will investigate higher-level languages that abstract the
    IR-specific nature of CAnDL and IDL, and provide a more streamlined
    programming experience.
    The pseudocode and corresponding IDL specifications in
    \Cref{csr_lilacwhat_fig,jds_lilacwhat_fig} in \Cref{chapter:idioms} already
    suggest how this can be achieved:
    For restricted domains (e.g.\ SPMV), generating IDL code from high-level
    expressions is straightforward.

\subsection*{Complementing with Dynamic Approaches}

    Several shortcomings of the presented approaches could be alleviated with
    dynamic approaches that complement the entirely static nature of constraint
    programming on compiler immediate representation.

    Dynamic analysis could be used to preselect candidate loops, drastically
    reducing the compile time overhead.
    Dynamic methods are required to efficiently transfer memory between
    heterogeneous participants, and to rule out pointer aliasing.

\subsection*{Generating Specifications by Example}

    Specification could be generated from example.
