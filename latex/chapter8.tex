
    The thesis developed a constraint programming methodology that operates on
    SSA compiler intermediate representation.
    The Compiler Analysis Description Language (CAnDL) and the extended Idiom
    Description Language (IDL) were designed and implemented in the LLVM
    framework, making the constraint programming methodology available for
    program analysis.

    Several computational idioms were implemented in these languages, which
    enabled the automatic recognition of adhering user code sections during
    compilation.
    Among these idioms, stencils and different forms of both sparse and
    dense linear algebra are well-understood kernels.
    Complementing these established computational idioms, Complex Reduction and
    Histogram Computations (CReHCs) were introduced as a new grouping of
    computations.
    Evaluation of established benchmark suites NPB and Parboil demonstrated that
    each of these idioms covers significant performance bottlenecks.

    Recognising these computational idioms enabled generic compilers to apply
    idiom-specific optimising transformations, which are traditionally available
    only within domain-specific tools.
    Such transformations included the automatic parallelisation of programs that
    were inaccessible for previous analysis approaches.

    In the final chapter, the idiom detection functionality was applied to
    achieve the automatic heterogeneous parallelisation of sequential C/C++
    code.
    Idiomatic loops in user code were automatically redirected to
    domain-specific code generators, leveraging the domain knowledge that is
    available for code that is formulated in restrictive idioms.
    This approach was effective on 10 of the 21 benchmark programs from NPB and
    Parboil, resulting in significant speedups from 1.26$\times$  to 275$\times$
    against the sequential baseline versions.

    In summary, the thesis demonstrated that computational idioms are an
    effective interface to heterogeneous acceleration and derived the
    methodology for automatically recognising such idioms via constraint
    programming.
    Main contributions are the methodology of constraint programming on SSA
    intermediate representation; the design and implementation of CAnDL and IDL;
    and the identification of CReHCs as a significant class of benchmark
    bottlenecks.

\section{Contributions}

\subsection*{Constraint Programming on SSA Code}

    \Cref{chapter:theory} introduced an approach for applying constraint
    programming to SSA intermediate representation code.
    Based on a model of the static structure of SSA programs, SSA constraint
    problems were defined.
    These are formulas that impose restrictions on compiler intermediate code,
    turning the detection of adhering program parts into a constraint
    satisfiability problem.
    The chapter derived efficient algorithms for solving SSA constraint problems
    and discussed several significant types of constraint formulas, reflecting
    compiler analysis methods like data flow and dominance relationships.

    The constraint programming methodology was derived from an algebraic
    formulation all the way to implementation considerations, applies
    to any SSA intermediate representation, and is suitable for a wide range of
    compiler analysis problems.

\subsection*{CAnDL and IDL}

    \Cref{chapter:candl,chapter:reductions} designed and implemented two
    custom declarative programming languages: The Compiler Analysis
    Description Language (CAnDL) and the extended Idiom Description Langauge
    (IDL).
    These languages make the constraint programming methodology directly
    available in the LLVM framework.
    This enables the previously derived approach to operate or real user code,
    directly from within the state-of-the-art Clang C/C++ compiler.

\subsection*{Complex Reduction and Histogram Computations}

    Complex Reduction and Histogram Computations (CReHCs) were introduced in
    \Cref{chapter:reductions} as a computational idiom.
    CReHCs are a generalisation of well-understood scalar reductions that
    encompasses indirect array accesses as typical in calculations of
    histograms.
    This class of loops was not previously studied as one unit, but the
    chapter showed that shared parallelisation opportunities exist.
    Furthermore, an evaluation of established benchmark suites demonstrated
    that significant performance bottlenecks in each of NPB, Parboil, and
    Rodinia are CReHCs.

\subsection*{Formulation of Computational Idioms}

    \Cref{chapter:candl,chapter:reductions,chapter:idioms} formulated a range of
    different computational idioms in CAnDL and IDL.
    These idioms included stencils, different forms of sparse and dense linear
    algebra, polyhedral Static Control Parts (SCoPs), and CReHCs.
    The forulations enabled the automatic recognition of high-level algorithmic
    structure during compilation, making it possible for generic compilers to
    apply domain-specific reasoning.

\subsection*{Heterogeneous Acceleration Pipeline}

    \Cref{chapter:idioms} implemented a heterogeneous acceleration pipeline that
    is based on the formulation of computational idioms in IDL.
    The detection results during compilation were used to redirect relevant
    program parts to domain-specific code generators for heterogeneous
    acceleration.
    This pipeline resulted in significant parallelisation speedups on benchmark
    programs that oreviously were mostly inaccessible to automatic compiler
    reasoning.

\section{Critical Analysis}

    The methods of this paper were built on a thorough derivation in
    \Cref{chapter:theory} and then evaluated in different scenarios on real
    C/C++ program code using the LLVM compiler framework.
    Despite this effort of bridging between the algebraic derivations and the
    subsequent application in real-world scenarios, the work remained a
    prototype and several issues need to be addressed before it becomes viable
    for productive use.
    Most importantly, this includes questions about the prevalence of idiomatic
    code, the affordability of significant compile-time overhead, and the
    ability to compensate fundamental limitations of the underlying solver
    technology.

\subsection*{Generality of Computational Idioms}  

    Computational idioms were defined in
    \Cref{chapter:reductions,chapter:idioms} and
    evaluated succesfully on a range of established benchmark collections.
    However, the NPB and Parboil collections both are from areas of scientific
    computing.
    It remains unclear how generally applicable these idioms are on more
    diverse code bases from other domains.
    Similar concerns are faced by competing approaches, particularly the
    polyhedral model.

    More generally, it is unclear how much code in complex applications could
    be captured as ``idiomatic'' with even a very range and diverse set of
    idioms.
    Some benchmark programs even within NPB and Parboil, e.g.\ the ``sad''
    program, contain idiosyncratic computations that are unlikely to reoccur
    frequently in other contexts.

\subsection*{Compile Time Cost}

    The compile-time cost of idiom detection was evaluated in
    \Cref{chapter:idioms}, showing that overheads between 35 and 115 precent
    occurred accross the benchmark programs.
    Considering that the approach is built on constraint satisfiability
    methods, this is reassuring result, as the compile times remain within the
    same order of magnitude.
    However, from the perspective of compiler optimisations, this might be
    considered an unacceptable cost.
    Furthermore, the compile time overhead is unevenly distributed, with
    disproportionately higher cost on large functions and especially on code
    sections that are near misses to satisfying the specifications.

\subsection*{Finite Varaiable Numbers}

    One of the most apparent imperfections of both CAnDL and IDL is the
    limitation to a finite amount of variables in the underlying constraint
    problem.
    This results in upper bounds for the number of features many idioms:
    CReHCs can only contain a maximum of two histograms, stencil codes can only
    use a neighbourhood of up to 32 items, and so on.
    In practice, this involves a tradeoff between performance and the genreality
    of the idiom specification, as each additional variable introduces a slight
    overhead for the solver.

    Similar problems are well-known in other research disciplines that involve
    an underlying solver with similar limitations.
    For example, Bounded Model Checking \cite{Clarke:2001:BMC:510986.510987}
    uses SAT solvers and involves the complete unrolling of loops.
    In order to fit the underlying solver, this requires the introduction of a
    certain finite iteration limit $k$.
    The entire checking process can then be repeated with larger and larger
    values of $k$, successively ruling out more possible violations.

    The same approach could be applied to IDL: The solver is run with with more
    and more variables.
    At the point when none of the \texttt{collect} statements exceed variable
    capacity, the process is terminated.
    That way easy solutions are found without unneccessary overhead, yet
    difficult solutions are not discarded entirely.

\section{Future Work}

    Future work will focus on extending the approach to pointer-intensive
    idioms, improving the usability of specification languages, and
    complementing the static detection approaches with dynamic and
    machine learning techniques.
    Furthermore, the automatic generation of constraint specifications from
    code examples may eventually replace the necessity of writing constraint
    specifications manually.

\subsection*{Complementing with Dynamic and Machine Learning Approaches}

    Several shortcomings of the presented approaches could be removed with
    dynamic approaches and machine learning.
    Such methods complement the entirely static reasoning of constraint
    programming on compiler immediate representation naturally, but were
    outside the scope of this thesis.

    Dynamic analysis could be used to preselect candidate hot loops, drastically
    reducing the compile time overhead.
    In particular, dynamic analysis could collect program features that are
    then fed into machine learning algorithms.
    The use of neural networks for guiding compiler optimisations has been
    succesfully studied in the relevant literature
    \citep{DBLP:journals/pieee/WangO18}.

    Dynamic methods are also required to automise the efficient memory transfers
    that are required between heterogeneous participants, and to rule out
    pointer aliasing.

\subsection*{Pointer-chasing Idioms}

    The ability to express sparse linear algebra sets the detection ability of
    constraint programming on SSA immediate representation apart from previous
    approaches using the polyhedral model or data-flow analysis.
    Going beyond simple data access indirection, IDL will be used on data access
    patters that involve pointer chases.
    This will enable it to detect graph operations, such as depth-first search
    and the PageRank algorithm.
    Furthermore, this approach could detect list traversal, insertion, and
    removal of elements.
    Such operations are not typically performance critical, but will be a first
    step to a deeper semantic understanding of programs in compilers.

\subsection*{Simpler Specification Languages}

    Compared to previous approaches, the specification languages CAnDL and IDL
    simplify the implementation of compiler analysis functionality, and enable
    the detection of more complex idiomatic structures.
    However, profound knowledge of compiler internals is still required to
    sucessfully write correct specifications.
    Furthermore, while the idiom specifications are mostly independent of
    LLVM IR as the specific underlying intermediate representation, the precise
    extent of this independence has not been explored thoroughly.

    Future work will investigate languages that abstract the IR-specific nature
    of CAnDL and IDL, and provide a better programming experience.
    The pseudocode and corresponding IDL specifications in
    \Cref{csr_lilacwhat_fig,jds_lilacwhat_fig} in \Cref{chapter:idioms} already
    suggest how this can be achieved:
    For restricted domains (e.g.\ SPMV), generating IDL from high-level
    expressions is straightforward.

\subsection*{Generating Specifications by Example}

    Future work will investigate the automatic generation of idiom
    specifications from example code fragments.
    Using a graph-matching algorithm that operates on SSA code, multiple hot
    loops that implement variations of the same idiom will be matched together
    according to a quality metric.
    This matching will automatically identify code structures that are shared
    between the examples, and discard those that are unique to individual hot
    loops.
    The common structures will then be separated and turned into constraint
    conditions.

    Suitable quality metrics for graph matching will be critical to the success
    of this approach and might profit from machine learning approaches, such as
    neural networks.
    The manually implemented idioms from this work could provide suitable
    training data.

    Eventually, the manual curation of example codes that group together into
    computational idioms could become redundant.
    Automatic profiling of large quantities of code will allow the automatic
    identification of all the relevant hot loops.
    The hot loops could then be grouped into computational idioms using
    automatic clustering algorithms that use a distance metric between SSA code
    based on a success score for the previously discussed graph matching
    approach.
